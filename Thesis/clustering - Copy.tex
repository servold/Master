\documentclass[11pt]{article} 
\usepackage[american]{babel}
\usepackage{makeidx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{cleveref}

\oddsidemargin= -12pt \evensidemargin= -12pt
\topmargin=-45pt
\binoppenalty=10000
\relpenalty=10000
\textwidth=6.5in\textheight=9in \baselineskip=18pt
\parskip=6pt plus 1pt
\numberwithin{equation}{section}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[proposition]
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

\def\abs#1{\left\lvert#1\right\rvert}

\begin{document}

%\begin{center} 
%\Large {Tel-Aviv University}
%
%\Large {The Raymond and Beverly Sackler Faculty of} \\
%\Large {Exact Sciences} \\
%\Large {The Department of Statistics and Operations} \\
%\Large {Research}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {\bf Draft }
%
%\bigskip
%\bigskip
%\bigskip
%\bigskip
%
%\Large{Thesis Submitted Towards the Degree of Master} \\
%\Large {of Science in Operations Research}
%
%\bigskip
%
%\Large {Sergey Voldman}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {Supervisors:} \\
%\Large {Prof. Marc Teboulle} \\
%\Large {Prof. Shoham Sabach}
%
%\bigskip
%\bigskip
%
%\Large{2015}
%
%\end{center}
%
%\newpage
%
%\begin{center} \Large {\bf Draft}
%\end{center}
%
%\begin{abstract} 
%
%To-do...
%
%\newpage
%
%\end{abstract}
%
%\section{Introduction}
%
%To-do...
%

\newpage

\section{The Clustering Problem}

Let $\mathcal{A}= \left\lbrace a^1, a^2, \ldots ,a^m \right\rbrace$ be a given set of points in $\mathbb{R}^n$, and let $1 < k < m$ be a fixed given number of clusters. The clustering problem consists of partitioning the data $\mathcal{A}$ into $k$ subsets $\left\lbrace C^1, C^2, \ldots ,C^k \right\rbrace$, called clusters. For each $l=1, 2, \ldots ,k$, the cluster $C^l$ is represented by its center $x^l$, and we want to determine $k$ cluster centers $\left\lbrace x^1, x^2, \ldots ,x^k \right\rbrace$ such that the sum of proximity measures from each point $a^i, i=1, 2, \ldots ,m$, to a nearest cluster center $x^l$ is minimized.

The clustering problem is given by

\begin{equation}
	\min\limits_{x^1, x^2, \ldots ,x^k \in \mathbb{R}^n} F(x^1, x^2, \ldots ,x^k) := \sum\limits_{i=1}^{m} \min\limits_{1 \le l \le k} d(x^l,a^i) , \label{StateEq1}
\end{equation}

\noindent with $\textit{d}(\cdot ,\cdot)$ being a distance-like function.


\section{Problem Reformulation and Notations}

We introduce some notations that will be used throughout this document.

\noindent $a = (a^1, a^2, \ldots , a^m) \in \mathbb{R}^{nm}$, where $a^i \in \mathbb{R}^n,\smallskip i=1, 2, \ldots , m$.

\noindent $w = (w^1, w^2, \ldots , w^m) \in \mathbb{R}^{km}$, where $w^i \in \mathbb{R}^k,\smallskip i=1, 2, \ldots , m$.

\noindent $x = (x^1, x^2, \ldots , x^k) \in \mathbb{R}^{nk}$, where $x^l \in \mathbb{R}^n,\smallskip l=1, 2, \ldots , k$.

\noindent $d^{i}(x) = (d(x^1,a^i), d(x^2,a^i), \ldots , d(x^k,a^i)) \in \mathbb{R}^k,\smallskip i=1, 2, \ldots , m$.

\noindent $\Delta = \left\lbrace u \in \mathbb{R}^k \mid \sum\limits_{l=1}^{k} u_l = 1, \: u_l \geq 0 , \: l=1, 2, \ldots ,k \right\rbrace$.

\noindent Let $S \subseteq \mathbb{R}^n$. The indicator function of $S$ is defined and denoted as follows $\delta_S(p) = \begin{cases} 0, &\mbox{if } p \in S, \\ 
\infty, &\mbox{if } p \not\in S. \end{cases}$

Using the fact that $\min\limits_{1 \leq l \leq k} u_l = \min \left\lbrace \langle u,v \rangle \mid v \in \Delta \right\rbrace$, and applying it over $\left(\ref{StateEq1}\right)$, gives a smooth reformulation of the clustering problem

\begin{equation}
	\min\limits_{x \in \mathbb{R}^{nk}} \sum\limits_{i=1}^{m} \min\limits_{w^i \in \Delta} \langle w^i , d^i(x) \rangle. \label{StateEq2}
\end{equation}

Replacing further the constraint $w^i \in \Delta$ by adding the indicator function $\delta_{\Delta}(\cdot)$ to the objective function, results in a equivalent formulation

\begin{equation}
	\min\limits_{x \in \mathbb{R}^{nk} , w \in \mathbb{R}^{km}} \left\lbrace \sum\limits_{i=1}^{m} \langle w^i , d^i(x) \rangle + \delta_{\Delta}(w^i) \right\rbrace . \label{StateEq3}
\end{equation}

Finally, introducing several more useful notations is needed. For each $i=1, 2, \ldots , m$, we denote
\begin{center}
$H(w,x) := \sum\limits_{i=1}^{m} H_i(w,x) = \sum\limits_{i=1}^{m} \langle w^i , d^i(x) \rangle$ and $G(w) = \sum\limits_{i=1}^{m} G(w^i) := \sum\limits_{i=1}^{m} \delta_{\Delta}(w^i) .$
\end{center}

Replacing the terms in (\ref{StateEq2}) with the functions defined above gives a compact form of the original clustering problem

\begin{equation}
	\min \left\lbrace \Psi(z) := H(w,x) + G(w) \mid z := (w,x) \in \mathbb{R}^{km} \times \mathbb{R}^{nk} \right\rbrace . \label{StateEq4}
\end{equation}


\section{Clustering via PALM Approach}

\subsection{Introduction to PALM Theory}

Presentation of PALM's requirements and of the algorithm steps  $\ldots$


\subsection{Clustering with PALM for Squared Euclidean Norm Distance Function}

In this section we tackle the clustering problem with the classical distance function defined by $d(u,v) = \|u-v\|^2$. We devise a PALM-like algorithm, based on the discussion about PALM in the previous subsection.
Since the clustering problem has a specific structure, we are ought to exploit it in the following manner.
First we notice that the function 
$w \mapsto H(w,x)$ is linear in $w$, so there is no need to linearize it. In addition, the function 
$x \mapsto H(w,x) = 
\sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l \|x^l - a^i\|^2 =
\\ \sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is convex and quadratic in $x$, hence we do not need to add a proximal term as in PALM algorithm.

Now we propose a PALM-like algorithm for clustering, which we call KPALM.
\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random vectors $(w(0),x(0)) \in \Delta^m \times \mathbb{R}^{nk} .$

	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace(w(t),x(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: Take any $\alpha_i(t) > 0$ and for each $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			w^i(t+1) = \arg\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i - w^i(t)\|^2 \right\rbrace . \label{StateEq5}
		\end{equation}
		
		\item Centers Update: For each $l=1, 2, \ldots ,k$ compute $x^l \in \mathbb{R}^n$ via
		\begin{equation}
			x(t+1) = \arg\min \left\lbrace H(w(t+1), x) \mid x \in \mathbb{R}^{nk} \right\rbrace . \label{StateEq6}
		\end{equation}
	\end{enumerate}
\end{enumerate}

\newpage

At each step $t \in \mathbb{N}$, the KPALM algorithm alternates between cluster assignment and centers update. The explicit formulas, at step $t$, are given below

\begin{equation}
w^i(t+1) = P_{\Delta} \left(w^i(t) - \frac{d^i(x(t))}{\alpha_i(t)}\right) , \quad i=1, 2, \ldots ,m , \label{StateEq7}
\end{equation}

\begin{equation}
x^l(t+1) = \frac{\sum_{i=1}^{m} w^i_l(t+1) a^i}{\sum_{i=1}^{m} w^i_l(t+1)} , \quad l=1, 2, \ldots ,k , \label{StateEq8}
\end{equation}
where $P_{\Delta}$ is the orthogonal projection onto the set $\Delta$.

\begin{assumption} \label{StateEq17}
For all $1 \leq l \leq k$ and $t \in \mathbb{N}$ we assume that $\sum\limits_{i=1}^{m} w^i_l(t) > 0$, or equivalently that for all $t \in \mathbb{N} \: \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t) \right\rbrace > 0$.
\end{assumption}
	
\begin{remark} 
	\begin{enumerate}[(i)] \label{StateEq15}
%		\item $\alpha(t)$ is the step-size, and it must be positive. If at some step $t \in \mathbb{N}$, $\alpha(t)=0$, then there exists $1 \leq l' \leq k$ such that $\beta(t) = \sum\limits_{i=1}^{m} w^i_{l'}(t) = 0$. Since for all $1 \leq l \leq k$ and $1 \leq i \leq m$ we know that $w^i_l(t) \geq 0$ then $w^i_{l'}(t)=0$ for each $1 \leq i \leq m$. Thus, none of the points in $\mathcal{A}$ belong to cluster $l'$, in that case the algorithm can halt. Hence from now on we assume that for all $t \in \mathbb{N}$, $\beta(t) = \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t)\right \rbrace > 0$, and thus $\alpha(t) > 0 $.
		\item Since for all $t \in \mathbb{N}$ we have that $w(t) \in \Delta^m$ then $G(w(t))=0$ and therefore $\Psi(z(t)) = H(w(t),x(t))$. \label{StateEq16}
		\item For any choice of distance-like function $d(\cdot, \cdot)$, the function $x \mapsto H(w,x)$ is separable in $x^l$ for all $l = 1, 2, \ldots, k$. Thus, regardless the choice of distance-like function $d(\cdot, \cdot)$, the centers update step can be done in parallel over all centers, that is, $x^l(t+1) = \arg\min\limits_{x^l \in \mathbb{R}^k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t) d(x^l , a^i) \right\rbrace, \\ l = 1, 2, \ldots, k$, and in the case of the squared Euclidean norm the result is given in (\ref{StateEq8}).
	\end{enumerate}
\end{remark}

\begin{lemma}[Boundedness of KPALM sequence]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace (w(t) , x(t)) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM. Then, the following statements hold true.
\begin{enumerate}[(i)]
	\item For all $l=1, 2, \ldots ,k$, the sequence $\left\lbrace x^l(t) \right\rbrace_{t \in \mathbb{N}}$ is contained in $Conv(\mathcal{A})$, where $Conv(\mathcal{A})$ is the convex hull of $\mathcal{A}$.
	\item For all $l=1, 2, \ldots ,k$, the sequence $\left\lbrace x^l(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded by $M = \max\limits_{1 \leq i \leq m} \| a^i \|$.
	\item The sequence $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded in $\mathbb{R}^{km} \times \mathbb{R}^{nk}$.
\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}[(i)]
	\item  Set $\lambda_i = \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)}, i=1, 2, \ldots ,m$, then $\lambda_i \geq 0$ and $\sum\limits_{i=1}^{m} \lambda_i = 1$. From (\ref{StateEq8}) we have
	\begin{equation*}
		x^l(t) = \frac{\sum_{i=1}^{m} w^i_l(t) a^i}{\sum_{i=1}^{m} w^i_l(t)} 
		= \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) a^i 
		= \sum\limits_{i=1}^{m} \lambda_i a^i \in Conv(\mathcal{A}).
	\end{equation*}
	Hence $x^l(t)$ is in the convex hull of $\mathcal{A}$, for all $l = 1, 2, \ldots, k$ and $t \in \mathbb{N}$.

	\item
	Taking the norm of $x^l(t)$ yields again from (\ref{StateEq8}) that
	\begin{equation*}
		\| x^l(t) \| = \left\lVert \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) a^i \right\lVert
		\leq \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) \| a^i \|
		\leq \sum_{i=1}^{m} \lambda_i \max\limits_{1 \leq i \leq m} \| a^i \| = M .
	\end{equation*}
	\item The sequence $\left\lbrace w(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded, since $w^i(t) \in \Delta$ for all $i=1, 2, \ldots ,m$ and $t \in \mathbb{N}$. Combined with the previous item, the result follows.
\end{enumerate} 
\end{proof}

\begin{lemma}[Strong convexity of $H(w,x)$ in $x$] \label{StateEq14}
The function $x \mapsto H(w,x)$ is strongly convex with parameter $\beta(w) = 2 \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l\right \rbrace$, whenever $\beta(w) > 0$.
\end{lemma}

\begin{proof}
Since the function $x \mapsto H(w(t),x) = 
\sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is $C^2$, it is strongly convex if and only if the smallest eigenvalue of the corresponding Hessian matrix is positive. Thus

\begin{center}
$\nabla_{x^j} \nabla_{x^l} H(w,x) = 
\begin{cases} 0 &\mbox{if } j \neq l, \quad 1 \leq j,l \leq k ,
\\ 2\sum\limits_{i=1}^{m} w^i_l &\mbox{if } j = l, \quad 1 \leq j,l \leq k. \end{cases} $
\end{center}

Since the Hessian is a diagonal matrix, the smallest eigenvalue is $\min\limits_{1 \leq l \leq k} 2\sum\limits_{i=1}^{m} w^i_l = \beta(w)$, and the result follows.
\end{proof}

Now we are ready to prove the decrease property of KPALM algorithm.

\begin{proposition}[Sufficient decrease property]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace \left( w(t) , x(t) \right) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then there exists $\rho_1 > 0$ such that 
\begin{equation*}
	\rho_1 \|z(t+1) - z(t)\|^2 \leq \Psi(z(t)) - \Psi(z(t+1)), \quad \forall t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
From (\ref{StateEq5}) we derive the following inequality
\begin{equation*}
\begin{aligned}
	H_i(w(t+1),x(t)) + \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 
	& = \langle w^i(t+1) , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \langle w^i(t) , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i(t) - w^i(t)\|^2 \\
	& = \langle w^i(t) , d^i(x(t)) \rangle \\
	& = H_i(w(t),x(t)) .
\end{aligned}
\end{equation*}
Hence, we obtain
\begin{equation}
	\frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 
	\leq H_i(w(t),x(t)) - H_i(w(t+1),x(t)) . \label{StateEq18}
\end{equation}
Denote $\alpha(t) = \min\limits_{1 \leq i \leq m} \left\lbrace \alpha_i(t) \right\rbrace$. Summing inequality (\ref{StateEq18}) over $i=1, 2, \ldots ,m$ yields
\begin{equation*}
\begin{aligned}
	\frac{\alpha(t)}{2} \|w(t+1) - w(t)\|^2 
	& = \frac{\alpha(t)}{2} \sum\limits_{i=1}^{m} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \sum\limits_{i=1}^{m} \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \sum\limits_{i=1}^{m} H_i(w(t),x(t)) - \sum\limits_{i=1}^{m} H_i(w(t+1),x(t)) \\
	& = H(w(t),x(t)) - H(w(t+1),x(t)) . \\
\end{aligned}
\end{equation*}

From \Cref{StateEq17} we have that $\beta(w(t)) = 2 \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t) \right \rbrace > 0$, and from \Cref{StateEq14} it follows that the function $x \mapsto H(w(t),x)$ is strongly convex with parameter $\beta(w(t))$, hence it follows that

\begin{equation*}
\begin{aligned}
	H(w(t+1),x(t)) & - H(w(t+1),x(t+1)) \geq \\
	& \geq \left\langle \nabla_x H(w(t+1),x(t+1)) , x(t)-x(t+1) \right\rangle + \frac{\beta(w(t))}{2} \|x(t) - x(t+1)\|^2 \\
	& = \frac{\beta(w(t))}{2} \|x(t+1) - x(t)\|^2 , \\
\end{aligned}
\end{equation*}
where the last equality follows from (\ref{StateEq6}), since $\nabla_{x} H(w(t+1), x(t+1)) = 0$.
Set \\$\rho_1 = \frac{1}{2}\min\left\lbrace \alpha(t) , \beta(w(t)) \right\rbrace$, combined with the previous inequalities, we have

\begin{equation*}
\begin{aligned}
	\rho_1 \|z(t+1) &- z(t)\|^2 
	 = \rho_1 \left( \|w(t+1) - w(t)\|^2 + \|x(t+1) - x(t)\|^2  \right) \leq \\
	& \leq \left[ H(w(t),x(t)) - H(w(t+1),x(t)) \right] + \left[ H(w(t+1),x(t)) - H(w(t+1),x(t+1)) \right] \\
	& = H(z(t)) - H(z(t+1)) = \Psi(z(t)) - \Psi(z(t+1)),
\end{aligned}
\end{equation*}
where the last equality follows from \Cref{StateEq15}(\ref{StateEq16}).
\end{proof}

Next, we aim to prove the subgradient lower bound for iterates gap property. The following lemma will be essential in our proof.

\begin{lemma} \label{StateEq11}
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace (w(t) , x(t)) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then 
\begin{equation*}
	\| d^i(x(t+1) - d^i(x(t)) \| \leq 4M \| x(t+1) - x(t)\|, \quad \forall i=1, 2, \ldots ,m, \: t \in \mathbb{N} ,
\end{equation*}
where $M = \max\limits_{1 \leq i \leq m} \|a^i\|$.
\end{lemma}

\begin{proof}
Since $d(u,v) = \| u-v \|^2$, we get that
{\allowdisplaybreaks
\begin{align*} 
	\| d^i(x(t&+1)  - d^i(x(t)) \| 
	 = \left[ \sum\limits_{l=1}^{k} \abs{ \|x^l(t+1) - a^i\|^2 - \| x^l(t) -a^i\|^2 }^2 \right]^{\frac{1}{2}} \\
	& = \left[ \sum\limits_{l=1}^{k} \left\lvert \|x^l(t+1)\|^2 - 2\left\langle x^l(t+1),a^i \right\rangle + \|a^i\|^2 - \|x^l(t)\|^2 + 2\left\langle x^l(t),a^i \right\rangle - \|a^i\|^2 \right\rvert ^2 \right]^{\frac{1}{2}} \\ 
	& \leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \|x^l(t+1)\|^2 - \|x^l(t)\|^2 } + \abs{ 2\left\langle x^l(t) - x^l(t+1) , a^i \right\rangle } \right)^2 \right]^{\frac{1}{2}} \\ 
	& \leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \|x^l(t+1)\| - \|x^l(t)\| } \cdot \abs{ \|x^l(t+1)\| + \|x^l(t)\| } + 2 \| x^l(t) - x^l(t+1) \| \cdot \|a^i\| \right)^2 \right]^{\frac{1}{2}} \\
	& \leq \left[ \sum\limits_{l=1}^{k} \left( \|x^l(t+1) - x^l(t)\| \cdot 2M + 2 \| x^l(t+1) - x^l(t) \| M \right)^2 \right]^{\frac{1}{2}} \\
	& = \left[ \sum\limits_{l=1}^{k} (4M)^2 \|x^l(t+1) - x^l(t)\|^2 \right]^{\frac{1}{2}} 
	= 4M \| x(t+1) - x(t)\| ,
\end{align*}
}
this proves the desired result.
\end{proof}

\begin{proposition}[Subgradient lower bound for iterates gap property]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace (w(t) , x(t)) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then there exists $\rho_2 > 0$ and $\gamma(t+1) \in \partial \Psi(z(t+1))$ such that 
\begin{equation*}
	\| \gamma(t+1)\| \leq \rho_2 \|z(t+1) - z(t)\|, \quad \forall t \in \mathbb{N} .
\end{equation*}

\end{proposition}

\begin{proof}
By the definition of $\Psi$ (see (\ref{StateEq4})) we get
\begin{equation*}
	\partial \Psi = \nabla H + \partial G  
= \left( \left( \nabla_{w^i} H_i + \partial_{w^i} \delta_{\Delta} \right)_{i=1, \ldots ,m} , \nabla_x H \right) .
\end{equation*}
Evaluating the last relation at $z(t+1)$ yields
\begin{equation*}
\begin{aligned}
	\partial \Psi(z(t & + 1)) = \\
	& = \left( \left( \nabla_{w^i} H_i(w(t+1),x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \ldots ,m} , \nabla_x H(w(t+1),x(t+1)) \right) \\
	& = \left( \left( d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \ldots ,m} , \nabla_x H(w(t+1),x(t+1)) \right) \\
	& = \left( \left( d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \ldots ,m} , \mathbf{0} \right) ,
\end{aligned}
\end{equation*}
where the last equality follows from (\ref{StateEq6}), that is, the optimality condition of $x(t+1)$. Taking the norm of the last equality yields
\begin{equation}
	\| \partial \Psi(z(t+1))\| 
	\leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \|. \label{StateEq9}
\end{equation}
The optimality condition of $w^i(t+1)$ that is derived from (\ref{StateEq5}), yields that for all $i=1, 2, \ldots ,m$ there exists $u^i(t+1) \in \partial \delta_{\Delta}(w^i(t+1))$ such that
\begin{equation}
	d^i(x(t)) + \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) + u^i(t+1) = \mathbf{0} . \label{StateEq10}
\end{equation}
Setting $\gamma(t+1) := \left( \left( d^i(x(t+1)) + u^i(t+1) \right)_{i=1, \ldots ,m}, \mathbf{0} \right) \in \partial \Psi(z(t+1))$, and plugging (\ref{StateEq10}) into (\ref{StateEq9}) we have
\begin{equation*}
\begin{aligned}
	\| \gamma(t+1) \|
	& \leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) - \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) \| \\
	& \leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) \| + \sum\limits_{i=1}^{m} \alpha_i(t) \| w^i(t+1) - w^i(t) \| \\
	& \leq \sum\limits_{i=1}^{m} 4M \| x(t+1) - x(t) \| + m \overline{\alpha}(t) \|z(t+1) - z(t)\| \\
	& \leq m \left( 4M + \overline{\alpha}(t) \right) \|z(t+1) - z(t)\| , \\
\end{aligned}
\end{equation*}
where the third inequality follows from \Cref{StateEq11}, and $\overline{\alpha}(t) = \max\limits_{1 \leq i \leq m} \alpha_i(t)$. Define \\$\rho_2 = m \left( 4M + \overline{\alpha}(t) \right)$ and the result follows.
\end{proof}


\subsection{Similarity to KMEANS}
The famous KMEANS algorithm has close proximity to KPALM algorithm. KMEANS alternates between cluster assignments and center updates as well. In detail, we can write its steps in the following manner

\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random centers $y(0) \in \mathbb{R}^{nk}$.

	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace(v(t),y(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: For $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			v^i(t+1) = \arg\min\limits_{v^i \in \Delta} \left\lbrace \langle v^i , d^i(y(t)) \rangle\right\rbrace . \label{StateEq12}
		\end{equation}
		
		\item Center Update: For $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			y^l(t+1) = \frac{\sum_{i=1}^{m} v^i_l(t+1) a^i}{\sum_{i=1}^{m} v^i_l(t+1)} . \label{StateEq13}
		\end{equation}
	\end{enumerate}
\end{enumerate}
The KMEANS algorithm obviously resemble KPALM algorithm. Denote $\overline{\alpha}(t) = \max\limits_{1 \leq i \leq m} \alpha_i(t)$. Assuming same starting point $x(0) = y(0)$ and by taking $\overline{\alpha}(t) \to 0$, we have
\begin{equation*}
	v(t) = \lim_{\overline{\alpha}(t) \to 0} w(t), \quad
	y(t) = \lim_{\overline{\alpha}(t) \to 0} x(t),
\end{equation*}
meaning, both algorithms converge to the same result.

\subsection{KMEANS Convergence Proof}

We start with rewriting the KMEANS algorithms, in its most familiar form
\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random centers $x(0) \in \mathbb{R}^{nk}$.

	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace (C(t),x(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: For $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			C^l(t+1) = \left\lbrace a \in \mathcal{A} \mid \| a - x^l(t) \| \leq \|a - x^j(t) \|, \quad \forall 1 \leq l \leq k \right\rbrace . \label{StateEq20}
		\end{equation}
		
		\item Center Update: For $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			x^l(t+1) = mean(C^l(t)) := \frac{1}{\left| C^l(t) \right|} \sum\limits_{a \in C^l(t)} a . \label{StateEq21}
		\end{equation}
		
		\item Stopping criteria: Halt if 
		\begin{equation}
			\forall 1 \leq l \leq k \quad C^l(t+1)=C^l(t) \label{StateEq22}
		\end{equation}
	\end{enumerate}
\end{enumerate}
As in KPALM, KMEANS needs \Cref{StateEq17} for step (\ref{StateEq21}) to be well defined. In order to prove the convergence of KMEANS to local minimum, we will need to following assumption.

\begin{assumption} \label{StateEq23}
For any step $t \in \mathbb{N}$, each $a \in \mathcal{A}$ belongs exclusively to single cluster $C^l(t)$.
\end{assumption}

For any $x \in \mathbb{R}^{nk}$ we denote the super-partition of $\mathcal{A}$ with respect to $x$ by $\overline{C^l}(x) = \left\lbrace a \in \mathcal{A} \mid \right. \\ \left. \|a - x^l\| \leq \|a - x^j\| , \quad \forall j \neq l \right\rbrace$, for all $1 \leq l \leq k$, and the sub-partition of $\mathcal{A}$ by $\underline{C^l}(x) = \left\lbrace a \in \mathcal{A} \mid \right. \\ \left. \|a - x^l\| < \|a - x^j\|, \quad \forall j \neq l \right\rbrace$.
Moreover, denote $R_{lj}(t) = \min\limits_{a \in C^l(t)} \left\lbrace \|a - x^j(t)\| - \|a - x^l(t)\| \right\rbrace$ for all $1 \leq l,j \leq k$, and $r(t) = \min\limits_{l \neq j} R_{lj}$. \\
Due to \Cref{StateEq23} we have that $\overline{C^l}(x(t)) = \underline{C^l}(x(t)) = C^l(t+1)$, for all $1 \leq l \leq k, \: t \in \mathbb{N}$, we also have that $r(t) > 0$ for all $t \in \mathbb{N}$.

\begin{proposition} \label{StateEq24}
Let $(C(t), x(t))$ be the clusters and centers KMEANS returns. Denote an open neighbourhood of $x(t)$ by $U = B\left( x^1(t),\frac{r(t)}{2}\right) \times  B\left( x^2(t),\frac{r(t)}{2}\right) \times \cdots \times B\left( x^l(t),\frac{r(t)}{2} \right)$, then for any $x \in U$ we have $\underline{C^l}(x)= C^l(t)$ for all $1 \leq l \leq k$.
Let $(C(t), x(t))$ be the clusters and centers KMEANS returns. Denote by $U = B\left( x^1(t),\frac{r(t)}{2}\right) \times  B\left( x^2(t),\frac{r(t)}{2}\right) \times \cdots \times B\left( x^l(t),\frac{r(t)}{2} \right)$ an open neighbourhood of $x(t)$, then for any $x \in U$ we have $C^l(t) = \underline{C^l}(x)$ for all $1 \leq l \leq k$.
\end{proposition}

\begin{proof}
Pick some $a \in C^l(t)$, then $x^l(t-1)$ is the closest center among the centers of $x(t-1)$. Since KMEANS halts at step $t$, then from (\ref{StateEq22}) we have $x(t)=x(t-1)$, thus $x^l(t)$ is the closest center to $a$ among the centers of $x(t)$. Further we have
\begin{equation}
	r(t) \leq \|x^j(t) - a\| - \|x^l(t) -a\| \quad \forall j \neq l . \label{StateEq25}
\end{equation}
Next, we show that $a \in \underline{C^l}(x)$, indeed
\begin{equation*}
\begin{aligned}
	\|a - x^l\| -  \|a - x^j\| &\leq \|a - x^l(t)\| + \|x^l(t) - x^l\| - \left( \|a - x^j(t)\| - \|x^j(t) - x^j\| \right) \\
	& = \|a - x^l\| - \|a - x^j(t)\| + \|x^l(t) - x^l\| + \|x^j(t) - x^j\| \\
	& < \|a - x^l\| - \|a - x^j(t)\| + r(t) \\
	& \leq -r(t) + r(t) = 0 ,
\end{aligned}
\end{equation*}
where the second inequality holds since $x^l \in B\left( x^l(t), \frac{r(t)}{2} \right)$ and $x^j \in B\left( x^j(t), \frac{r(t)}{2} \right)$, and the third inequality follows from (\ref{StateEq25}), and we get that $C^l(t) \subseteq \underline{C^l}(x)$. 
By definition of $\underline{C^l}(x)$ we have that for any $l \neq j, \: \underline{C^l}(x) \cap \underline{C^j}(x)=\emptyset$, and for all $1 \leq l \leq k, \: \underline{C^l}(x) \subseteq \mathcal{A}$. Now, since $C(t)$ is a partition of $\mathcal{A}$, then $C^l(t) = \underline{C^l}(x)$ for all $1 \leq l \leq k$.
\end{proof}

\begin{proposition}[KMEANS converges to local minimum]
Let $(C(t), x(t))$ be the clusters and centers KMEANS returns, then $x(t)$ is local minimum of $F$ in $U = B\left( x^1(t),\frac{r(t)}{2}\right) \times  B\left( x^2(t),\frac{r(t)}{2}\right) \\ \times \cdots \times B\left( x^l(t),\frac{r(t)}{2} \right) \subset \mathbb{R}^{nk}$.
\end{proposition}

\begin{proof}
The minimum of $F$ in $U$ is
\begin{equation*}
\min\limits_{x \in U} F(x) = \min\limits_{x \in U} \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(x)} \|a - x^l \|^2 = \min\limits_{x \in U} \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(t)} \|a - x^l \|^2 ,
\end{equation*}
where the last equality follows from \Cref{StateEq24}.\\
The function $x \mapsto \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(t)} \|a - x^l \|^2$ is strictly convex, separable in $x^l$ for all $1 \leq l \leq k$, and reaches its minimum at $\left( x^l \right)^{*} = \frac{1}{\left| C^l(t) \right|} \sum\limits_{a \in C^l(t)} a = mean(C^l(t)) = x^l(t),$ and the result follows.
\end{proof}

\section{Clustering via Alternation with Weiszfeld Step}

In this section we tackle the clustering problem with distance-like function being the Euclidean norm in $\mathbb{R}^n$, namely
\begin{equation}
	\min_{x^1, x^2, \ldots, x^k \in \mathbb{R}^n} \left\lbrace \sum\limits_{i=1}^{m} \min\limits_{1 \leq l \leq k} \|x^l - a^i\| \right\rbrace . \label{StateEq30}
\end{equation}

In the previous sections we showed that (\ref{StateEq30}) has the following equivalent form
\begin{equation*}
	\min \left\lbrace \Psi(z) := H(w,x) + G(w) \mid z := (w,x) \in \mathbb{R}^{km} \times \mathbb{R}^{nk} \right\rbrace ,
\end{equation*}
where $	H(w,x) = \sum\limits_{i=1}^{m} \left\langle w^i , d^i(x) \right\rangle
	= \sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \| x^l - a^i \| $ and $G(w) = \sum\limits_{i=1}^{m} \delta_{\Delta}(w^i)$.

\subsection{The Smoothed Fermat-Weber Problem}
Solving the smoothed Fermat-Weber plays a significant role in the algorithm that addresses the clustering problem with Euclidean norm distance-like function.
The Fermat-Weber problem is formulated as follows
\begin{equation}
	\min_{x \in \mathbb{R}^n} \left\lbrace \sum\limits_{i=1}^{m} w_i\|x - a^i\| \right\rbrace , \label{StateEq60}
\end{equation}
where $w_i>0, \: i=1,2, \ldots, m$, are given positive weights and $a^1, a^2, \ldots, a^m \in \mathbb{R}^n$ are given vectors.



\subsection{Clustering with Weiszfeld Step}

We introduce some useful notations that will be useful for this section. 
For $1 \leq l \leq k$ denote $L_l(w,x)= \sum\limits_{i=1}^{m}\frac{w^i_l}{\|x_l - a^i\|}$ and $H_l(w,x)= \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|$. \\
Weiszfeld single iteration is defined for each $1 \leq l \leq k$ via
\begin{equation}
T_l(w,x) = \frac{\sum\limits_{i=1}^{m}\frac{w^i_la^i}{\|x^l - a^i\|}}{\sum\limits_{i=1}^{m}\frac{w^i_l}{\|x^l - a^i\|}} . \label{StateEq31}
\end{equation} \\
Next we present our algorithm for solving problem (\ref{StateEq30}).  The algorithm alternates between clusters assignment step, which is exactly as in KPALM, and centers update step that is based on a single Weiszfeld iteration.

\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random vectors $(w(0),x(0)) \in \Delta^m \times \mathbb{R}^{nk} .$

	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace(w(t),x(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: Take any $\alpha_i(t) > 0$ and for each $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			w^i(t+1) = \arg\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i - w^i(t)\|^2 \right\rbrace . \label{StateEq32}
		\end{equation}
		
		\item Centers Update: For each $l=1, 2, \ldots ,k$ compute $x^l \in \mathbb{R}^n$ via
		\begin{equation}
			x^l(t+1) = T_l(w(t+1),x(t)) . \label{StateEq33}
		\end{equation}
	\end{enumerate}
\end{enumerate}

\begin{assumption} \label{StateEq34}
For any step $t \in \mathbb{N}$ and for all $1 \leq l \leq k$, we assume that $x^l(t) \notin \mathcal{A}$ .
\end{assumption}

\begin{remark} 
	\begin{enumerate}[(i)]
	\item Due to \Cref{StateEq34} it follows that the centers update step in (\ref{StateEq33}) is well defined.
	\item It is easy to verify that for all $1 \leq l \leq k$ the following equations hold true:
	\begin{equation}
		\nabla_{x^l} H_l(w,x) = \sum\limits_{i=1}^{m} w^i_l \frac{x^l - a^i}{\|x^l - a^i\|} , \quad \forall x^l \notin \mathcal{A}, \label{StateEq35}
	\end{equation}
	and that 
	\begin{equation}
		T_l(w,x) = x^l - \frac{1}{L_l(w,x)}\nabla_{x^l} H_l(w,x), \quad \forall x^l \notin \mathcal{A} . \label{StateEq36}
	\end{equation}
	\end{enumerate}
\end{remark}

As in KPALM case, we aim to prove the the sufficient decrease property and subgradient lower bounds for iterates gap property. Note that
\begin{equation*}
x^l(t+1) = T_l(w(t+1),x(t)) 
= \frac{\sum\limits_{i=1}^{m}\frac{w^i_l(t+1)a^i}{\|x^l(t) - a^i\|}}{\sum\limits_{i=1}^{m}\frac{w^i_l(t+1)}{\|x^l(t) - a^i\|}} 
= \sum\limits_{i=1}^{m}\frac{\frac{w^i_l(t+1)}{\|x^l(t) - a^i\|}}{\sum\limits_{j=1}^{m}\frac{w^j_l(t+1)}{\|x^l(t) - a^j\|}} a^i \in Conv(\mathcal{A}) ,
\end{equation*}
hence the sequence generated by (Alg-Name) is bounded as well.

\begin{proposition}[Sufficient decrease property]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace \left( w(t) , x(t) \right) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by (Alg-Name), then there exists $\rho_1 > 0$ such that 
\begin{equation*}
	\rho_1 \|z(t+1) - z(t)\|^2 \leq \Psi(z(t)) - \Psi(z(t+1)) \quad \forall t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
In the proof of sufficient decrease property of KPALM we showed that
\begin{equation}
	\frac{\alpha(t)}{2} \|w(t+1) - w(t)\|^2 \leq H(w(t),x(t)) - H(w(t+1),x(t)) , \\ \label{StateEq37}
\end{equation}
where $\alpha(t) = \min\limits_{1 \leq i \leq m} \left\lbrace \alpha_i(t) \right\rbrace$. This part was proven independently of the distance function $d(\cdot, \cdot)$, and since in (Alg-Name) the clusters assignment step is identical to KPALM, the claim is correct in the case of (Alg-Name) as well.

Applying Lemma 4.2 from [reference to Weiszfeld paper] yields
\begin{equation*}
	\begin{split}
	H_l(w(t+1)&,x(t+1)) - H_l(w(t+1),x) \leq \\ 
	& \leq \frac{L_l(w(t+1),x(t))}{2} \left( \|x^l(t) - x^l\|^2 - \|x^l(t+1) - x^l\|^2 \right), \quad \forall x \in \mathbb{R}^{nk}, 1 \leq l \leq k .
	\end{split}
\end{equation*}
Setting $x = x(t)$ and rearranging yields
\begin{equation}
	\frac{L_l(w(t+1),x(t))}{2} \|x^l(t+1) - x^l(t)\|^2 \leq H_l(w(t+1),x(t)) - H_l(w(t+1),x(t+1)), \quad \forall 1 \leq l \leq k . \label{StateEq38}
\end{equation}

Denote ${L(t)}= \min\limits_{1 \leq l \leq k} \left\lbrace L_l(w(t+1),x(t)) \right\rbrace$. Summing (\ref{StateEq38}) over $l = 1,2, \ldots ,k$ leads to
\begin{equation}
	\begin{split}
	\frac{L(t)}{2} \|x(t+1) - x(t)\|^2 &= 
	\frac{L(t)}{2} \sum\limits_{l=1}^{k} \|x^l(t+1) - x^l(t)\|^2 \\
	& \leq \sum\limits_{l=1}^{k} \frac{L_l(t)}{2} \|x^l(t+1) - x^l(t)\|^2 \\
	& \leq \sum\limits_{l=1}^{k} \left( H_l(w(t+1),x(t)) - H_l(w(t+1),x(t+1)) \right) \\
	& = H(w(t+1),x(t)) - H(w(t+1),x(t+1)) .
	\end{split} \label{StateEq39}
\end{equation}

Set $\rho_1 = \frac{1}{2} \min \left\lbrace \alpha(t), L(t) \right\rbrace$, and then \Cref{StateEq17} assures that $\rho_1 > 0$. Combining (\ref{StateEq37}) and (\ref{StateEq39}) yields
\begin{equation*}
\begin{aligned}
	\rho_1 \|z(t+1) &- z(t)\|^2 
	 = \rho_1 \left( \|w(t+1) - w(t)\|^2 + \|x(t+1) - x(t)\|^2  \right) \leq \\
	& \leq \left[ H(w(t),x(t)) - H(w(t+1),x(t)) \right] + \left[ H(w(t+1),x(t)) - H(w(t+1),x(t+1)) \right] \\
	& = H(z(t)) - H(z(t+1)) = \Psi(z(t)) - \Psi(z(t+1)),
\end{aligned}
\end{equation*}
which proves the desired result.
\end{proof}

Next we prove the subgradient lower bounds for iterates gap property of the sequence generated by (Alg-Name), we start with proving two useful lemmas.

\begin{lemma} \label{StateEq40}
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace (w(t) , x(t)) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by (Alg-Name), then 
\begin{equation*}
	\| d^i(x(t+1) - d^i(x(t)) \| \leq \| x(t+1) - x(t)\|, \quad \forall i=1, 2, \ldots ,m, \: t \in \mathbb{N} .
\end{equation*}
\end{lemma}

\begin{proof}
Since $d(u,v) = \| u-v \|$, we get that
\begin{equation*}
\begin{aligned}
	\|d^i(x(t+1)) - d^i(x(t))\| 
	&= \left[ \sum_{l=1}^{k} \abs{ \| x^l(t+1) - a^i\| - \|x^l(t) - a^i\| }^2 \right]^\frac{1}{2} \\
	&\leq \left[ \sum_{l=1}^{k}  \left\lVert \left( x^l(t+1) - a^i \right) - \left( x^l(t) - a^i\right) \right\rVert^2 \right]^\frac{1}{2} \\
	&= \left[ \sum_{l=1}^{k} \| x^l(t+1) - x^l(t) \|^2 \right]^\frac{1}{2} = \|x(t+1) - x(t)\| .
\end{aligned}
\end{equation*}
\end{proof}

The following lemma is based on the results within [reference to Weiszfeld paper]. We remind the notations in that context. For non-negative weights $w_i \geq 0, i=1,2, \ldots, m$, and $m$ points $\mathcal{A} = \left\lbrace a^1, a^2, \ldots, a^m \right\rbrace \subset \mathbb{R}^{n}$, the functions $f:\mathbb{R}^n \rightarrow \mathbb{R}$, $h:\mathbb{R}^n \times \mathbb{R}^n \setminus \mathcal{A} \rightarrow \mathbb{R}$ and $L:\mathbb{R}^n \rightarrow \mathbb{R}$ are respectively defined by
\begin{equation*}
	f(x)= \sum\limits_{i=1}^{m} w_i \|x - a^i\|, \quad h(x,y):=\sum\limits_{i=1}^{m} w_i \frac{\|x - a^i\|^2}{\|y - a^i\|} , \quad L(x):=\sum\limits_{i=1}^{m} \frac{w_i}{\|x - a^i\|} .
\end{equation*}

\begin{lemma} \label{StateEq41}
 For all $y^0,y \in \mathbb{R}^n \setminus \mathcal{A}$ the following statement holds true
\begin{equation*}
	\| \nabla f(y) - \nabla f(y^0) \| \leq \frac{2L(y^0)L(y)}{L(y^0)+L(y)} \|y^0 - y\|.
\end{equation*}
\end{lemma}

\begin{proof}
Let $y^0 \in \mathbb{R}^n \setminus \mathcal{A}$ be a fixed vector. Define the following function
\begin{equation*}
	\widetilde{f}(y) = f(y) - \left\langle \nabla f(y^0), y \right\rangle ,
\end{equation*}
and similarly to the paper [reference to Weiszfeld paper] we define
 \begin{equation*}
	\widetilde{h}(x,y) = h(x,y) - \left\langle \nabla f(y^0), x \right\rangle .
\end{equation*}
It is clear that $x \mapsto \widetilde{h}(x,y)$ is still quadratic function with associated matrix $L(y)\mathbf{I}$. Therefore, we can write
\begin{equation}
\begin{aligned}
	\widetilde{h}(x,y) &= \widetilde{h}(y,y) + \left\langle \nabla_x \widetilde{h}(y,y), x-y \right\rangle + L(y) \|x-y\|^2 \\
	&= \widetilde{f}(y) + \left\langle 2\nabla f(y) - \nabla f(y^0), x-y \right\rangle + L(y) \|x-y\|^2.	\label{StateEq42}
\end{aligned}
\end{equation}
On the other hand, from [BS2015, Lemma 3.1(ii), Page 7] we have that
\begin{equation}
\begin{aligned}
	\widetilde{h}(x,y) &= h(x,y) - \left\langle	\nabla f(y^0),x \right\rangle \geq 2f(x) - f(y) - \left\langle \nabla f(y^0),x \right\rangle \\
	&= 2 \widetilde{f}(x) - \widetilde{f}(y) + \left\langle \nabla f(y^0), y-x \right\rangle, \label{StateEq43}
\end{aligned}
\end{equation}
where the last equality follows from the definition of $\widetilde{f}$. Combining (\ref{StateEq42}) and (\ref{StateEq43}) yields
\begin{equation*}
\begin{aligned}
	2\widetilde{f}(x) &\leq 2\widetilde{f}(y) + 2 \left\langle \nabla f(y) - \nabla f(y^0), x-y \right\rangle + L(y) \|x-y\|^2 \\
	&= 2\widetilde{f}(y) + 2 \left\langle \nabla \widetilde{f}(y), x-y \right\rangle + L(y) \|x-y\|^2.
\end{aligned}
\end{equation*}
Dividing the last inequality by $2$ leads to
\begin{equation}
	\widetilde{f}(x) \leq \widetilde{f}(y) + \left\langle \nabla \widetilde{f}(y), x-y \right\rangle + \frac{L(y)}{2} \|x-y\|^2. \label{StateEq44}
\end{equation}
It is clear that the optimal point of $\widetilde{f}$ is $y^0$ since $\nabla \widetilde{f}(y^0) = 0$, therefore from (\ref{StateEq44}) we obtain
\begin{equation*}
\begin{aligned}
	\widetilde{f}(y^0) &\leq \widetilde{f}\left( y - \frac{1}{L(y)} \nabla \widetilde{f}(y) \right) \leq \widetilde{f}(y) + \left\langle \nabla \widetilde{f}(y), - \frac{1}{L(y)} \nabla \widetilde{f}(y) \right\rangle + \frac{L(y)}{2} \left\lVert \frac{1}{L(y)} \nabla \widetilde{f}(y) \right\rVert ^2 \\
	&= \widetilde{f}(y) - \frac{1}{2 L(y)} \left\lVert \nabla \widetilde{f}(y) \right\rVert ^2.
\end{aligned}
\end{equation*}
Thus, using the definition of $\widetilde{f}$ and the face that $\nabla \widetilde{f}(y) = \nabla f(y) - \nabla f(y^0)$, yields that
\begin{equation*}
	f(y^0) \leq f(y) + \left\langle \nabla f(y^0), y^0 - y \right\rangle - \frac{1}{2 L(y)} \| \nabla f(y) - \nabla f(y^0) \|^2 .
\end{equation*}
Now, following the same arguments we can show that
\begin{equation*}
	f(y) \leq f(y^0) + \left\langle \nabla f(y), y - y^0 \right\rangle - \frac{1}{2 L(y^0)} \| \nabla f(y^0) - \nabla f(y) \|^2
\end{equation*}
and combining last two inequalities yields that
\begin{equation*}
	\left( \frac{1}{2 L(y^0)} + \frac{1}{2 L(y)} \right) \| \nabla f(y) - \nabla f(y^0) \|^2 \leq \left\langle \nabla f(y^0) - \nabla f(y), y^0 - y \right\rangle ,
\end{equation*}
that is 
\begin{equation*}
	\| \nabla f(y) - \nabla f(y^0) \| \leq \frac{2L(y^0)L(y)}{L(y^0) + L(y)} \|y^0 - y\| ,
\end{equation*}
for all $y^0,y \in \mathbb{R}^n \setminus \mathcal{A}$.
\end{proof}

\begin{proposition}[Subgradient lower bound for iterates gap property]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace (w(t) , x(t)) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by (Alg-Name), then there exists $\rho_2 > 0$ and $\gamma(t+1) \in \partial \Psi(z(t+1))$ such that 
\begin{equation*}
	\| \gamma(t+1)\| \leq \rho_2 \|z(t+1) - z(t)\|, \quad \forall t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
Repeating the steps of the proof in the case of KPALM yields that 
\begin{equation}
	\gamma(t+1) := \left( \left( d^i(x(t+1)) + u^i(t+1) \right)_{i=1, \ldots ,m}, \nabla_x H(w(t+1),x(t+1)) \right) \in \partial \Psi(z(t+1)) , \label{StateEq45}
\end{equation}
where for all $1 \leq i \leq m, \: u^i(t+1) \in \partial \delta_{\Delta}(w^i(t+1))$ such that
\begin{equation}
	d^i(x(t)) + \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) + u^i(t+1) = \mathbf{0} . \label{StateEq46}
\end{equation}
Plugging (\ref{StateEq46}) into (\ref{StateEq45}), and taking norm yields
\begin{equation*}
\begin{aligned}
	\| \gamma(t+1) \|
	&\leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) - \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) \| + \| \nabla_x H(w(t+1),x(t+1)) \| \\
	&\leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) \| + \sum\limits_{i=1}^{m} \alpha_i(t) \| w^i(t+1) - w^i(t) \| + \| \nabla_x H(w(t+1),x(t+1)) \| \\
	&\leq m \|x(t+1) - x(t)\| + m\overline{\alpha}(t) \|w(t+1) - w(t)\| + \| \nabla_x H(w(t+1),x(t+1)) \|,
\end{aligned}
\end{equation*}
where the last inequality follows from \Cref{StateEq40} and the fact that $\overline{\alpha}(t) = \max\limits_{1 \leq i \leq m} \alpha_i(t)$. \\ 
Next we will derive the following bound $\| \nabla_x H(w(t+1),x(t+1)) \| \leq c\|x(t+1) - x(t)\|$, for some constant $c>0$. Indeed, we have
\begin{equation}
\begin{aligned}
	&\| \nabla_x H(w(t+1),x(t+1)) \| \leq \sum\limits_{l=1}^{k} \| \nabla_{x^l} H_l(w(t+1),x(t+1)) \| \\
	&\leq \sum\limits_{l=1}^{k} \| \nabla_{x^l} H_l(w(t+1),x(t)) \| + \sum\limits_{l=1}^{k} \| \nabla_{x^l} H_l(w(t+1),x(t+1)) - \nabla_{x^l} H_l(w(t+1),x(t))\| . \label{StateEq47}
\end{aligned}
\end{equation}
From (\ref{StateEq36}) we have
\begin{equation*}
\nabla_{x^l} H_l(w(t+1),x(t)) = L_l(w(t+1),x(t)) \left( x^l(t+1) - x^l(t) \right) , \quad \forall 1 \leq l \leq k,
\end{equation*}
applying \Cref{StateEq41} with respect to $H_l(w(t+1),\cdot)$ and plugging into (\ref{StateEq47}) yields
\begin{equation*}
\begin{aligned}
	&\| \nabla_x H(w(t+1),x(t+1)) \| \leq \\
	&\leq \sum\limits_{l=1}^{k} \left( L_l(w(t+1),x(t)) + \frac{2 L_l(w(t+1),x(t)) L_l(w(t+1),x(t+1))}{L_l(w(t+1),x(t)) + L_l(w(t+1),x(t+1))} \right) \|x^l(t+1) - x^l(t)\| .
\end{aligned}
\end{equation*}
Therefore, denote $\overline{L}(t) = \max\limits_{1 \leq l \leq k} \left\lbrace L_l(w(t+1),x(t)) + \frac{2 L_l(w(t+1),x(t)) L_l(w(t+1),x(t+1))}{L_l(w(t+1),x(t)) + L_l(w(t+1),x(t+1))} \right\rbrace$, and set \\ $\rho_2 = m + m\overline{\alpha}(t) + k\overline{L}(t)$, and the result follows.
\end{proof}

\subsection{Refactoring the Method}
In this section we use the modification of the Weiszfeld method as presented in [reference to Weiszfeld paper]. We will show that this approach allows us to drop \Cref{StateEq34} and treat the cases when $x^l(t) \in \mathcal{A}$ for some $1 \leq l \leq k$ and $t \in \mathbb{N}$.

At first we return to the notations that are used in [reference to Weiszfeld paper]. We remind that in order to solve a single update of cluster center we need to solve the following minimization problem
\begin{equation}
	\min\limits_{x \in \mathbb{R}^n} \left\lbrace f(x) = \sum\limits_{i=1}^{m} w_i \|x - a_i\| \right\rbrace . \label{StateEq50}
\end{equation}
The operator that is invloved in solving this problem $\tilde{T}: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is defined as follows
\begin{equation*}
	\tilde{T}(x) = \begin{cases} T(x), \quad &x \notin \mathcal{A}, \\ 
a_j, \quad &x=a_j \: (1 \leq j \leq m) \mbox{ and } \|R_j\| \leq w_j, \\
S(a_j), \quad &x=a_j \: (1 \leq j \leq m) \mbox{ and } \|R_j\| > w_j, \end{cases}
\end{equation*}
where the operator $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is defined by
\begin{equation*}
	T(x) = \frac{\sum\limits_{i=1}^{m}\frac{w_i a_i}{\|x - a_i\|}}{\sum\limits_{i=1}^{m}\frac{w_i}{\|x - a_i\|}} ,
\end{equation*}
the values $R_j$ are computed via
\begin{equation*}
	R_j= \sum\limits_{i=1,i \neq j} w_i\frac{a_j - a_i}{\|a_i - a_j\|}, \quad j=1,2, \ldots ,m ,
\end{equation*}
and the operator $S: \mathcal{A} \rightarrow \mathbb{R}^n$ is defined as follows
\begin{equation*}
	S(a_j)= a_j + t_j d_j,
\end{equation*}
with the direction $d_j = -\frac{R_j}{\|R_j\|}$ and the stepsize $t_j = \frac{\|R_j\| - w_j}{L(a_j)}$.
The function $L: \mathbb{R}^n \rightarrow \mathbb{R}$ that is used in the definition of the stepsize $t_j$ is also extended to $\mathcal{A}$  in the following manner
\begin{equation*}
	L(x)= \begin{cases} \sum\limits_{i=1}^{m} \frac{w_i}{\|x - a_i\|}, \quad &x \notin \mathcal{A}, \\ 
\sum\limits_{\substack{i=1 \\ i \neq j}}^{m} \frac{w_i}{\|a_j - a_i\|}, \quad &x=a_j \: (1 \leq j \leq m). \end{cases}
\end{equation*}

Based on these notations, the algorithm that is proposed in [reference to Weiszfeld paper] to solve the problem (\ref{StateEq50}) is called the Modified Weiszfeld Method, and it is a simple iterative application of operator $\tilde{T}$, as specified below.
\begin{enumerate}[(1)]
	\item Initialization: Pick random vector $x_0 \in \mathbb{R}^{n}.$
	\item General Step: For each $k=0,1, \ldots$ generate a sequence $x_{k+1} = \tilde{T}(x_k)$.
\end{enumerate}

\begin{lemma}
Let $\left\lbrace x_k \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by the Modified Weiszfeld Method, and assume that for some $k \in \mathbb{N}$ either $x_k \in \mathcal{A}$ or $x_{k+1} \in \mathcal{A}$, then the following statements hold true.
\begin{enumerate}[(i)]
	\item There exists $M_1 > 0$ such that $M_1 \|x_{k+1} - x_k\|^2 \leq f(x_k) - f(x_{k+1})$.
	\item There exist $M_2 > 0$ and $v_{k+1} \in \partial f(x_{k+1})$ such that $\|v_{k+1}\| \leq M_2 \|x_{k+1} - x_k\|$.
\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}[(i)]
	\item First assume that $x_k = a_j \in \mathcal{A}$. If $\|R_j\| \leq w_j$ then $x_{k+1} = a_j$ and the result follows. If $\|R_j\| > w_j$ then $x_{k+1} = S(a_j)$ then from [BS 2015, Lemma 7.1, page 20] we have that 
	\begin{equation*}
		f(x_k) - f(x_{k+1}) \geq \frac{L(a_j)}{2} {t_j}^2 = \frac{L(a_j)}{2} \|x_{k+1} - x_k\|^2.
	\end{equation*} \\
	Now, suppose $x_k \notin \mathcal{A}$, and $x_{k+1} = a_j \in \mathcal{A}$. Plugging $x=x_k$ in [BS 2015, Lemma 5.2, page 12] yields 
	\begin{equation*}
		f(x_{k+1}) - f(x_k) \leq \frac{L(x_k)}{2} \left( -\|x_{k+1} - x_k\|^2 \right).
	\end{equation*}
	Thus, setting $M_1 = L(x_k)$ gives the desired result for all cases.
	\item Again, first we inspect the case $x_k = a_j \in \mathcal{A}$. If $\|R_j\| \leq w_j$ then according to [BS 2015] $x=a_j$ is the optimal solution to problem (\ref{StateEq50}), hence $ \mathbf{0} \in \partial f(a_j)$ and the results follows. If $\|R_j\| > w_j$ then $x_{k+1} = S(a_j)$, hence 
	\begin{equation}
		0 < t_j = \|x_{k+1} - x_k\| . \label{StateEq51}
	\end{equation}
	Moreover, evaluating the subgradient of $f$ leads to
	\begin{equation*}
		\partial f(x) = \begin{cases} \left\lbrace \nabla f(x) \right\rbrace = \left\lbrace \sum\limits_{i=1}^{m} w_i \frac{x - a_i}{\|x - a_i\|} \right\rbrace, \quad &x \notin \mathcal{A}, \\ 
\left\lbrace \sum\limits_{\substack{i=1 \\ i \neq j}}^{m} w_i \frac{x - a_i}{\|a_j - a_i\|} + w_j z_j \mid \|z_j\| \leq 1 \right\rbrace, \quad &x=a_j \in \mathcal{A} \end{cases} .
	\end{equation*}
	Thus we have that
	\begin{equation}
		\| \partial f(x) \| \leq \sum\limits_{i=1}^{m} w_i \leq m .  \label{StateEq52}
	\end{equation}
	From (\ref{StateEq51}) and (\ref{StateEq52}) it follows that there exists $M_2 >0$ such that $\| \partial f(x) \| \leq M_2 \|x_{k+1} - x_k\|$. \\
	Now, assume that $x_k \notin \mathcal{A}$, and $x_{k+1} = a_j \in \mathcal{A}$. Thus $\|x_{k+1} - x_k\| > 0$ and combining the fact that $\| \partial f(x)\| \leq m$ then again we can assure that there exists $M_2 >0$ such that $\| \partial f(x) \| \leq M_2 \|x_{k+1} - x_k\|$. 
\end{enumerate}
\end{proof}

The last lemma allows us to drop \Cref{StateEq34} and bypass points in $\mathcal{A}$ by simply replacing the operator $T$ in center update step in (Alg-Name) (\ref{StateEq33}) with the operator $\tilde{T}$, and still reserve both, the sufficient decrease and the subgradient lower bound for iterates gap properties, that are essential for the conversion of the algorithm to some limit point.
\newpage

\section{Clustering via ADMM Approach}
Introducing some new variable into the problem leads to the following clustering problem notation
\begin{equation*}
\begin{split}
	&\min\limits_{x \in \mathbb{R}^{nk}} \min\limits_{w \in \mathbb{R}^{km}} \left\lbrace \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l d(x^l,a^i) \mid w^i \in \Delta, i=1,2, \ldots, m \right\rbrace \\
	&= \min\limits_{x \in \mathbb{R}^{nk}, w \in \mathbb{R}^{km}, z \in \mathbb{R}^{km}} \left\lbrace \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l z^i_l \: \bigg| 
\begin{array}{c c c}
 w^i \in \Delta, & i=1,2, \ldots, m, & \\
 z^i_l = d(x^l,a^i), & i=1,2, \ldots, m, & l=1,2, \ldots, k
\end{array}
\right\rbrace .
\end{split}
\end{equation*}

The augmented Lagrangian that is associated with this problem is 
\begin{equation}
	L_{\rho}(w,x,z,y) = \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l z^i_l + \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} y^i_l (z^i_l - d(x^l,a^i)) + \frac{\rho}{2} \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} \left(z^i_l - d(x^l,a^i)\right)^2 . \label{StateEq59}
\end{equation}
Thus the ADMM formulas for (\ref{StateEq60}) are as follows
\begin{equation*}
\begin{split}
	w(t+1) &= \arg\min\limits_{w \in \Delta^m} L_{\rho}(w,x(t),z(t),y(t)), \\
	\Rightarrow \quad w^i(t+1) &= \arg\min\limits_{w^i \in \Delta} \sum\limits_{l=1}^{k} w^i_l z^i_l(t) = \arg\min\limits_{w^i \in \Delta} \left\langle w^i, z^i(t) \right\rangle, \quad 1 \leq i \leq m , \\
	x(t+1) &= \arg\min\limits_{x \in \mathbb{R}^{nk}} L_{\rho}(w(t+1),x,z(t),y(t)), \\
	\Rightarrow \quad x^l(t+1) &= \arg\min\limits_{x^l \in \mathbb{R}^{nk}} -\sum\limits_{i=1}^{m} y^i_l(t) d(x^l, a^i) + \frac{\rho}{2} \sum\limits_{i=1}^{m} \left(z^i_l - d(x^l,a^i)\right)^2, \quad 1 \leq l \leq k , \\
	z(t+1) &= \arg\min\limits_{z \in \mathbb{R}^{km}} L_{\rho}(w(t+1),x(t+1),z,y(t)), \\
	\Rightarrow \quad z^i(t+1) &= \arg\min\limits_{z^i \in \mathbb{R}^{km}} \left\langle w^i(t+1), z^i \right\rangle +  \left\langle y^i(t), z^i \right\rangle + \frac{\rho}{2} \left\lVert z^i - \left( d(x^l(t+1), a^i \right)_{l=1, \ldots , k} \right\lVert^2 \\
	&= \left(d(x^l(t+1), a^i \right)_{l=1, \ldots , k} - \frac{1}{\rho}\left( w^i(t+1) + y^i(t) \right), \quad 1 \leq i \leq m, \\
	y^i_l(t+1) &= y^i_l(t) + \rho (z^i_l(t+1) - d(x^l(t+1), a^i), \quad 1 \leq i \leq m, \: 1 \leq l \leq k.
\end{split}
\end{equation*}

%\begin{thebibliography}{99}

%\end{thebibliography}

\end{document}
