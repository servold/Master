\documentclass[11pt]{article} 
\usepackage[american]{babel}
\usepackage{makeidx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{cleveref}

\oddsidemargin= -12pt \evensidemargin= -12pt
\topmargin=-45pt
\binoppenalty=10000
\relpenalty=10000
\textwidth=6.5in\textheight=9in \baselineskip=18pt
\parskip=6pt plus 1pt
\numberwithin{equation}{section}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[proposition]
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

\def\abs#1{\left\lvert#1\right\rvert}

\begin{document}

%\begin{center} 
%\Large {Tel-Aviv University}
%
%\Large {The Raymond and Beverly Sackler Faculty of} \\
%\Large {Exact Sciences} \\
%\Large {The Department of Statistics and Operations} \\
%\Large {Research}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {\bf Draft }
%
%\bigskip
%\bigskip
%\bigskip
%\bigskip
%
%\Large{Thesis Submitted Towards the Degree of Master} \\
%\Large {of Science in Operations Research}
%
%\bigskip
%
%\Large {Sergey Voldman}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {Supervisors:} \\
%\Large {Prof. Marc Teboulle} \\
%\Large {Prof. Shoham Sabach}
%
%\bigskip
%\bigskip
%
%\Large{2015}
%
%\end{center}
%
%\newpage
%
%\begin{center} \Large {\bf Draft}
%\end{center}
%
%\begin{abstract} 
%
%To-do...
%
%\newpage
%
%\end{abstract}
%
%\section{Introduction}
%
%To-do...
%

\newpage

\section{The Clustering Problem}

Let $\mathcal{A}= \left\lbrace a^1, a^2, \ldots ,a^m \right\rbrace$ be a given set of points in $\mathbb{R}^n$, and let $1 < k < m$ be a fixed given number of clusters. The clustering problem consists of partitioning the data $\mathcal{A}$ into $k$ subsets $\left\lbrace C^1, C^2, \ldots ,C^k \right\rbrace$, called clusters. For each $l=1, 2, \ldots ,k$, the cluster $C^l$ is represented by its center $x^l$, and we want to determine $k$ cluster centers $\left\lbrace x^1, x^2, \ldots ,x^k \right\rbrace$ such that the sum of proximity measures from each point $a^i, i=1, 2, \ldots ,m$, to a nearest cluster center $x^l$ is minimized.

The clustering problem is given by

\begin{equation}
	\min\limits_{x^1, x^2, \ldots ,x^k \in \mathbb{R}^n} F(x^1, x^2, \ldots ,x^k) := \sum\limits_{i=1}^{m} \min\limits_{1 \le l \le k} d(x^l,a^i) , \label{StateEq1}
\end{equation}

\noindent with $\textit{d}(\cdot ,\cdot)$ being a distance-like function.


\section{Problem Reformulation and Notations}

We introduce some notations that will be used throughout this document.

\noindent $a = (a^1, a^2, \ldots , a^m) \in \mathbb{R}^{nm}$, where $a^i \in \mathbb{R}^n,\smallskip i=1, 2, \ldots , m$.

\noindent $w = (w^1, w^2, \ldots , w^m) \in \mathbb{R}^{km}$, where $w^i \in \mathbb{R}^k,\smallskip i=1, 2, \ldots , m$.

\noindent $x = (x^1, x^2, \ldots , x^k) \in \mathbb{R}^{nk}$, where $x^l \in \mathbb{R}^n,\smallskip l=1, 2, \ldots , k$.

\noindent $d^{i}(x) = (d(x^1,a^i), d(x^2,a^i), \ldots , d(x^k,a^i)) \in \mathbb{R}^k,\smallskip i=1, 2, \ldots , m$.

\noindent $\Delta = \left\lbrace u \in \mathbb{R}^k \mid \sum\limits_{l=1}^{k} u_l = 1, \: u_l \geq 0 , \: l=1, 2, \ldots ,k \right\rbrace$.

\noindent Let $S \subseteq \mathbb{R}^n$. The indicator function of $S$ is defined and denoted as follows $\delta_S(p) = \begin{cases} 0, &\mbox{if } p \in S, \\ 
\infty, &\mbox{if } p \not\in S. \end{cases}$

Using the fact that $\min\limits_{1 \leq l \leq k} u_l = \min \left\lbrace \langle u,v \rangle \mid v \in \Delta \right\rbrace$, and applying it over $\left(\ref{StateEq1}\right)$, gives a smooth reformulation of the clustering problem

\begin{equation}
	\min\limits_{x \in \mathbb{R}^{nk}} \sum\limits_{i=1}^{m} \min\limits_{w^i \in \Delta} \langle w^i , d^i(x) \rangle. \label{StateEq2}
\end{equation}

Replacing further the constraint $w^i \in \Delta$ by adding the indicator function $\delta_{\Delta}(\cdot)$ to the objective function, results in a equivalent formulation

\begin{equation}
	\min\limits_{x \in \mathbb{R}^{nk} , w \in \mathbb{R}^{km}} \left\lbrace \sum\limits_{i=1}^{m} \langle w^i , d^i(x) \rangle + \delta_{\Delta}(w^i) \right\rbrace . \label{StateEq3}
\end{equation}

Finally, introducing several more useful notations is needed. For each $i=1, 2, \ldots , m$, we denote
\begin{center}
$H(w,x) := \sum\limits_{i=1}^{m} H_i(w,x) = \sum\limits_{i=1}^{m} \langle w^i , d^i(x) \rangle$ and $G(w) = \sum\limits_{i=1}^{m} G(w^i) := \sum\limits_{i=1}^{m} \delta_{\Delta}(w^i) .$
\\
\end{center}

Replacing the terms in (\ref{StateEq2}) with the functions defined above gives a compact form of the original clustering problem

\begin{equation}
	\min \left\lbrace \Psi(z) := H(w,x) + G(w) \mid z := (w,x) \in \mathbb{R}^{km} \times \mathbb{R}^{nk} \right\rbrace . \label{StateEq4}
\end{equation}


\section{Clustering via PALM Approach}

\subsection{Introduction to PALM Theory}

Presentation of PALM's requirements and of the algorithm steps  $\ldots$


\subsection{Clustering with PALM for Squared Euclidean Norm Distance Function}

In this section we tackle the clustering problem with the classical distance function defined by $d(u,v) = \|u-v\|^2$. We devise a PALM-like algorithm, based on the discussion about PALM in the previous subsection.
Since the clustering problem has a specific structure, we are ought to exploit it in the following manner.
First we notice that the function 
$w \mapsto H(w,x)$ is linear in $w$, so there is no need to linearize it. In addition, the function 
$x \mapsto H(w,x) = 
\sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l \|x^l - a^i\|^2 =
\\ \sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is convex and quadratic in $x$, hence we do not need to add a proximal term as in PALM algorithm.

Now we propose a PALM-like algorithm for clustering, which we call KPALM.
\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random vectors $(w(0),x(0)) \in \Delta^m \times \mathbb{R}^{nk} .$

	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace(w(t),x(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: Take any $\alpha_i(t) > 0$ and for each $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			w^i(t+1) = \arg\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i - w^i(t)\|^2 \right\rbrace . \label{StateEq5}
		\end{equation}
		
		\item Centers Update: For each $l=1, 2, \ldots ,k$ compute $x^l \in \mathbb{R}^n$ via
		\begin{equation}
			x(t+1) = \arg\min \left\lbrace H(w(t+1), x) \mid x \in \mathbb{R}^{nk} \right\rbrace . \label{StateEq6}
		\end{equation}
	\end{enumerate}
\end{enumerate}

\newpage

At each step $t \in \mathbb{N}$, the KPALM algorithm alternates between cluster assignment and centers update. The explicit formulas, at step $t$, are given below

\begin{equation}
w^i(t+1) = P_{\Delta} \left(w^i(t) - \frac{d^i(x(t))}{\alpha_i(t)}\right) , \quad i=1, 2, \ldots ,m , \label{StateEq7}
\end{equation}

\begin{equation}
x^l(t+1) = \frac{\sum_{i=1}^{m} w^i_l(t+1) a^i}{\sum_{i=1}^{m} w^i_l(t+1)} , \quad l=1, 2, \ldots ,k , \label{StateEq8}
\end{equation}
where $P_{\Delta}$ is the orthogonal projection onto the set $\Delta$.

\begin{assumption} \label{StateEq17}
We assume that none of the clusters $C^l, l = 1,2, \ldots ,k$, get empty during this process, hence for all $1 \leq l \leq k$ and $t \in \mathbb{N}$ we have that $\sum\limits_{i=1}^{m} w^i_l(t) > 0$.
\end{assumption}
	
\begin{remark} 
	\begin{enumerate}[(i)] \label{StateEq15}
%		\item $\alpha(t)$ is the step-size, and it must be positive. If at some step $t \in \mathbb{N}$, $\alpha(t)=0$, then there exists $1 \leq l' \leq k$ such that $\beta(t) = \sum\limits_{i=1}^{m} w^i_{l'}(t) = 0$. Since for all $1 \leq l \leq k$ and $1 \leq i \leq m$ we know that $w^i_l(t) \geq 0$ then $w^i_{l'}(t)=0$ for each $1 \leq i \leq m$. Thus, none of the points in $\mathcal{A}$ belong to cluster $l'$, in that case the algorithm can halt. Hence from now on we assume that for all $t \in \mathbb{N}$, $\beta(t) = \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t)\right \rbrace > 0$, and thus $\alpha(t) > 0 $.
		\item Since for all $t \in \mathbb{N}$ we have that $w(t) \in \Delta^m$ then $G(w(t))=0$ and therefore $\Psi(z(t)) = H(w(t),x(t))$. \label{StateEq16}
		\item For any choice of distance-like function $d(\cdot, \cdot)$, the function $x \mapsto H(w,x)$ is separable in $x^l$ for all $l = 1, 2, \ldots, k$. Thus, regardless the choice of distance-like function $d(\cdot, \cdot)$, the centers update step can be done in parallel over all centers, that is, $x^l(t+1) = \arg\min\limits_{x^l \in \mathbb{R}^k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l d(x^l , a^i) \right\rbrace, \\ l = 1, 2, \ldots, k$, and in the case of the squared Euclidean norm the result is given in (\ref{StateEq8}).
	\end{enumerate}
\end{remark}

\begin{lemma}[Boundedness of KPALM sequence]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace (w(t) , x(t)) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM. Then, the following statements hold true.
\begin{enumerate}[(i)]
	\item For all $l=1, 2, \ldots ,k$, the sequence $\left\lbrace x^l(t) \right\rbrace_{t \in \mathbb{N}}$ is contained in $Conv(\mathcal{A})$, where $Conv(\mathcal{A})$ is the convex hull of $\mathcal{A}$.
	\item For all $l=1, 2, \ldots ,k$, the sequence $\left\lbrace x^l(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded by $M = \max\limits_{1 \leq i \leq m} \| a^i \|$.
	\item The sequence $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded in $\mathbb{R}^{km} \times \mathbb{R}^{nk}$.
\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}[(i)]
	\item  Set $\lambda_i = \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)}, i=1, 2, \ldots ,m$, then $\lambda_i \geq 0$ and $\sum\limits_{i=1}^{m} \lambda_i = 1$. From (\ref{StateEq8}) we have
	\begin{equation*}
		x^l(t) = \frac{\sum_{i=1}^{m} w^i_l(t) a^i}{\sum_{i=1}^{m} w^i_l(t)} 
		= \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) a^i 
		= \sum\limits_{i=1}^{m} \lambda_i a^i \in Conv(\mathcal{A}).
	\end{equation*}
	Hence $x^l(t)$ is in the convex hull of $\mathcal{A}$, for all $l = 1, 2, \ldots, k$ and $t \in \mathbb{N}$.

	\item
	Taking the norm of $x^l(t)$ yields again from (\ref{StateEq8}) that
	\begin{equation*}
		\| x^l(t) \| = \left\lVert \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) a^i \right\lVert
		\leq \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) \| a^i \|
		\leq \sum_{i=1}^{m} \lambda_i \max\limits_{1 \leq i \leq m} \| a^i \| = M .
	\end{equation*}
	\item The sequence $\left\lbrace w(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded, since $w^i(t) \in \Delta$ for all $i=1, 2, \ldots ,m$ and $t \in \mathbb{N}$. Combined with the previous item, the result follows.
\end{enumerate} 
\end{proof}

\begin{lemma}[Strong convexity of $H(w,x)$ in $x$] \label{StateEq14}
The function $x \mapsto H(w,x)$ is strongly convex with parameter $\beta(w) = 2 \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l\right \rbrace$, whenever $\beta(w) > 0$.
\end{lemma}

\begin{proof}
Since the function $x \mapsto H(w(t),x) = 
\sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is $C^2$, it is strongly convex if and only if the smallest eigenvalue of the corresponding Hessian matrix is positive. Thus

\begin{center}
$\nabla_{x^j} \nabla_{x^l} H(w,x) = 
\begin{cases} 0 &\mbox{if } j \neq l, \quad 1 \leq j,l \leq k ,
\\ 2\sum\limits_{i=1}^{m} w^i_l &\mbox{if } j = l, \quad 1 \leq j,l \leq k. \end{cases} $
\end{center}

Since the Hessian is a diagonal matrix, the smallest eigenvalue is $\min\limits_{1 \leq l \leq k} 2\sum\limits_{i=1}^{m} w^i_l = \beta(w)$, and the result follows.
\end{proof}

Now we are ready to prove the decrease property of KPALM algorithm.

\begin{proposition}[Sufficient decrease property]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace \left( w(t) , x(t) \right) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then there exists $\rho_1 > 0$ such that 
\begin{equation*}
	\rho_1 \|z(t+1) - z(t)\|^2 \leq \Psi(z(t)) - \Psi(z(t+1)) \quad \forall t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
From (\ref{StateEq5}) we derive the following inequality
\begin{equation*}
\begin{aligned}
	H_i(w(t+1),x(t)) + \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 
	& = \langle w^i(t+1) , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \langle w^i(t) , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i(t) - w^i(t)\|^2 \\
	& = \langle w^i(t) , d^i(x(t)) \rangle \\
	& = H_i(w(t),x(t)) .
\end{aligned}
\end{equation*}
Hence, we obtain
\begin{equation}
	\frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 
	\leq H_i(w(t),x(t)) - H_i(w(t+1),x(t)) . \label{StateEq18}
\end{equation}
Denote $\alpha(t) = \min\limits_{1 \leq i \leq m} \left\lbrace \alpha_i(t) \right\rbrace$. Summing inequality (\ref{StateEq18}) over $i=1, 2, \ldots ,m$ yields
\begin{equation*}
\begin{aligned}
	\frac{\alpha(t)}{2} \|w(t+1) - w(t)\|^2 
	& = \frac{\alpha(t)}{2} \sum\limits_{i=1}^{m} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \sum\limits_{i=1}^{m} \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \sum\limits_{i=1}^{m} H_i(w(t),x(t)) - \sum\limits_{i=1}^{m} H_i(w(t+1),x(t)) \\
	& = H(w(t),x(t)) - H(w(t+1),x(t)) . \\
\end{aligned}
\end{equation*}

From \Cref{StateEq17} we have that $\beta(w(t)) = 2 \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t) \right \rbrace > 0$, and from \Cref{StateEq14} it follows that the function $x \mapsto H(w(t),x)$ is strongly convex with parameter $\beta(w(t))$, hence it follows that

\begin{equation*}
\begin{aligned}
	H(w(t+1),x(t)) & - H(w(t+1),x(t+1)) \geq \\
	& \geq \left\langle \nabla_x H(w(t+1),x(t+1)) , x(t)-x(t+1) \right\rangle + \frac{\beta(w(t))}{2} \|x(t) - x(t+1)\|^2 \\
	& = \frac{\beta(w(t))}{2} \|x(t+1) - x(t)\|^2 , \\
\end{aligned}
\end{equation*}
where the last equality follows from (\ref{StateEq6}), since $\nabla_{x} H(w(t+1), x(t+1)) = 0$.
Set \\$\rho_1 = \frac{1}{2}\min\left\lbrace \alpha(t) , \beta(w(t)) \right\rbrace$, combined with the previous inequalities, we have

\begin{equation*}
\begin{aligned}
	\rho_1 \|z(t+1) &- z(t)\|^2 
	 = \rho_1 \left( \|w(t+1) - w(t)\|^2 + \|x(t+1) - x(t)\|^2  \right) \leq \\
	& \leq \left[ H(w(t),x(t)) - H(w(t+1),x(t)) \right] + \left[ H(w(t+1),x(t)) - H(w(t+1),x(t+1)) \right] \\
	& = H(z(t)) - H(z(t+1)) = \Psi(z(t)) - \Psi(z(t+1)),
\end{aligned}
\end{equation*}
where the last equality follows from \Cref{StateEq15}(\ref{StateEq16}).
\end{proof}

Next, we aim to prove the subgradient lower bound for iterates gap property. The following lemma will be essential in our proof.

\begin{lemma} \label{StateEq11}
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace (w(t) , x(t)) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then 
\begin{equation*}
	\| d^i(x(t+1) - d^i(x(t)) \| \leq 4M \| x(t+1) - x(t)\|, \quad \forall i=1, 2, \ldots ,m, \: t \in \mathbb{N} ,
\end{equation*}
where $M = \max\limits_{1 \leq i \leq m} \|a^i\|$.
\end{lemma}

\begin{proof}
Since $d(u,v) = \| u-v \|^2$, we get that
{\allowdisplaybreaks
\begin{align*} 
	\| d^i(x(t&+1)  - d^i(x(t)) \| 
	 = \left[ \sum\limits_{l=1}^{k} \abs{ \|x^l(t+1) - a^i\|^2 - \| x^l(t) -a^i\|^2 }^2 \right]^{\frac{1}{2}} \\
	& = \left[ \sum\limits_{l=1}^{k} \left\lvert \|x^l(t+1)\|^2 - 2\left\langle x^l(t+1),a^i \right\rangle + \|a^i\|^2 - \|x^l(t)\|^2 + 2\left\langle x^l(t),a^i \right\rangle - \|a^i\|^2 \right\rvert ^2 \right]^{\frac{1}{2}} \\ 
	& \leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \|x^l(t+1)\|^2 - \|x^l(t)\|^2 } + \abs{ 2\left\langle x^l(t) - x^l(t+1) , a^i \right\rangle } \right)^2 \right]^{\frac{1}{2}} \\ 
	& \leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \|x^l(t+1)\| - \|x^l(t)\| } \cdot \abs{ \|x^l(t+1)\| + \|x^l(t)\| } + 2 \| x^l(t) - x^l(t+1) \| \cdot \|a^i\| \right)^2 \right]^{\frac{1}{2}} \\
	& \leq \left[ \sum\limits_{l=1}^{k} \left( \|x^l(t+1) - x^l(t)\| \cdot 2M + 2 \| x^l(t+1) - x^l(t) \| M \right)^2 \right]^{\frac{1}{2}} \\
	& = \left[ \sum\limits_{l=1}^{k} (4M)^2 \|x^l(t+1) - x^l(t)\|^2 \right]^{\frac{1}{2}} 
	= 4M \| x(t+1) - x(t)\| ,
\end{align*}
}
this proves the desired result.
\end{proof}

\begin{proposition}[Subgradient lower bound for iterates gap property]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace (w(t) , x(t)) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then there exists $\rho_2 > 0$ and $\gamma(t+1) \in \partial \Psi(z(t+1))$ such that $\| \gamma(t+1)\| \leq \rho_2 \|z(t+1) - z(t)\|^2$ for all $t \in \mathbb{N} $.
\end{proposition}

\begin{proof}
By the definition of $\Psi$ (see (\ref{StateEq4})) we get
\begin{equation*}
	\partial \Psi = \nabla H + \partial G  
= \left( \left( \nabla_{w^i} H_i + \partial_{w^i} \delta_{\Delta} \right)_{i=1, \ldots ,m} , \nabla_x H \right) .
\end{equation*}
Evaluating the last relation at $z(t+1)$ yields
\begin{equation*}
\begin{aligned}
	\partial \Psi(z(t & + 1)) = \\
	& = \left( \left( \nabla_{w^i} H_i(w(t+1),x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \ldots ,m} , \nabla_x H(w(t+1),x(t+1)) \right) \\
	& = \left( \left( d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \ldots ,m} , \nabla_x H(w(t+1),x(t+1)) \right) \\
	& = \left( \left( d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \ldots ,m} , \bf{0} \right) ,
\end{aligned}
\end{equation*}
where the last equality follows from (\ref{StateEq6}), that is, the optimality condition of $x(t+1)$. Taking the norm of the last equality yields
\begin{equation}
	\| \partial \Psi(z(t+1))\| 
	\leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \|. \label{StateEq9}
\end{equation}
The optimality condition of $w^i(t+1)$ that is derived from (\ref{StateEq5}), yields that for all $i=1, 2, \ldots ,m$ there exists $u^i(t+1) \in \partial \delta_{\Delta}(w^i(t+1))$ such that
\begin{equation}
	d^i(x(t)) + \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) + u^i(t+1) = \bf{0} . \label{StateEq10}
\end{equation}
Setting $\gamma(t+1) := \left( \left( d^i(x(t+1)) + u^i(t+1) \right)_{i=1, \ldots ,m}, \bf{0} \right) \in \partial \Psi(z(t+1))$, and plugging (\ref{StateEq10}) into (\ref{StateEq9}) we have
\begin{equation*}
\begin{aligned}
	\| \gamma(t+1) \|
	& \leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) - \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) \| \\
	& \leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) \| + \sum\limits_{i=1}^{m} \alpha_i(t) \| w^i(t+1) - w^i(t) \| \\
	& \leq \sum\limits_{i=1}^{m} 4M \| x(t+1) - x(t) \| + m \overline{\alpha}(t) \|z(t+1) - z(t)\| \\
	& \leq m \left( 4M + \overline{\alpha}(t) \right) \|z(t+1) - z(t)\| , \\
\end{aligned}
\end{equation*}
where the third inequality follows from \Cref{StateEq11}, and $\overline{\alpha}(t) = \max\limits_{1 \leq i \leq m} \alpha_i(t)$. Define \\$\rho_2 = m \left( 4M + \overline{\alpha}(t) \right)$ and the result follows.
\end{proof}


\subsection{Similarity to KMEANS}
The famous KMEANS algorithm has close proximity to KPALM algorithm. KMEANS alternates between cluster assignments and center updates as well. In detail, we can write its steps in the following manner

\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random centers $y(0) \in \mathbb{R}^{nk}$.

	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace(v(t),y(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: For $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			v^i(t+1) = \arg\min\limits_{v^i \in \Delta} \left\lbrace \langle v^i , d^i(y(t)) \rangle\right\rbrace . \label{StateEq12}
		\end{equation}
		
		\item Center Update: For $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			y^l(t+1) = \frac{\sum_{i=1}^{m} v^i_l(t+1) a^i}{\sum_{i=1}^{m} v^i_l(t+1)} . \label{StateEq13}
		\end{equation}
	\end{enumerate}
\end{enumerate}
The KMEANS algorithm obviously resemble KPALM algorithm. Denote $\overline{\alpha}(t) = \max\limits_{1 \leq i \leq m} \alpha_i(t)$. Assuming same starting point $x(0) = y(0)$ and by taking $\overline{\alpha}(t) \to 0$, we have
\begin{equation*}
	v(t) = \lim_{\overline{\alpha}(t) \to 0} w(t), \quad
	y(t) = \lim_{\overline{\alpha}(t) \to 0} x(t),
\end{equation*}
meaning, both algorithms converge to the same result.

\subsection{KMEANS Convergence Proof}

We start with rewriting the KMEANS algorithms, in its most familiar form
\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random centers $x(0) \in \mathbb{R}^{nk}$.

	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace (C(t),x(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: For $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			C^l(t+1) = \left\lbrace a \in \mathcal{A} \mid \| a - x^l(t) \| \leq \|a - x^j(t) \|, \quad \forall 1 \leq l \leq k \right\rbrace . \label{StateEq20}
		\end{equation}
		
		\item Center Update: For $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			x^l(t+1) = mean(C^l(t)) := \frac{1}{\left| C^l(t) \right|} \sum\limits_{a \in C^l(t)} a . \label{StateEq21}
		\end{equation}
		
		\item Stopping criteria: Halt if 
		\begin{equation}
			\forall 1 \leq l \leq k \quad C^l(t+1)=C^l(t) \label{StateEq22}
		\end{equation}
	\end{enumerate}
\end{enumerate}
As in KPALM, KMEANS needs \Cref{StateEq17} for step (\ref{StateEq21}) to be well defined. In addition, to prove the convergence of KMEANS to local minimum, we will need to following assumption.

\begin{assumption} \label{StateEq23}
For each step $t \in \mathbb{N}$, eack $a \in \mathcal{A}$ belongs to exclusively single cluster $C^l(t)$.
\end{assumption}

For any $x \in \mathbb{R}^{nk}$ we denote the super-partition of $\mathcal{A}$ with respect to $x$ by $\overline{C^l}(x) = \left\lbrace a \in \mathcal{A} \mid \right. \\ \left. \|a - x^l\| \leq \|a - x^j\| \quad \forall j \neq l \right\rbrace$, for all $1 \leq l \leq k$, and the sub-partition of $\mathcal{A}$ by $\underline{C^l}(x) = \left\lbrace a \in \mathcal{A} \mid \right. \\ \left. \|a - x^l\| < \|a - x^j\| \quad \forall j \neq l \right\rbrace$.
Moreover, denote $R_{lj}(t) = \min\limits_{a \in C^l(t)} \left\lbrace \|a - x^j(t)\| - \|a - x^l(t)\| \right\rbrace$ for all $1 \leq l,j \leq k$, and $r(t) = \min\limits_{l \neq j} R_{lj}$. \\
Due to \Cref{StateEq23} we have that $\overline{C^l}(x(t)) = \underline{C^l}(x(t)) = C^l(t+1)$, for all $1 \leq l \leq k, \: t \in \mathbb{N}$, we also have that $r(t) > 0$ for all $t \in \mathbb{N}$.

\begin{proposition} \label{StateEq24}
Let $(C(t), x(t))$ be the clusters and centers KMEANS returns. Denote the open neighborhood of $x(t)$ by $U = B\left( x^1(t),\frac{r(t)}{2}\right) \times  B\left( x^2(t),\frac{r(t)}{2}\right) \times \cdots \times B\left( x^l(t),\frac{r(t)}{2} \right)$, then for any $x \in U$ we have $C^l(t) = \underline{C^l}(x)$ for all $1 \leq l \leq k$.
\end{proposition}

\begin{proof}
Pick some $a \in C^l(t)$, then $x^l(t-1)$ is the closest center among the centers of $x(t-1)$. Since KMEANS halts at step $t$, then from (\ref{StateEq22}) we have $x(t)=x(t-1)$, thus $x^l(t)$ is the closest center to $a$ among the centers of $x(t)$. Further we have
\begin{equation}
	r(t) \leq \|x^j(t) - a\| - \|x^l(t) -a\| \quad \forall j \neq l . \label{StateEq25}
\end{equation}
Next, we show that $a \in \underline{C^l}(x)$, indeed
\begin{equation*}
\begin{aligned}
	\|a - x^l\| -  \|a - x^j\| &\leq \|a - x^l(t)\| + \|x^l(t) - x^l\| - \left( \|a - x^j(t)\| - \|x^j(t) - x^j\| \right) \\
	& = \|a - x^l\| - \|a - x^j(t)\| + \|x^l(t) - x^l\| + \|x^j(t) - x^j\| \\
	& < \|a - x^l\| - \|a - x^j(t)\| + r(t) \\
	& \leq -r(t) + r(t) = 0 ,
\end{aligned}
\end{equation*}
where the second inequality holds since $x^l \in B\left( x^l(t), \frac{r(t)}{2} \right)$ and $x^j \in B\left( x^j(t), \frac{r(t)}{2} \right)$, and the third inequality follows from (\ref{StateEq25}), and we get that $C^l(t) \subseteq \underline{C^l}(x)$. 
By definition of $\underline{C^l}(x)$ we have that for any $l \neq j, \: \underline{C^l}(x) \cap \underline{C^j}(x)=\emptyset$, and for all $1 \leq l \leq k, \: \underline{C^l}(x) \subseteq \mathcal{A}$, now, since $C(t)$ is a partition of $\mathcal{A}$, then $C^l(t) = \underline{C^l}(x)$ for all $1 \leq l \leq k$.
\end{proof}

\begin{proposition}[KMEANS converges to local minimum]
Let $(C(t), x(t))$ be the clusters and centers KMEANS returns, then $x(t)$ is local minimum of $F$ in $U = B\left( x^1(t),\frac{r(t)}{2}\right) \times  B\left( x^2(t),\frac{r(t)}{2}\right) \\ \times \cdots \times B\left( x^l(t),\frac{r(t)}{2} \right) \subset \mathbb{R}^{nk}$.
\end{proposition}

\begin{proof}
The minimum of $F$ in $U$ is
\begin{equation*}
\min\limits_{x \in U} F(x) = \min\limits_{x \in U} \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(x)} \|a - x^l \|^2 = \min\limits_{x \in U} \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(t)} \|a - x^l \|^2 ,
\end{equation*}
where the last equality follows from \Cref{StateEq24}.\\
The function $x \mapsto \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(t)} \|a - x^l \|^2$ is strictly convex, separable in $x^l$ for all $1 \leq l \leq k$, and reaches its minimum at $\left( x^l \right)^{*} = \frac{1}{\left| C^l(t) \right|} \sum\limits_{a \in C^l(t)} a = mean(C^l(t)) = x^l(t),$ and the result follows.
\end{proof}

%\section{Clustering via Alternation with Weiszfeld Step}
%
%In this section we tackle the clustering problem with distance-like function being the Euclidean norm in $\mathbb{R}^n$, namely
%\begin{equation}
%	\min_{x^1, x^2, \ldots, x^k \in \mathbb{R}^n} \left\lbrace \sum\limits_{i=1}^{m} \min\limits_{1 \leq l \leq k} d(x^l,a^i) \right\rbrace , \label{StateEq19}
%\end{equation}
%where $d(u,v) = \|u - v\|$. \\
%In the previous section we showed that (\ref{StateEq19}) has the following equivalent form
%\begin{equation*}
%	\min \left\lbrace \Psi(z) := H(w,x) + G(w) \mid z := (w,x) \in \mathbb{R}^{km} \times \mathbb{R}^{nk} \right\rbrace ,
%\end{equation*}
%where
%\begin{equation*}
%	H(w,x) = \sum\limits_{i=1}^{m} \left\langle w^i , d^i(x) \right\rangle
%	= \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l d(x^l,a^i)
%	= \sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l d(x^l,a^i) 
%	= \sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \| x^l - a^i \| ,
%\end{equation*}
%and
%\begin{equation*}
%	G(w) = \sum\limits_{i=1}^{m} \delta_{\Delta}(w^i) .
%\end{equation*}
%
%\newpage
%
%Solving clustering problem with distance function $d(\cdot,\cdot) = \| u - v \|$
%\begin{equation*}
%	\min_{x^1, \ldots, x^k \in \mathbb{R}^n} \left\lbrace \sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l -a^i \| \mid w^i \in \Delta, \: i=1,2, \ldots, m \right\rbrace
%\end{equation*}
%similarly to KMEANS it leads to the following alternation steps: \\
%\begin{enumerate}[(1)]
%	\item Clusters Assignment: For $i=1, 2, \ldots ,m$ compute
%		\begin{equation}
%			w^i(t+1) = \arg\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(x(t)) \rangle\right\rbrace \label{StateEq31}
%		\end{equation}
%		
%	\item Centers Update: For $l=1, 2, \ldots ,k$ compute
%		\begin{equation}
%			x^l(t+1) = \arg\min\limits_{x^l \in \mathbb{R}^n} \sum\limits_{i=1}^{m} w^i_l(t+1) \| x^l -a^i \| \label{StateEq32}
%		\end{equation}
%\end{enumerate}
%The center update step could be computed via Weiszfeld algorithm.
%
%\newpage

%\section{Clustering via ADMM Approach }
%
%First we add new variables $z^l, l=1, \ldots ,k$, and formulate an equivalent problem to the clustering problem (see (1.2)):
%
%\begin{equation}
%	\min\limits_{x^1, \dots ,x^k \in \mathbb{R}^n} \min\limits_{w^1, \dots ,w^m \in \mathbb{R}^k} \min\limits_{z^1, \dots ,z^k \in S} \left\lbrace \sum\limits_{i=1}^{m} v_i \sum\limits_{l=1}^{k} w^i_l d(x^l , a^i) \mid w^i \in \Delta^i , i=1, \dots ,m , x^l = z^l, l=1, \dots ,k \right\rbrace
%\end{equation}
%
%We present the augmented Lagrangian associated with the clustering problem
%
%\begin{equation}
%	L_\rho({\bf x},{\bf z},{\bf y},{\bf w}) = H({\bf x},{\bf w}) + \sum\limits_{l=1}^k (y^l)^T (x^l -z^l) + \frac{\rho}{2} \sum\limits_{l=1}^k \| x^l -z^l \|^2
%\end{equation}
%
%\noindent ADMM update:
%\begin{center}
%	$x^{l,t+1} := \frac{\rho z^{l,t} - y^{l,t} + 2\sum\limits_{i=1}^m {v_i w^{i,t}_l a^i} }{\rho + 2\sum\limits_{i=1}^m v_i w^{i,t}_l}$ \\ \bigskip
%	$z^{l,t+1} := \Pi_S(x^{l,t+1} + \frac{y^{l,t}}{\rho} )$ \\ \bigskip
%	$y^{l,t+1} := y^{l.t} + \rho (x^{l,t+1} - z^{l,t+1})$ \\ \bigskip
%	$w^{i,t+1} \in \left\lbrace w \in \mathbb{R}^k \mid w \in \Delta^i,\text{ such that if }l \not\in Nearest({\bf x}^{t+1} ,a^i) \text{ then } w^i_l=0 \right\rbrace $
%\smallskip
%	$\text{where } Nearest({\bf x} ,a^i) := \left\lbrace 1 \leq l \leq k \mid \|x^l -a^i\|= \min\limits_{1 \leq j \leq k} \|x^j - a^i\| \right\rbrace$
%\end{center}
%
%\newpage


%\begin{thebibliography}{99}

%\end{thebibliography}



\end{document}
