\documentclass[11pt]{article} 
\usepackage[american]{babel}
\usepackage{makeidx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{cleveref}

\oddsidemargin= -12pt \evensidemargin= -12pt
\topmargin=-45pt
\binoppenalty=10000
\relpenalty=10000
\textwidth=6.5in\textheight=9in \baselineskip=18pt
\parskip=6pt plus 1pt
\numberwithin{equation}{section}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[proposition]
\newtheorem{remark}{Remark}

\def\abs#1{\left\lvert#1\right\rvert}

\begin{document}

%\begin{center} 
%\Large {Tel-Aviv University}
%
%\Large {The Raymond and Beverly Sackler Faculty of} \\
%\Large {Exact Sciences} \\
%\Large {The Department of Statistics and Operations} \\
%\Large {Research}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {\bf Draft }
%
%\bigskip
%\bigskip
%\bigskip
%\bigskip
%
%\Large{Thesis Submitted Towards the Degree of Master} \\
%\Large {of Science in Operations Research}
%
%\bigskip
%
%\Large {Sergey Voldman}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {Supervisors:} \\
%\Large {Prof. Marc Teboulle} \\
%\Large {Prof. Shoham Sabach}
%
%\bigskip
%\bigskip
%
%\Large{2015}
%
%\end{center}
%
%\newpage
%
%\begin{center} \Large {\bf Draft}
%\end{center}
%
%\begin{abstract} 
%
%To-do...
%
%\newpage
%
%\end{abstract}
%
%\section{Introduction}
%
%To-do...
%

\newpage

\section{The Clustering Problem}

Let $\mathcal{A}= \left\lbrace a^1, a^2, \ldots ,a^m \right\rbrace$ be a given set of points in $\mathbb{R}^n$, and let $1 < k < m$ be a fixed given number of clusters. The clustering problem consists of partitioning the data $\mathcal{A}$ into $k$ subsets $\left\lbrace A^1, A^2, \ldots ,A^k \right\rbrace$, called clusters. For each $l=1, 2, \ldots ,k$, the cluster $A_l$ is represented by its center $x^l$, and we want to determine $k$ cluster centers $\left\lbrace x^1, x^2 \ldots ,x^k \right\rbrace$ such that the sum of proximity measures from each point $a^i, i=1, 2, \ldots ,m$, to a nearest cluster center $x^l$ is minimized.

The clustering problem formulation is given by

\begin{equation}
	\min\limits_{x^1, \ldots ,x^k \in \mathbb{R}^n} \sum\limits_{i=1}^{m} \min\limits_{1 \le l \le k} d(x^l,a^i) , \label{StateEq1}
\end{equation}

\noindent with $\textit{d}(\cdot ,\cdot)$ being a distance-like function.

\section{Problem Reformulation and Notations}

We introduce some notations that will be used throughout this document.

\noindent $A = (a^1, \ldots , a^m) \in \left(\mathbb{R}^n\right)^m$, where $a^i \in \mathbb{R}^n,\smallskip i=1, 2, \ldots , m$

\noindent $W = (w^1, \ldots , w^m) \in \left(\mathbb{R}^k\right)^m$, where $w^i \in \mathbb{R}^k,\smallskip i=1, 2, \ldots , m$

\noindent $X = (x^1, \ldots , x^k) \in \left(\mathbb{R}^n\right)^k$, where $x^l \in \mathbb{R}^n,\smallskip l=1, 2, \ldots , k$

\noindent $d^{i}(X) = (d(x^1,a^i), \ldots , d(x^k,a^i)) \in \mathbb{R}^k,\smallskip i=1, 2, \ldots , m$

\noindent $\Delta = \left\lbrace u \in \mathbb{R}^k \mid \sum\limits_{l=1}^{k} u_l = 1, u_l \geq 0 , l=1, 2, \ldots ,k \right\rbrace$

\noindent For some $S \subseteq \mathbb{R}^n$, the indicator function of the set$s$ is defined and denoted as follows \\ $\delta_S(p) = \begin{cases} 0 &\mbox{if } p \in S \\ 
\infty &\mbox{if } p \not\in S \end{cases}.$

Using the fact that $\min\limits_{1 \leq l \leq k} u_l = \min \left\lbrace \langle u,v \rangle \mid v \in \Delta \right\rbrace$, and applying it over $\left(\ref{StateEq1}\right)$, gives a smooth reformulation of the clustering problem

\begin{equation}
	\min\limits_{X \in \left(\mathbb{R}^n\right)^k} \sum\limits_{i=1}^{m} \min\limits_{w^i \in \Delta} \langle w^i , d^i(X) \rangle. \label{StateEq2}
\end{equation}

Further replacing the constrain over $w^i \in \Delta$ by adding the indicator function $\delta_{\Delta}(\cdot)$ to the objective function, results in a equivalent formulation

\begin{equation}
	\min\limits_{X \in \left(\mathbb{R}^n\right)^k , W \in \left(\mathbb{R}^k\right)^m} \left\lbrace \sum\limits_{i=1}^{m} \langle w^i , d^i(X) \rangle + \delta_{\Delta}(w^i) \right\rbrace \label{StateEq3}
\end{equation}

Finally, introducing several more useful definitions, for each $i=1, 2, \ldots , m$

\begin{center}
$H_i(W,X) = \langle w^i , d^i(X) \rangle \quad G(w^i) = \delta_{\Delta}(w^i)$
\\ \smallskip
$H(W,X) = \sum\limits_{i=1}^{m} H_i(W,X)\quad G(W) = \sum\limits_{i=1}^{m} G(w^i)$
\\
\end{center}

Replacing the terms in (\ref{StateEq2}) with the functions above gives a compact form that is equivalent to the original clustering problem

\begin{equation}
	\min \left\lbrace \Psi(Z) := H(W,X) + G(W) \mid Z := (W,X) \in (\mathbb{R}^k)^m \times (\mathbb{R}^n)^k \right\rbrace \label{StateEq4}
\end{equation}


\section{Clustering via PALM Approach}

\subsection{Introduction to PALM Theory}

Presentation of PALM's requirements and of the algorithm steps  $\ldots$

\subsection{Clustering with PALM for Squared Euclidean Norm Distance-Like Function}

In this section we tackle the clustering problem with distance-like function $d(u,v) = \|u-v\|^2$. We devise a PALM-like algorithm, based on the discussion about PALM in the previous subsection.
Since the clustering problem has a specific structure, we are ought to exploit it in the following manner.
First we notice that the map 
$W \mapsto H(W,X)$ is linear in $W$, so there is no need to linearize it. In addition, the map 
$X \mapsto H(W,X) = 
\sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l \|x^l - a^i\|^2 =
\sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is convex and quadratic in $X$, hence we do not need to add a proximal term as in PALM algorithm.

Now we propose the PALM-like algorithm for clustering, we name it KPALM.
\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random vectors $(W(0),X(0)) \in \Delta^m \times (\mathbb{R}^n)^k$

	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace(W(t),X(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: Take $\nu \in \left(0,1\right]$, compute $\beta(t) = \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t)\right \rbrace$, set
		$\alpha(t)=\nu \beta(t) $ and for each $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			w^i(t+1) = \arg\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(X(t)) \rangle + \frac{\alpha(t)}{2} \|w^i - w^i(t)\|^2 \right\rbrace \label{StateEq5}
		\end{equation}
		
		\item Centers Update: For each $l=1, 2, \ldots ,k$ compute $x^l \in \mathbb{R}^n$ via
		\begin{equation}
			X(t+1) = \arg\min \left\lbrace H(W(t+1), X) \mid X \in (\mathbb{R}^n)^k \right\rbrace \label{StateEq6}
		\end{equation}
	\end{enumerate}
\end{enumerate}

\newpage

At each step $t \in \mathbb{N}$, the KPALM algorithm alternates between cluster assignment and centers update. The explicit formulas for step $t$ are given below

\begin{equation}
w^i(t+1) = \Pi_{\Delta} \left(w^i(t) - \frac{d^i(X(t))}{\alpha(t)}\right) , \quad i=1, 2, \ldots ,m , \label{StateEq7}
\end{equation}

\begin{equation}
x^l(t+1) = \frac{\sum_{i=1}^{m} w^i_l(t+1) a^i}{\sum_{i=1}^{m} w^i_l(t+1)} , \quad l=1, 2, \ldots ,k . \label{StateEq8}
\end{equation}

\begin{remark} 
	\begin{enumerate}[(i)] \label{StateEq15}
		\item $\alpha(t)$ is the step-size, and it must be positive. If for some step $t \in \mathbb{N}$ $\alpha(t)=0$ then there exists $1 \leq l' \leq k$ such that $\beta(t) = \sum\limits_{i=1}^{m} w^i_{l'} = 0$, since for all $1 \leq l \leq k$, and for all $1 \leq i \leq m$ such that $w^i_l \geq 0$ then for all $1 \leq i \leq m, \quad w^i_{l'}=0$. Thus, none of the points in $\mathcal{A}$ belong to cluster $l'$, in that case the algorithm can halt. Hence from now on we assume that for all $t \in \mathbb{N}$, $\beta(t) = \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t)\right \rbrace > 0$, and it follows that $\alpha(t) > 0 $.
		\item Since for all $t \in \mathbb{N} \quad W(t) \in \Delta^m$ then $\Psi(Z(t)) = H(W(t),X(t)) + G(W(t)) = H(W(t),X(t))$. \label{StateEq16}
		\item Note that the function $X \mapsto H(W,X)$ is separable in $x^l$ for all $l = 1, 2, \ldots, k$. Thus, regardless of the specific distance-like function $d(\cdot, \cdot)$, the centers update step is $x^l(t+1) = \\ \arg\min\limits_{x^l \in \mathbb{R}^k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l d(x^l , a^i) \right\rbrace, l = 1, 2, \ldots, k$, and in the case of squared Euclidean norm the result is as in (\ref{StateEq8}).
	\end{enumerate}
\end{remark}

\begin{lemma}[Boundedness of KPALM sequence] \ \\
Let $\left\lbrace Z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace W(t) , X(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM. Then 
\begin{enumerate}[(i)]
	\item $x^l(t) \in Conv(\mathcal{A})$ for all $l=1, 2, \ldots ,k$ and $t \in \mathbb{N}$, where $Conv(\mathcal{A})$ is the convex hull of $\mathcal{A}$.
	\item For all $l=1, 2, \ldots ,k$ and $t \in \mathbb{N}$, $\| x^l(t) \|$ is bounded by $M = \max\limits_{1 \leq i \leq m} \| a^i \|$.
	\item $\left\lbrace Z(t) \right\rbrace_{t \in \mathbb{N}}$ is a bounded sequence in $(\mathbb{R}^k)^m \times (\mathbb{R}^n)^k$.
\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}[(i)]
	\item  Set $\alpha_i = \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)}, i=1, 2, \ldots ,m$, then $\alpha_i \geq 0$ and $\sum\limits_{i=1}^{m} \alpha_i =1$. From (\ref{StateEq8}) we have
	\begin{equation*}
		x^l(t) = \frac{\sum_{i=1}^{m} w^i_l(t) a^i}{\sum_{i=1}^{m} w^i_l(t)} 
		= \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) a^i 
		= \sum\limits_{i=1}^{m} \alpha_i a^i \in Conv(\mathcal{A}).
	\end{equation*}
	Hence $x^l(t)$ is in the convex hull of $\mathcal{A}$, for all $l = 1, 2, \ldots, k$, and $t \in \mathbb{N}$.

	\item
	Taking the norm of $x^l(t)$ yields 
	\begin{equation*}
		\| x^l(t) \| = \left\lVert \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) a^i \right\lVert
		\leq \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) \| a^i \|
		\leq \sum_{i=1}^{m} \alpha_i \max\limits_{1 \leq i \leq m} \| a^i \| = M .
	\end{equation*}
	\item $w^i(t)$ is bounded, since $w^i(t) \in \Delta$ for all $i=1, 2, \ldots ,m$ and $t \in \mathbb{N}$. Combined with the previous item, the result follows. 
\end{enumerate}
\end{proof}

\begin{lemma}[Strong convexity of $H(W,X)$ in $X$] \label{StateEq14}
At step $t$, the function $X \mapsto H(W(t),X)$ is strongly convex iff $\beta(t) > 0$.
\end{lemma}

\begin{proof}
Since the function $X \mapsto H(W(t),X) = 
\sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is $C^2$, it is strongly convex iff the smallest eigenvalue of the Hessian matrix is positive. Thus

\begin{center}
$\nabla_{x^j} \nabla_{x^l} H(W(t),X) = 
\begin{cases} 0 &\mbox{if } j \neq l, \quad 1 \leq j,l \leq k ,
\\ 2\sum\limits_{i=1}^{m} w^i_l(t) &\mbox{if } j = l, \quad 1 \leq j,l \leq k. \end{cases} $
\end{center}

Since the Hessian is diagonal, the smallest eigenvalue is $\min\limits_{1 \leq l \leq k} 2\sum\limits_{i=1}^{m} w^i_l(t) = 2\beta(t)$, and the result follows.
\end{proof}

Now we are ready to prove the decrease property of KPALM algorithm.

\begin{proposition}[Sufficient decrease property]\ \\
Let $\left\lbrace Z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace W(t) , X(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then there exists $\rho_1 > 0$ such that $\rho_1 \|Z(t+1) - Z(t)\|^2 \leq \Psi(Z(t)) - \Psi(Z(t+1))$ for all $t \in \mathbb{N}$ .
\end{proposition}

\begin{proof}
From (\ref{StateEq5}) we derive the following inequality
\begin{equation*}
	\begin{split}
	H_i(W(t+1),X(t)) + \frac{\alpha(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	= \langle w^i(t+1) , d^i(X(t)) \rangle + \frac{\alpha(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	\leq \langle w^i(t) , d^i(X(t)) \rangle + \frac{\alpha(t)}{2} \|w^i(t) - w^i(t)\|^2 \\
	= \langle w^i(t) , d^i(X(t)) \rangle \\
	= H_i(W(t),X(t))
	\end{split}
\end{equation*}
Hence, we obtain
\begin{equation*}
	\frac{\alpha(t)}{2} \|w^i(t+1) - w^i(t)\|^2 
	\leq H_i(W(t),X(t)) - H_i(W(t+1),X(t)) .
\end{equation*}
Summing this inequality over $i=1, 2, \ldots ,m$ yields
\begin{equation*}
	\begin{split}
	\frac{\alpha(t)}{2} \|W(t+1) - W(t)\|^2 
	= \frac{\alpha(t)}{2} \sum\limits_{i=1}^{m} \|w^i(t+1) - w^i(t)\|^2 \\
	\leq \sum\limits_{i=1}^{m} H_i(W(t),X(t)) - \sum\limits_{i=1}^{m} H_i(W(t+1),X(t)) \\
	= H(W(t),X(t)) - H(W(t+1),X(t)) \\
	\end{split}
\end{equation*}

From \cref{StateEq14} we have that the function $X \mapsto H(W(t),X)$ is strongly convex with parameter $2 \beta(t) > 0$, hence it follows that

\begin{equation*}
	\begin{split}
	H(W(t+1),X(t)) - H(W(t+1),X(t+1)) \\
	\geq \nabla_X H(W(t+1),X(t+1))^{T}(X(t)-X(t+1)) + \frac{2\beta(t)}{2} \|X(t) - X(t+1)\|^2 \\
	= \beta(t) \|X(t) - X(t+1)\|^2 \\
	\end{split}
\end{equation*}
where the last equality follows from (\ref{StateEq6}), since $\nabla_{X} H(W(t+1), X(t+1)) = 0$. \\
Set $\rho_1 = \min\left\lbrace \alpha(t) , \beta(t) \right\rbrace = \alpha(t)$, combined with the previous inequalities, we have

\begin{equation*}
	\begin{split}
	\rho_1 \|Z(t+1) - Z(t)\|^2 
	= \rho_1 \left( \|W(t+1) - W(t)\|^2 + \|X(t+1) - X(t)\|^2 \right) \\
	\leq \left[ H(W(t),X(t)) - H(W(t+1),X(t)) \right] + \left[ H(W(t+1),X(t)) - H(W(t+1),X(t+1)) \right] \\
	= H(Z(t)) - H(Z(t+1)) = \Psi(Z(t)) - \Psi(Z(t+1)),
	\end{split}
\end{equation*}
where the last equality follows from \cref{StateEq15}(\ref{StateEq16}).
\end{proof}

Next, we aim to prove the subgradient lower bound for iterates gap property. The following lemma will be handy in our proof.

\begin{lemma} \label{StateEq11} \ \\
Let $\left\lbrace Z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace W(t) , X(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then \\ $\| d^i(X(t+1) - d^i(X(t)) \| \leq 2M \| X(t+1) - X(t)\|$ for all $i=1, 2, \ldots ,m$ and $t \in \mathbb{N}$, where $M = \max\limits_{1 \leq i \leq m} \|a^i\|$.
\end{lemma}

\begin{proof}
\begin{equation*}
	\begin{split}
	\| d^i(X(t+1) - d^i(X(t)) \| = \left[ \sum\limits_{l=1}^{k} \abs{ \|x^l(t+1) - a^i\|^2 - \| x^l(t) -a^i\|^2 }^2 \right]^{\frac{1}{2}} \\
	= \left[ \sum\limits_{l=1}^{k} \abs{ \|x^l(t+1)\|^2 - 2\left\langle x^l(t+1),a^i \right\rangle + \|a^i\|^2 - \|x^l(t)\|^2 + 2\left\langle x^l(t),a^i \right\rangle - \|a^i\|^2 }^2 \right]^{\frac{1}{2}} \\
	\leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \|x^l(t+1)\|^2 - \|x^l(t)\|^2 } + \abs{ 2\left\langle x^l(t) - x^l(t+1) , a^i \right\rangle } \right)^2 \right]^{\frac{1}{2}} \\
	\leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \|x^l(t+1)\| - \|x^l(t)\| } \abs{ \|x^l(t+1)\| + \|x^l(t)\| } + 2 \| x^l(t) - x^l(t+1) \| \|a^i\| \right)^2 \right]^{\frac{1}{2}} \\
	\leq \left[ \sum\limits_{l=1}^{k} \left( \|x^l(t+1) - x^l(t)\| \cdot 2M + 2 \| x^l(t+1) - x^l(t) \| M \right)^2 \right]^{\frac{1}{2}} \\
	= \left[ \sum\limits_{l=1}^{k} (4M)^2 \|x^l(t+1) - x^l(t)\|^2 \right]^{\frac{1}{2}} 
	= 4M \| X(t+1) - X(t)\|
	\end{split}
\end{equation*}
\end{proof}

\begin{proposition}[Subgradient lower bound for iterates property gap]\ \\
Let $\left\lbrace Z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace W(t) , X(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then there exists $\rho_2 > 0$ and $\gamma(t+1) \in \partial \Psi(Z(t+1))$ such that $\| \gamma(t+1)\| \leq \rho_2 \|Z(t+1) - Z(t)\|^2$, for all $t \in \mathbb{N} $.
\end{proposition}

\begin{proof}
$\Psi = H + G$, then
\begin{equation*}
	\begin{split}
	\partial \Psi = \nabla H + \partial G  
	= \left( \nabla_{W}H, \nabla_{X}H \right) + \left( \left( \partial_{w^i}\delta_{\Delta} \right)_{i=1, \ldots ,m}, \bf{0} \right) \\
	= \left( \left( \nabla_{w^i} H_i + \partial_{w^i} \delta_{\Delta} \right)_{i=1, \ldots ,m} , \nabla_X H \right) .
	\end{split}
\end{equation*}
Evaluating the last relation at $Z(t+1)$ yields
\begin{equation*}
	\begin{split}
	\partial \Psi(Z(t+1)) 
	= \left( \left( \nabla_{w^i} H_i(W(t+1),X(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \ldots ,m} , \nabla_X H(W(t+1),X(t+1)) \right) \\
	= \left( \left( d^i(X(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \ldots ,m} , \nabla_X H(W(t+1),X(t+1)) \right) \\
	= \left( \left( d^i(X(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \ldots ,m} , \bf{0} \right) ,
	\end{split}
\end{equation*}
where the last equality follows from (\ref{StateEq6}), that is the optimality condition of $X(t+1)$. \\
Taking the norm of the last equation yields
\begin{equation}
	\| \partial \Psi(Z(t+1))\| 
	\leq \sum\limits_{i=1}^{m} \| d^i(X(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \|. \label{StateEq9}
\end{equation}
The optimality condition of $w^i(t+1)$ that is derived from (\ref{StateEq5}), yields that for all $i=1, 2, \ldots ,m$ there exists $u^i(t+1) \in \partial \delta_{\Delta}(w^i(t+1))$ such that
\begin{equation}
	d^i(X(t)) + \alpha(t) \left( w^i(t+1) - w^i(t) \right) + u^i(t+1) = 0 . \label{StateEq10}
\end{equation}
Setting $\gamma(t+1) := \left( \left( d^i(X(t+1)) + u^i(t+1) \right)_{i=1, \ldots ,m}, \bf{0} \right) \in \partial \Psi(Z(t+1))$, and plugging (\ref{StateEq10}) into (\ref{StateEq9}) we have
\begin{equation*}
	\begin{split}
	\| \gamma(t+1) \|
	\leq \sum\limits_{i=1}^{m} \| d^i(X(t+1)) - d^i(X(t)) - \alpha(t) \left( w^i(t+1) - w^i(t) \right) \| \\
	\leq \sum\limits_{i=1}^{m} \| d^i(X(t+1)) - d^i(X(t)) \| + \sum\limits_{i=1}^{m} \alpha(t) \| w^i(t+1) - w^i(t) \| \\
	\leq \sum\limits_{i=1}^{m} 4M \| X(t+1)) - X(t) \| + m \alpha(t) \|Z(t+1) - Z(t)\| \\
	\leq m \left( 4M + \alpha(t) \right) \|Z(t+1) - Z(t)\| \\
	\end{split}
\end{equation*}
where the third inequality follows from \cref{StateEq11}. \\
Define $\rho_2 = m \left( 4M + \alpha(t) \right)$ and the result follows.
\end{proof}

\newpage

\subsection{Similarity to KMEANS}
The famous KMEANS algorithm has close proximity to KPALM algorithm. KMEANS alternates between cluster assignments and center updates as well. In detail, we can write its steps in the following manner

\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random centers $Y(0) \in (\mathbb{R}^n)^k$

	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace(V(t),Y(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: For $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			v^i(t+1) = \arg\min\limits_{v^i \in \Delta} \left\lbrace \langle v^i , d^i(Y(t)) \rangle\right\rbrace \label{StateEq12}
		\end{equation}
		
		\item Center Update: For $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			y^l(t+1) = \frac{\sum_{i=1}^{m} v^i_l(t+1) a^i}{\sum_{i=1}^{m} v^i_l(t+1)} \label{StateEq13}
		\end{equation}
	\end{enumerate}
\end{enumerate}
The KMEANS algorithm obviously resemble KPALM algorithm. Assuming same starting point $X(0) = Y(0)$ and by taking $\nu \to 0$, we have
\begin{equation*}
	\begin{split}
	V(t) = \lim_{\nu \to 0} W(t) \\
	Y(t) = \lim_{\nu \to 0} X(t)
	\end{split}
\end{equation*}

\newpage

%\section{Clustering via ADMM Approach }
%
%First we add new variables $z^l, l=1, \ldots ,k$, and formulate an equivalent problem to the clustering problem (see (1.2)):
%
%\begin{equation}
%	\min\limits_{x^1, \dots ,x^k \in \mathbb{R}^n} \min\limits_{w^1, \dots ,w^m \in \mathbb{R}^k} \min\limits_{z^1, \dots ,z^k \in S} \left\lbrace \sum\limits_{i=1}^{m} v_i \sum\limits_{l=1}^{k} w^i_l d(x^l , a^i) \mid w^i \in \Delta^i , i=1, \dots ,m , x^l = z^l, l=1, \dots ,k \right\rbrace
%\end{equation}
%
%We present the augmented Lagrangian associated with the clustering problem
%
%\begin{equation}
%	L_\rho({\bf x},{\bf z},{\bf y},{\bf w}) = H({\bf x},{\bf w}) + \sum\limits_{l=1}^k (y^l)^T (x^l -z^l) + \frac{\rho}{2} \sum\limits_{l=1}^k \| x^l -z^l \|^2
%\end{equation}
%
%\noindent ADMM update:
%\begin{center}
%	$x^{l,t+1} := \frac{\rho z^{l,t} - y^{l,t} + 2\sum\limits_{i=1}^m {v_i w^{i,t}_l a^i} }{\rho + 2\sum\limits_{i=1}^m v_i w^{i,t}_l}$ \\ \bigskip
%	$z^{l,t+1} := \Pi_S(x^{l,t+1} + \frac{y^{l,t}}{\rho} )$ \\ \bigskip
%	$y^{l,t+1} := y^{l.t} + \rho (x^{l,t+1} - z^{l,t+1})$ \\ \bigskip
%	$w^{i,t+1} \in \left\lbrace w \in \mathbb{R}^k \mid w \in \Delta^i,\text{ such that if }l \not\in Nearest({\bf x}^{t+1} ,a^i) \text{ then } w^i_l=0 \right\rbrace $
%\smallskip
%	$\text{where } Nearest({\bf x} ,a^i) := \left\lbrace 1 \leq l \leq k \mid \|x^l -a^i\|= \min\limits_{1 \leq j \leq k} \|x^j - a^i\| \right\rbrace$
%\end{center}
%
%\newpage


%\begin{thebibliography}{99}

%\end{thebibliography}



\end{document}
