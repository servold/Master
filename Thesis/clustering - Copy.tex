\documentclass[11pt]{article} 
\usepackage[american]{babel}
\usepackage{makeidx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{cleveref}
\usepackage{framed}

\oddsidemargin= -12pt \evensidemargin= -12pt
\topmargin=-45pt
\binoppenalty=10000
\relpenalty=10000
\textwidth=6.5in\textheight=9in \baselineskip=18pt
\parskip=6pt plus 1pt
\numberwithin{equation}{section}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[proposition]
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}

\def\abs#1{\left\lvert#1\right\rvert}
\makeatletter
\def\Let@{\def\\{\notag\math@cr}}
\makeatother

\begin{document}

%\begin{center} 
%\Large {Tel-Aviv University}
%
%\Large {The Raymond and Beverly Sackler Faculty of} \\
%\Large {Exact Sciences} \\
%\Large {The Department of Statistics and Operations} \\
%\Large {Research}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {\bf Draft }
%
%\bigskip
%\bigskip
%\bigskip
%\bigskip
%
%\Large{Thesis Submitted Towards the Degree of Master} \\
%\Large {of Science in Operations Research}
%
%\bigskip
%
%\Large {Sergey Voldman}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {Supervisors:} \\
%\Large {Prof. Marc Teboulle} \\
%\Large {Prof. Shoham Sabach}
%
%\bigskip
%\bigskip
%
%\Large{2015}
%
%\end{center}
%
%\newpage
%
%\begin{center} \Large {\bf Draft}
%\end{center}
%
%\begin{abstract} 
%
%To-do...
%
%\newpage
%
%\end{abstract}
%
%\section{Introduction}
%
%To-do...
%

\newpage

\section{The Clustering Problem}

Let $\mathcal{A}= \left\lbrace a^1, a^2, \ldots ,a^m \right\rbrace$ be a given set of points in $\mathbb{R}^n$, and let $1 < k < m$ be a fixed given number of clusters. The clustering problem consists of partitioning the data $\mathcal{A}$ into $k$ subsets $\left\lbrace C^1, C^2, \ldots ,C^k \right\rbrace$, called clusters. For each $l=1, 2, \ldots ,k$, the cluster $C^l$ is represented by its center $x^l \in \mathbb{R}^n$, and we are interested to determine $k$ cluster centers $\left\lbrace x^1, x^2, \ldots ,x^k \right\rbrace$ such that the sum of certain proximity measures from each point $a^i$, $i=1, 2, \ldots ,m$, to a nearest cluster center $x^l$ is minimized. We define the vector of all centers by  $x = (x^1, x^2, \ldots , x^k) \in \mathbb{R}^{nk}$.

The clustering problem is given by
\begin{equation}
	\min\limits_{x \in \mathbb{R}^{nk}} \left\lbrace F(x) := \sum\limits_{i=1}^{m} \min\limits_{1 \le l \le k} d(x^l,a^i) \right\rbrace , \label{StateEq1}
\end{equation}
\noindent with $\textit{d}(\cdot ,\cdot)$ being a distance-like function.


\section{Problem Reformulation and Notations}

We begin with a reformulation of the clustering problem which will be the basis for our developments in this work. The reformulation is based on the following fact:
\begin{equation*}
	\min\limits_{1 \leq l \leq k} u_l = \min \left\lbrace \langle u,v \rangle : v \in \Delta \right\rbrace ,
\end{equation*}
where $\Delta$ denotes the well-known simplex defined by
\begin{equation*}
	\Delta = \left\lbrace u \in \mathbb{R}^k : \sum\limits_{l=1}^{k} u_l = 1, \: u \geq 0 \right\rbrace .
\end{equation*}
Using this fact in Problem (\ref{StateEq1}) and introducing new variables $w^i \in \mathbb{R}^k$, $i=1,2, \ldots, m$, gives a smooth reformulation of the clustering problem
\begin{equation}
	\min\limits_{x \in \mathbb{R}^{nk}} \sum\limits_{i=1}^{m} \min\limits_{w^i \in \Delta} \langle w^i , d^i(x) \rangle , \label{StateEq2}
\end{equation}
where 
\begin{equation*}
d^{i}(x) = (d(x^1,a^i), d(x^2,a^i), \ldots , d(x^k,a^i)) \in \mathbb{R}^k, \quad i=1, 2, \ldots , m.
\end{equation*}
Replacing further the constraint $w^i \in \Delta$ by adding the indicator function $\delta_{\Delta}(\cdot)$, which defined to be $0$ in $\Delta$ and $\infty$ otherwise, to the objective function, results in a equivalent formulation
\begin{equation}
	\min\limits_{x \in \mathbb{R}^{nk} , w \in \mathbb{R}^{km}} \left\lbrace \sum\limits_{i=1}^{m} \left( \langle w^i , d^i(x) \rangle + \delta_{\Delta}(w^i) \right) \right\rbrace , \label{StateEq3}
\end{equation}
where $w = (w^1, w^2, \ldots , w^m) \in \mathbb{R}^{km}$.
Finally, for the simplicity of the yet to come expositions, we define the following functions
\begin{center}
$H(w,x) := \sum\limits_{i=1}^{m} H^i(w,x) = \sum\limits_{i=1}^{m} \langle w^i , d^i(x) \rangle \quad$ and $\quad G(w) = \sum\limits_{i=1}^{m} G(w^i) := \sum\limits_{i=1}^{m} \delta_{\Delta}(w^i) .$
\end{center}

Replacing the terms in (\ref{StateEq3}) with the functions defined above gives a compact equivalent form of the original clustering problem

\begin{equation}
	\min \left\lbrace \Psi(z) := H(w,x) + G(w) \mid z := (w,x) \in \mathbb{R}^{km} \times \mathbb{R}^{nk} \right\rbrace . \label{StateEq4}
\end{equation}


\section{Clustering: The Squared Euclidean Norm Case} \label{State_Clustering_SqNorm}

\subsection{Introduction to the PALM Theory} \label{State_PALM_Theory}

Presentation of PALM's requirements and of the algorithm steps  $\ldots$


\subsection{Clustering with PALM}

In this section we tackle the clustering problem, given in (\ref{StateEq4}), with the classical distance function defined by $d(u,v) = \|u-v\|^2$. We devise a PALM-like algorithm, based on the discussion about PALM in the previous subsection.
Since the clustering problem has a specific structure, we are ought to exploit it in the following manner.
\begin{enumerate}[(1)]
	\item The function $w \mapsto H(w,x)$, for fixed $x$, is linear and therefore there is no need to linearize it as suggested in PALM.
	\item The function $x \mapsto H(w,x)$, for fixed $w$, is quadratic and convex. Hence, there is no need to add a proximal term as suggested in PALM.
\end{enumerate}

As in the PALM algorithm, our algorithm is based on alternating minimization, with the following adaptations which are motivated by the observations mentioned above. More precisely, with respect to $w$ we suggest to regularize the first subproblem with proximal term as follows
\begin{equation}
	w^i(t+1) = \arg\!\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i - w^i(t)\|^2 \right\rbrace, \quad i=1,2, \ldots, m . \label{State_w_update}
\end{equation}
On the other hand, with respect to $x$ we perform exact minimization
\begin{equation}
	x(t+1) = \arg\!\min \left\lbrace H(w(t+1), x) \mid x \in \mathbb{R}^{nk} \right\rbrace . \label{State_x_update}
\end{equation}
It is easy to check that all subproblems, with respect to $w^i$, $i=1,2, \ldots, m$, and $x$, can be written explicitly  as follows:
\begin{equation}
w^i(t+1) = P_{\Delta} \left(w^i(t) - \frac{d^i(x(t))}{\alpha_i(t)}\right) , \quad i=1, 2, \ldots ,m , \label{StateEq7}
\end{equation}
where $P_{\Delta}$ is the orthogonal projection onto the set $\Delta$, and
\begin{equation}
x^l(t+1) = \frac{\sum_{i=1}^{m} w^i_l(t+1) a^i}{\sum_{i=1}^{m} w^i_l(t+1)} , \quad l=1, 2, \ldots ,k . \label{StateEq8}
\end{equation}
\newpage
Therefore we can record now the suggested KPALM algorithm.
\begin{framed}
\noindent \textbf{KPALM}
\begin{enumerate}[(1)]
	\item Initialization: $(w(0),x(0)) \in \Delta^m \times \mathbb{R}^{nk} .$
	\item General step $\left( t=0,1, \ldots \right)$:
	\begin{enumerate}[(2.1)]
		\item Cluster assignment: choose certain $\alpha_i(t) > 0$, $i=1,2, \ldots, m$, and compute
		\begin{equation}
			w^i(t+1) = P_{\Delta} \left(w^i(t) - \frac{d^i(x(t))}{\alpha_i(t)}\right) . \label{StateEq5}
		\end{equation}
		\item Centers update: for each $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			x^l(t+1) = \frac{\sum_{i=1}^{m} w^i_l(t+1) a^i}{\sum_{i=1}^{m} w^i_l(t+1)} . \label{StateEq6}
		\end{equation}
	\end{enumerate}
\end{enumerate}
\end{framed}

We begin our analysis of KPALM algorithm with the following boundedness property of the generated sequence. For simplicity, from now on, we denote $z(t):=\left( w(t),x(t) \right)$, $t \in \mathbb{N}$.
\begin{proposition}[Boundedness of KPALM sequence]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM. Then, the following statements hold true.
\begin{enumerate}[(i)]
	\item For all $l=1, 2, \ldots ,k$, the sequence $\left\lbrace x^l(t) \right\rbrace_{t \in \mathbb{N}}$ is contained in $Conv(\mathcal{A})$, the convex hull of $\mathcal{A}$, and therefore bounded by $M = \max\limits_{1 \leq i \leq m} \| a^i \|$
	\item The sequence $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded in $\mathbb{R}^{km} \times \mathbb{R}^{nk}$.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[(i)]
	\item  Set $\lambda_i = \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)}, i=1, 2, \ldots ,m$, then $\lambda_i \geq 0$ and $\sum\limits_{i=1}^{m} \lambda_i = 1$. From (\ref{StateEq8}) we have
	\begin{equation}
		x^l(t) = \frac{\sum_{i=1}^{m} w^i_l(t) a^i}{\sum_{i=1}^{m} w^i_l(t)} 
		= \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) a^i 
		= \sum\limits_{i=1}^{m} \lambda_i a^i \in Conv(\mathcal{A}). \label{StateBound}
	\end{equation}
	Hence $x^l(t)$ is in the convex hull of $\mathcal{A}$, for all $l = 1, 2, \ldots, k$ and $t \in \mathbb{N}$. Taking the norm of $x^l(t)$ and using (\ref{StateBound}) yields that
	\begin{equation*}
		\| x^l(t) \| = \left\lVert \sum_{i=1}^{m} \lambda_i a^i \right\lVert
		\leq \sum_{i=1}^{m} \lambda_i \| a^i \|
		\leq \sum_{i=1}^{m} \lambda_i \max\limits_{1 \leq i \leq m} \| a^i \| = M .
	\end{equation*}
	\item The sequence $\left\lbrace w(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded, since $w^i(t) \in \Delta$ for all $i=1, 2, \ldots ,m$ and $t \in \mathbb{N}$. Combined with the previous item, the result follows.
\end{enumerate} 
\end{proof}

The following assumption will be crucial for the coming analysis.
\begin{assumption} \label{StateMainAssum}
\begin{enumerate}[(i)] 
	\item The chosen sequences of parameters $\left\lbrace \alpha_i(t) \right\rbrace_{t \in \mathbb{N}}$, $i=1,2, \ldots, m$, are bounded, that is, there exist $\underline{\alpha_i} > 0$ and $\overline{\alpha_i} < \infty$ for all $i=1,2, \ldots, m$, such that
		\begin{equation}
			\underline{\alpha_i} \leq \alpha_i(t) \leq \overline{\alpha_i}, \quad \forall \: t \in \mathbb{N}.
		\end{equation}		 \label{StateMainAssum1}
	\item For all $t \in \mathbb{N}$ there exists $\underline{\beta} > 0$ such that
		\begin{equation}
			2 \min\limits_{1 \leq l \leq k} \sum\limits_{i=1}^{m} w^i_l(t) := \beta(w(t)) \geq \underline{\beta}. \label{State_beta}
		\end{equation}		 \label{StateMainAssum2}
\end{enumerate}
\end{assumption}
It should be noted that \Cref{StateMainAssum}(\ref{StateMainAssum1}) is very mild since the parameters $\alpha_i(t)$, $1 \leq i \leq m$ and $t \in \mathbb{N}$, can be chosen arbitrarily by the user and therefore it can be controlled such that the boundedness property holds true. \Cref{StateMainAssum}(\ref{StateMainAssum2}) is essential since if it is not true then $w^i_l(t)=0$ for all $1 \leq i \leq m$, which means that the center $x^l$ does not involved in the objective function.

\begin{lemma}[Strong convexity of $H(w,x)$ in $x$] \label{StateEq14}
The function $x \mapsto H(w,x)$ is strongly convex with parameter $\beta(w)$ which defined in (\ref{State_beta}), whenever $\beta(w) > 0$.
\end{lemma}

\begin{proof}
Since the function $x \mapsto H(w(t),x) = 
\sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is $C^2$, it is strongly convex if and only if the smallest eigenvalue of the corresponding Hessian matrix is positive. Indeed, the Hessian is given by

\begin{center}
$\nabla_{x^j} \nabla_{x^l} H(w,x) = 
\begin{cases} 0 &\mbox{if } j \neq l, \quad 1 \leq j,l \leq k ,
\\ 2\sum\limits_{i=1}^{m} w^i_l &\mbox{if } j = l, \quad 1 \leq j,l \leq k. \end{cases} $
\end{center}

Since the Hessian is a diagonal matrix, the smallest eigenvalue is $\beta(w) = 2\min\limits_{1 \leq l \leq k} \sum\limits_{i=1}^{m} w^i_l$, and the result follows.
\end{proof}

Now we are ready to prove the descent property of the KPALM algorithm.

\begin{proposition}[Sufficient decrease property] \label{State_KPALM_SDP}
Suppose that \Cref{StateMainAssum} holds true and let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} $ be the sequence generated by KPALM. Then, there exists $\rho_1 > 0$ such that 
\begin{equation*}
	\rho_1 \|z(t+1) - z(t)\|^2 \leq \Psi(z(t)) - \Psi(z(t+1)), \quad \forall \: t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
From step (\ref{StateEq5}), see also (\ref{State_w_update}), for each $i=1,2, \ldots, m$, we derive the following inequality
\begin{align*}
	H^i(w(t+1),x(t)) + \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 
	& = \langle w^i(t+1) , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \langle w^i(t) , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i(t) - w^i(t)\|^2 \\
	& = \langle w^i(t) , d^i(x(t)) \rangle \\
	& = H^i(w(t),x(t)) .
\end{align*}
Hence, we obtain
\begin{equation}
	\frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 
	\leq H^i(w(t),x(t)) - H^i(w(t+1),x(t)) . \label{StateEq18}
\end{equation}
Denote $\underline{\alpha} = \min\limits_{1 \leq i \leq m} \underline{\alpha_i}$. Summing inequality (\ref{StateEq18}) over $i=1, 2, \ldots ,m$ yields
\begin{align}
	\frac{\underline{\alpha}}{2} \|w(t+1) - w(t)\|^2 
	& = \frac{\underline{\alpha}}{2} \sum\limits_{i=1}^{m} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \sum\limits_{i=1}^{m} \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \sum\limits_{i=1}^{m} \left[ H^i(w(t),x(t)) - H^i(w(t+1),x(t)) \right] \\
	& = H(w(t),x(t)) - H(w(t+1),x(t)) ,  \label{StateEq16}
\end{align}
where the first inequality follows from \Cref{StateMainAssum}(\ref{StateMainAssum1}).

From \Cref{StateMainAssum}(\ref{StateMainAssum2}) we have that $\beta(w(t)) = 2 \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t) \right \rbrace \geq \underline{\beta}$, and from \Cref{StateEq14} it follows that the function $x \mapsto H(w(t),x)$ is strongly convex with parameter $\beta(w(t))$, hence it follows that
\begin{align}
	H(w(t+1)&,x(t))  - H(w(t+1),x(t+1)) \geq \\
	& \geq \left\langle \nabla_x H(w(t+1),x(t+1)) , x(t)-x(t+1) \right\rangle + \frac{\beta(w(t))}{2} \|x(t) - x(t+1)\|^2 \\
	& = \frac{\beta(w(t))}{2} \|x(t+1) - x(t)\|^2  \\
	& \geq \frac{\underline{\beta}}{2} \|x(t+1) - x(t)\|^2 , \label{StateEq17}
\end{align}
where the equality follows from (\ref{State_x_update}), since $\nabla_{x} H(w(t+1), x(t+1)) = 0$.
Set $\rho_1 = \frac{1}{2}\min\left\lbrace \underline{\alpha} , \underline{\beta} \right\rbrace$, by combining (\ref{StateEq16}) and (\ref{StateEq17}), we get
\begin{align*}
	\rho_1 \|z(t+1) &- z(t)\|^2 
	 = \rho_1 \left( \|w(t+1) - w(t)\|^2 + \|x(t+1) - x(t)\|^2  \right) \leq \\
	&\leq \left[ H(w(t),x(t)) - H(w(t+1),x(t)) \right] + \left[ H(w(t+1),x(t)) - H(w(t+1),x(t+1)) \right] \\
	&= H(z(t)) - H(z(t+1)) \\
	&= \Psi(z(t)) - \Psi(z(t+1)),
\end{align*}
where the last equality follows from the fact that $G(w(t)) = 0$ for all $t \in \mathbb{N}$ and therefore $H(z(t))=\Psi(z(t))$, $t \in \mathbb{N}$.
\end{proof}

Now, we aim to prove the subgradient lower bound for the iterates gap. The following lemma will be essential in our proof.

\begin{lemma} \label{StateEq11}
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM, then 
\begin{equation*}
	\| d^i(x(t+1) - d^i(x(t)) \| \leq 4M \| x(t+1) - x(t)\|, \quad \forall \: i=1, 2, \ldots ,m, \: t \in \mathbb{N} ,
\end{equation*}
where $M = \max\limits_{1 \leq i \leq m} \|a^i\|$.
\end{lemma}

\begin{proof}
Since $d(u,v) = \| u-v \|^2$, we get that
{\allowdisplaybreaks
\begin{align*} 
	\| d^i(x(t&+1))  - d^i(x(t)) \| 
	 = \left[ \sum\limits_{l=1}^{k} \abs{ \|x^l(t+1) - a^i\|^2 - \| x^l(t) -a^i\|^2 }^2 \right]^{\frac{1}{2}} \\
	& = \left[ \sum\limits_{l=1}^{k} \left\lvert \|x^l(t+1)\|^2 - 2\left\langle x^l(t+1),a^i \right\rangle + \|a^i\|^2 - \|x^l(t)\|^2 + 2\left\langle x^l(t),a^i \right\rangle - \|a^i\|^2 \right\rvert ^2 \right]^{\frac{1}{2}} \\ 
	& \leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \|x^l(t+1)\|^2 - \|x^l(t)\|^2 } + \abs{ 2\left\langle x^l(t) - x^l(t+1) , a^i \right\rangle } \right)^2 \right]^{\frac{1}{2}} \\ 
	& \leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \|x^l(t+1)\| - \|x^l(t)\| } \cdot \abs{ \|x^l(t+1)\| + \|x^l(t)\| } + 2 \| x^l(t) - x^l(t+1) \| \cdot \|a^i\| \right)^2 \right]^{\frac{1}{2}} \\
	& \leq \left[ \sum\limits_{l=1}^{k} \left( \|x^l(t+1) - x^l(t)\| \cdot 2M + 2 \| x^l(t+1) - x^l(t) \| M \right)^2 \right]^{\frac{1}{2}} \\
	& = \left[ \sum\limits_{l=1}^{k} (4M)^2 \|x^l(t+1) - x^l(t)\|^2 \right]^{\frac{1}{2}} 
	= 4M \| x(t+1) - x(t)\| ,
\end{align*}
}
this proves the desired result.
\end{proof}

\begin{proposition}[Subgradient lower bound for the iterates gap]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM. Then, there exists $\rho_2 > 0$ and $\gamma(t+1) \in \partial \Psi(z(t+1))$ such that 
\begin{equation*}
	\| \gamma(t+1)\| \leq \rho_2 \|z(t+1) - z(t)\|, \quad \forall \: t \in \mathbb{N} .
\end{equation*}

\end{proposition}

\begin{proof}
By the definition of $\Psi$ (see (\ref{StateEq4})) we get
\begin{equation*}
	\partial \Psi = \nabla H + \partial G  
= \left( \left( \nabla_{w^i} H^i + \partial_{w^i} \delta_{\Delta} \right)_{i=1,2, \ldots ,m} , \nabla_x H \right) .
\end{equation*}
Evaluating the last relation at $z(t+1)$ yields
\begin{align}
	\partial \Psi(z(t & + 1)) = \\
	& = \left( \left( \nabla_{w^i} H^i(w(t+1),x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1,2, \ldots ,m} , \nabla_x H(w(t+1),x(t+1)) \right) \\
	& = \left( \left( d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1,2, \ldots ,m} , \nabla_x H(w(t+1),x(t+1)) \right) \\
	& = \left( \left( d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1,2, \ldots ,m} , \mathbf{0} \right) , \label{StateEq9}
\end{align}
where the last equality follows from (\ref{State_x_update}), that is, the optimality condition of $x(t+1)$.

The optimality condition of $w^i(t+1)$ which derived from (\ref{State_w_update}), yields that for all $i=1, 2, \ldots ,m$ there exists $u^i(t+1) \in \partial \delta_{\Delta}(w^i(t+1))$ such that
\begin{equation}
	d^i(x(t)) + \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) + u^i(t+1) = \mathbf{0} . \label{StateEq10}
\end{equation}
Setting $\gamma(t+1) := \left( \left( d^i(x(t+1)) + u^i(t+1) \right)_{i=1,2, \ldots ,m}, \mathbf{0} \right)$ and from (\ref{StateEq9}) it follows that $\gamma(t+1) \in \\ \partial \Psi(z(t+1))$. Using (\ref{StateEq10}) we obtain
\begin{equation*}
	\gamma(t+1) = \left( \left( d^i(x(t+1)) - d^i(x(t)) - \alpha_i(t)(w^i(t+1) - w^i(t)) \right)_{i=1,2, \ldots, m}, \mathbf{0} \right).
\end{equation*}
Hence, by defining $\overline{\alpha} = \max\limits_{1 \leq i \leq m} \overline{\alpha_i}$, we obtain
\begin{align*}
	\| \gamma(t+1) \|
	& \leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) - \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) \| \\
	& \leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) \| + \sum\limits_{i=1}^{m} \alpha_i(t) \| w^i(t+1) - w^i(t) \| \\
	& \leq \sum\limits_{i=1}^{m} 4M \| x(t+1) - x(t) \| + m \overline{\alpha} \|z(t+1) - z(t)\| \\
	& \leq m \left( 4M + \overline{\alpha} \right) \|z(t+1) - z(t)\| , 
\end{align*}
where the third inequality follows from \Cref{StateEq11}. Define $\rho_2 = m \left( 4M + \overline{\alpha} \right)$, and the result follows.
\end{proof}


\section{Clustering: The Euclidean Norm Case}

%In this section we tackle the clustering problem with distance-like function being the Euclidean norm in $\mathbb{R}^n$, namely
%\begin{equation}
%	\min_{x^1, x^2, \ldots, x^k \in \mathbb{R}^n} \left\lbrace \sum\limits_{i=1}^{m} \min\limits_{1 \leq l \leq k} \|x^l - a^i\| \right\rbrace . \label{StateEq30}
%\end{equation}
%
%We are about to develop an algorithm that is based on PALM theory to treat this problem. However, first we need to discuss the Fermat-Weber problem that bears close relation with the algorithm that we will present later, and develop some useful tools.
%
%\subsection{The Smoothed Fermat-Weber Problem}
%Solving the smoothed Fermat-Weber plays a significant role in the algorithm that addresses the clustering problem with Euclidean norm distance-like function.
%The Fermat-Weber problem is formulated as follows
%\begin{equation}
%	\min_{x \in \mathbb{R}^n} \left\lbrace f(x) := \sum\limits_{i=1}^{m} w_i\|x - a^i\| \right\rbrace , \label{StateEq60}
%\end{equation}
%where $w_i>0, \: i=1,2, \ldots, m$, are given positive weights and $\mathcal{A} = \left\lbrace a^1, a^2, \ldots, a^m \right\rbrace \subset \mathbb{R}^n$ are given vectors. As shown in [BS2015] this problem can be solved via the consecutive appliance of the operator $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ defined by
%\begin{equation*}
%	T(x) = \frac{1}{\sum\limits_{i=1}^{m}\frac{w_i}{\|x - a^i\|}} \sum\limits_{i=1}^{m}\frac{w_i a^i}{\|x - a^i\|} .
%\end{equation*}
%It is easily noticed that $f(x)$ is not differentiable over $\mathcal{A}$. For our purposes we are interested in the smoothed Fermat-Weber problem, that can be formulated in the following manner
%\begin{equation}
%	\min_{x \in \mathbb{R}^n} \left\lbrace f_{\varepsilon}(x) := \sum\limits_{i=1}^{m} w_i \left( \|x - a^i\|^2 + {\varepsilon}^2 \right)^{1/2} \right\rbrace , \label{StateEq61}
%\end{equation}
%with $\varepsilon > 0$ being some small perturbation constant. Next we introduce the operator $T_{\varepsilon}: \mathbb{R}^n \rightarrow \mathbb{R}^n$ defined by 
%\begin{equation*}
%	T_{\varepsilon}(x) = \frac{1}{\sum\limits_{i=1}^{m}\frac{w_i}{\left( \|x - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}}} \sum\limits_{i=1}^{m}\frac{w_i a^i}{\left( \|x - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} .
%\end{equation*}
%
%This version of the operator together with its properties that are to be discussed below are the cornerstone to prove the properties needed by PALM, and in turn to show the convergence of the sequence generated by the algorithm proposed to tackle the smooth version of the clustering problem presented later on.
%In order to prove some properties of $T_{\varepsilon}$, which are the same as the properties of $T$ described in [BS2015], we also will need an auxiliary function $h_{\varepsilon}: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ given by
%\begin{equation*}
%	h_{\varepsilon}(x,y) = \sum\limits_{i=1}^m \frac{w_i \left( \|x-a^i\|^2 + {\varepsilon}^2 \right)}{\left( \|y-a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} .
%\end{equation*}
%Another useful function $L_{\varepsilon}: \mathbb{R}^n \rightarrow \mathbb{R}$ that serves somewhat like Lipschitz function for the gradient of $f_{\varepsilon}$ is defined by
%\begin{equation*}
%	L_{\varepsilon}(x) = \sum\limits_{i=1}^{m}\frac{w_i}{\left( \|x - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} .
%\end{equation*}
%It is easy to verify the following equality
%\begin{equation}
%	T_{\varepsilon}(x) = x - \frac{1}{L_{\varepsilon}(x)}\nabla f_{\varepsilon}(x) , \quad \forall x \in \mathbb{R}^n . \label{StateEq62}
%\end{equation}


\subsection{A Smoothed Clustering Problem}

In the previous section we have formulated Problem (\ref{StateEq2}) in the following equivalent form
\begin{equation*}
	\min \left\lbrace \Psi(z) := H(w,x) + G(w) \mid z := (w,x) \in \mathbb{R}^{km} \times \mathbb{R}^{nk} \right\rbrace ,
\end{equation*}
where , in this setting, the involved functions are
\begin{equation*}
	H(w,x) = \sum\limits_{i=1}^{m} \left\langle w^i , d^i(x) \right\rangle
	= \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l \| x^l - a^i \| ,
\end{equation*}
and 
\begin{equation*}
G(w) = \sum\limits_{i=1}^{m} \delta_{\Delta}(w^i) .
\end{equation*}

In order to be able to use the theory mentioned in Section \ref{State_PALM_Theory}, we have used the fact that the coupled function $H(w,x)$ is smooth, which is not the case now. Therefore, for any $\varepsilon > 0$, it leads us to the following smoothed form of the clustering problem
\begin{equation}
	\min \left\lbrace \Psi_{\varepsilon}(z) := H_{\varepsilon}(w,x) + G(w) \mid z := (w,x) \in \mathbb{R}^{km} \times \mathbb{R}^{nk} \right\rbrace , \label{StateEq31}
\end{equation}
where 
\begin{equation}
	H_{\varepsilon}(w,x) = \sum\limits_{l=1}^{k} H^l_{\varepsilon}(w,x)
	= \sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \left( \| x^l - a^i \|^2 + {\epsilon}^2 \right)^{1/2} , \label{StateEq34}
\end{equation}
and for all $i=1,2, \ldots, m$,
\begin{equation*}
	d_{\varepsilon}^i(x) = \left( \left( \|x^1 - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}, \left( \|x^2 - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}, \ldots , \left( \|x^k - a^i\|^2 + {\varepsilon}^2 \right)^{1/2} \right) \in \mathbb{R}^k .
\end{equation*}
Note that $\Psi_{\varepsilon}(z)$ is a perturbed form of $\Psi(z)$ for some small $\varepsilon > 0$, and $\Psi_0(z)=\Psi(z)$.

Now we would like to develop an algorithm which is based on the methodology of PALM to solve Problem (\ref{StateEq31}). It is easy to see that with respect to $w$, the objective $\Psi_{\varepsilon}$ keeps on the same structure as $\Psi$ and therefore we apply the same step as in KPALM. More precisely, for all $i=1,2, \ldots, m$, we have
\begin{align*}
	w^i(t+1) &= \arg\!\min\limits_{w^i \in \Delta} \left\lbrace \left\langle w^i, d^i_{\varepsilon}(x(t)) \right\rangle + \frac{\alpha_i(t)}{2} \|w^i -w^i(t)\|^2 \right\rbrace \\
	&= P_{\Delta} \left( w^i(t) - \frac{d^i_{\varepsilon}(x(t))}{\alpha_i(t)} \right), \quad \forall \: t \in \mathbb{N},
\end{align*}
where $\alpha_i(t)$, $i=1,2, \ldots, m$, is arbitrarily chosen. On the other hand, with respect to $x$ we tackle the subproblem differently than in KPALM. Here we follow exactly the idea of PALM, that is, linearizing the function and adding regularizing term
\begin{equation*}
	x^l(t+1) = \arg\!\min\limits_{x^l} \left\lbrace \left\langle x^l - x^l(t) , \nabla_{x^l}H_{\varepsilon}(w(t+1),x(t)) \right\rangle + \frac{L^l_{\varepsilon}(w(t+1),x(t))}{2} \|x^l - x^l(t)\|^2 \right\rbrace,
\end{equation*}
where 
\begin{equation*}
	L^l_{\varepsilon}(w(t+1),x(t)) = \sum\limits_{i=1}^{m} \frac{w^i_l(t+1)}{\left( \|x^l(t)-a^i\|^2 + {\varepsilon}^2 \right)^{1/2}}, \quad \forall \: l=1,2, \ldots, k.
\end{equation*}

%Next we extend the notations of the previous subsection, so that the functions and operators defined there are to be dependent on the weights $w$. For each $1 \leq l \leq k$, denote $w_l = \left( w_l^1, w_l^2, \ldots , w_l^m \right) \\ \in \mathbb{R}_+^m$ and define
%\begin{equation*}
% L_{\varepsilon}^{w_l}(x^l) = \sum\limits_{i=1}^m \frac{w^i_l}{\left( \|x^l-a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} ,
%\end{equation*}
%and
%\begin{equation*}
% T_{\varepsilon}^{w_l}(x^l) = \frac{1}{L_{\varepsilon}^{w_l}(x^l)} \sum\limits_{i=1}^m \frac{w^i_l a^i}{\left( \|x^l-a^i\|^2 + {\varepsilon}^2 \right)^{1/2}}.
%\end{equation*}
%For all $1 \leq l \leq k$ we define $H_{\varepsilon}^{w_l}: \mathbb{R}^n \rightarrow \mathbb{R}$ as follows
%\begin{equation*}
% H_{\varepsilon}^{w_l}(x^l) = \sum\limits_{i=1}^m w^i_l \left( \|x^l-a^i\|^2 + {\varepsilon}^2 \right)^{1/2} ,
%\end{equation*}
%thus we have 
%\begin{equation*}
% H_{\varepsilon}(w,x) = \sum\limits_{l=1}^k H_{\varepsilon}^{w_l}(x^l) .
%\end{equation*}

Now we present our algorithm for solving Problem (\ref{StateEq31}), we call it $\varepsilon$-KPALM.  The algorithm alternates between cluster assignment step, similar to  KPALM, and centers update step that is based on certain gradient step.

\begin{framed}
\noindent \textbf{$\varepsilon$-KPALM}
\begin{enumerate}[(1)]
	\item Initialization: $(w(0),x(0)) \in \Delta^m \times \mathbb{R}^{nk} .$
	\item General step $\left( t=0,1, \ldots \right)$:
	\begin{enumerate}[(2.1)]
		\item Cluster assignment: choose certain $\alpha_i(t) > 0$, $i=1,2, \ldots, m$, and compute
		\begin{equation}
			w^i(t+1) = P_{\Delta} \left(w^i(t) - \frac{d_{\varepsilon}^i(x(t))}{\alpha_i(t)}\right) . \label{StateEq32}
		\end{equation}
		\item Centers update: for each $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			x^l(t+1) = x^l(t) - \frac{1}{L^l_{\varepsilon}(w(t+1), x(t))}\nabla_{x^l} H_{\varepsilon}(w(t+1), x(t)) . \label{StateEq33}
		\end{equation}
	\end{enumerate}
\end{enumerate}
\end{framed}

%
%\begin{enumerate}[(1)]
%	\item Initialization: Set $t=0$, and pick random vectors $(w(0),x(0)) \in \Delta^m \times \mathbb{R}^{nk} .$
%
%	\item For each $t=0,1, \ldots$ generate a sequence $\left\lbrace(w(t),x(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
%	\begin{enumerate}[(2.1)]
%		\item Cluster Assignment: Take any $\alpha_i(t) > 0$ and for each $i=1, 2, \ldots ,m$ compute
%		\begin{equation}
%		\begin{split}
%			w^i(t+1) 
%			&= \arg\!\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d_{\varepsilon}^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i - w^i(t)\|^2 \right\rbrace \\ 
%			&= P_{\Delta} \left(w^i(t) - \frac{d_{\varepsilon}^i(x(t))}{\alpha_i(t)}\right) . \label{StateEq32}
%		\end{split}
%		\end{equation}
%		
%		\item Center Update: For each $l=1, 2, \ldots ,k$ compute 
%		\begin{equation}
%			x^l(t+1) = T_{\varepsilon}^{w_l(t+1)}(x^l(t)) . \label{StateEq33}
%		\end{equation}
%	\end{enumerate}
%\end{enumerate}
%
%\begin{remark} 
%	\begin{enumerate}[(i)]
%	\item \Cref{StateEq17} is still valid, hence the center update step in (\ref{StateEq33}) is well defined.
%	\item It is easy to verify that for all $1 \leq l \leq k$ the following equations hold true:
%	\begin{equation}
%		\nabla H_{\varepsilon}^{w_l}(x^l) = \sum\limits_{i=1}^{m} w^i_l \frac{x^l - a^i}{\left( \|x^l - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} , \quad \forall \: x^l \in \mathbb{R}^n, \label{StateEq35}
%	\end{equation}
%	and that 
%	\begin{equation}
%		T_{\varepsilon}^{w_l}(x^l) = x^l - \frac{1}{L_{\varepsilon}^{w_l}(x^l)}\nabla H_{\varepsilon}^{w_l}(x^l), \quad \forall \: x^l \in \mathbb{R}^n . \label{StateEq36}
%	\end{equation}
%	\end{enumerate}
%\end{remark}

Similarly to the KPALM algorithm, the sequence generated by $\varepsilon$-KPALM is also bounded, since here we also have that
\begin{align*}
	x^l(t+1) &= x^l(t) - \frac{1}{L^l_{\varepsilon}(w(t+1),x(t))} \nabla_{x^l} H(w(t+1),x(t)) \\
	&= x^l(t) - \frac{1}{L^l_{\varepsilon}(w(t+1),x(t))} \sum\limits_{i=1}^{m} w^i_l(t+1) \cdot \frac{x^l(t) - a^i}{\left( \|x^l(t) - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} \\
	&= \frac{1}{L^l_{\varepsilon}(w(t+1),x(t))} \sum\limits_{i=1}^{m} \left( \frac{w^i_l(t+1)}{\left( \|x^l(t) - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} \right) a^i \in Conv(\mathcal{A}).
\end{align*}

Before we will be able to prove the two properties needed for global convergence of the sequence $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ generated by $\varepsilon$-KPALM, we will need several auxiliary results. For the simplicity of the expositions we define the  function $f_{\varepsilon}: \mathbb{R}^n \rightarrow \mathbb{R}$ by
\begin{equation*}
	f_{\varepsilon}(x) = \sum\limits_{i=1}^{m} v_i \left( \|x - a^i\|^2 + {\varepsilon}^2 \right)^{1/2},
\end{equation*}
for fixed positive numbers $v_1,v_2, \ldots, v_m \in \mathbb{R}$ and $a^i \in \mathbb{R}^n$, $i=1,2, \ldots, m$. We also need the following auxiliary function $h_{\varepsilon}: \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ given by
\begin{equation*}
	h_{\varepsilon}(x,y) = \sum\limits_{i=1}^m \frac{v_i \left( \|x-a^i\|^2 + {\varepsilon}^2 \right)}{\left( \|y-a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} .
\end{equation*}
%Finally need introduce the following two operators, $T_{\varepsilon}: \mathbb{R}^n \rightarrow \mathbb{R}^n$ defined by 
%\begin{equation*}
%	T_{\varepsilon}(x) = \frac{1}{\sum\limits_{i=1}^{m}\frac{w_i}{\left( \|x - b^i\|^2 + {\varepsilon}^2 \right)^{1/2}}} \sum\limits_{i=1}^{m}\frac{w_i b^i}{\left( \|x - b^i\|^2 + {\varepsilon}^2 \right)^{1/2}} ,
%\end{equation*}
%and 
Finally we introduce the following operator, $L_{\varepsilon}: \mathbb{R}^n \rightarrow \mathbb{R}$ defined by
\begin{equation*}
	L_{\varepsilon}(x) = \sum\limits_{i=1}^{m}\frac{v_i}{\left( \|x - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} .
\end{equation*}

\begin{lemma} [Properties of the auxiliary function $h_{\varepsilon}$]\label{State_h_prop}
The following properties of $h_{\varepsilon}$ hold.
\begin{enumerate}[(i)]
	\item For any $y \in \mathbb{R}^n$, \label{State_h_prop1}
	\begin{equation*}
		h_{\varepsilon}(y,y) = f_{\varepsilon}(y) .
	\end{equation*}
	\item For any $x,y \in \mathbb{R}^n$, \label{State_h_prop2}
	\begin{equation*}
		h_{\varepsilon}(x,y) \geq 2f_{\varepsilon}(x) - f_{\varepsilon}(y) .
	\end{equation*}
%	\item For any $y \in \mathbb{R}^n$, \label{State_h_prop3}
%	\begin{equation*}
%	 	T_{\varepsilon}(y) = \arg\!\min_{x \in \mathbb{R}^n} h_{\varepsilon}(x,y) .
%	 \end{equation*}
	\item For any $x,y \in \mathbb{R}^n$, \label{State_h_prop4}
	\begin{equation*}
		f_{\varepsilon}(x) \leq f_{\varepsilon}(y) + \left\langle \nabla f_{\varepsilon}(y), x-y \right\rangle + \frac{L_{\varepsilon}}{2}\|x-y\|^2 .
	\end{equation*}
\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}[(i)]
	\item Follows by substituting $x=y$ in $h_{\varepsilon}(x,y)$.
	\item For any two numbers $a \in \mathbb{R}$ and $b>0$ the inequality 
	\begin{equation*}
		\frac{a^2}{b} \geq 2a - b ,
	\end{equation*}
	holds true. Thus, for every $i=1,2, \ldots ,m$, we have that
	\begin{equation*}
		\frac{\|x-a^i\|^2 + {\varepsilon}^2}{\left( \|y-a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} \geq 2\left( \|x-a^i\|^2 + {\varepsilon}^2 \right)^{1/2} - \left( \|y-a^i\|^2 + {\varepsilon}^2 \right)^{1/2} .
	\end{equation*}
	Multiplying the last inequality by $v_i$ and summing over $i=1,2, \ldots, m$, the results follows. 
%	\item The function $x \mapsto h_{\varepsilon}(x,y)$ is strongly convex and its unique minimizer is determined by the optimality equation
%	\begin{equation*}
%		\nabla_x h_{\varepsilon}(x,y) = \sum\limits_{i=1}^m \frac{2w_i \left( x-b^i \right) }{\left( \|y-b^i\|^2 + {\varepsilon}^2 \right)^{1/2}} = 0 .
%	\end{equation*}
%	Simple algebraic manipulation leads to the relation
%	\begin{equation*}
%		x = T_{\varepsilon}(y) ,
%	\end{equation*}
%	and the desired results follows.
	\item The function $x \mapsto h_{\varepsilon}(x,y)$ is quadratic with associated matrix $L_{\varepsilon}(y)\textbf{I}$. Therefore, its second-order taylor expansion around y leads to the following identity
	\begin{equation*}
		h_{\varepsilon}(x,y) = h_{\varepsilon}(y,y) + \left\langle \nabla_x h_{\varepsilon}(y,y) , x-y \right\rangle + L_{\varepsilon}(y) \|x-y\|^2 .
	\end{equation*}
	Using the first two items and the fact that $\nabla_x h_{\varepsilon}(y,y) = 2\nabla f_{\varepsilon}(y)$ yields the desired result.
\end{enumerate}
\end{proof}

%The following proofs are based on the properties of the auxiliary function $h_{\varepsilon}$, and they are similar to the proofs in [BS2015], hence we will just state them here. \Cref{StateEq65} does not appear in that paper, and its proof is given here.
%
%\begin{lemma}[Monotonicity property of $T_{\varepsilon}$, similar to  (BS2015, Lemma 3.2, page 7)] For every $y \in \mathbb{R}^n$ we have
%\begin{equation*}
%	f_{\varepsilon}(T_{\varepsilon}(y)) \leq f_{\varepsilon}(y).
%\end{equation*}
%\end{lemma}
%
%\begin{lemma}[Decent lemma for function $f_{\varepsilon}$, similar to  (BS2015, Lemma 5.1, page 10)] For every $y \in \mathbb{R}^n$ we have
%\begin{equation*}
%	f_{\varepsilon}(T_{\varepsilon}(y)) \leq f_{\varepsilon}(y) + \left\langle \nabla f_{\varepsilon}(y), T_{\varepsilon}(y) - y \right\rangle + \frac{L_{\varepsilon}(y)}{2} \|T_{\varepsilon}(y) - y\|^2 .
%\end{equation*}
%\end{lemma}
%
%\begin{lemma} [Similar to  (BS2015, Lemma 5.2, page 12)] \label{StateEq64} 
%For every $x,y \in \mathbb{R}^n$ we have
%\begin{equation*}
%	f_{\varepsilon}(T_{\varepsilon}(y)) - f_{\varepsilon}(x) \leq \frac{L_{\varepsilon}(y)}{2} \left( \|y-x\|^2 - \|T_{\varepsilon}(y) - x\|^2 \right) .
%\end{equation*}
%\end{lemma}

Now we can prove that the function $f_{\varepsilon}$ has Lipschitz continuous gradient.

\begin{lemma} \label{StateEq65}
For all $y,z \in \mathbb{R}^n$ the following statement holds true
\begin{equation*}
	\| \nabla f_{\varepsilon}(y) - \nabla f_{\varepsilon}(z) \| \leq \frac{2L_{\varepsilon}(z)L_{\varepsilon}(y)}{L_{\varepsilon}(z)+L_{\varepsilon}(y)} \|z - y\|.
\end{equation*}
\end{lemma}

\begin{proof}
Let $z \in \mathbb{R}^n$ be a fixed vector. Define the following two functions
\begin{equation*}
	\widetilde{f_{\varepsilon}}(y) = f_{\varepsilon}(y) - \left\langle \nabla f_{\varepsilon}(z), y \right\rangle ,
\end{equation*}
and 
 \begin{equation*}
	\widetilde{h_{\varepsilon}}(x,y) = h_{\varepsilon}(x,y) - \left\langle \nabla f_{\varepsilon}(z), x \right\rangle .
\end{equation*}
It is clear that $x \mapsto \widetilde{h_{\varepsilon}}(x,y)$ is also a quadratic function with associated matrix $L_{\varepsilon}(y)\mathbf{I}$. Therefore, from \Cref{State_h_prop}(\ref{State_h_prop1}) we can write
\begin{align}
	\widetilde{h_{\varepsilon}}(x,y) &= \widetilde{h_{\varepsilon}}(y,y) + \left\langle \nabla_x \widetilde{h_{\varepsilon}}(y,y), x-y \right\rangle + L_{\varepsilon}(y) \|x-y\|^2 \\
	&= \widetilde{f_{\varepsilon}}(y) + \left\langle 2\nabla f_{\varepsilon}(y) - \nabla f_{\varepsilon}(z), x-y \right\rangle + L_{\varepsilon}(y) \|x-y\|^2.	\label{StateEq66}
\end{align}
On the other hand, from \Cref{State_h_prop}(\ref{State_h_prop2}) we have that
\begin{align}
	\widetilde{h_{\varepsilon}}(x,y) &= h_{\varepsilon}(x,y) - \left\langle	\nabla f_{\varepsilon}(z),x \right\rangle \geq 2f_{\varepsilon}(x) - f_{\varepsilon}(y) - \left\langle \nabla f_{\varepsilon}(z),x \right\rangle \\
	&= 2 \widetilde{f_{\varepsilon}}(x) - \widetilde{f_{\varepsilon}}(y) + \left\langle \nabla f_{\varepsilon}(z), x-y \right\rangle, \label{StateEq67}
\end{align}
where the last equality follows from the definition of $\widetilde{f_{\varepsilon}}$. Combining (\ref{StateEq66}) and (\ref{StateEq67}) yields
\begin{align*}
	2\widetilde{f_{\varepsilon}}(x) &\leq 2\widetilde{f_{\varepsilon}}(y) + 2 \left\langle \nabla f_{\varepsilon}(y) - \nabla f_{\varepsilon}(z), x-y \right\rangle + L_{\varepsilon}(y) \|x-y\|^2 \\
	&= 2\widetilde{f_{\varepsilon}}(y) + 2 \left\langle \nabla \widetilde{f_{\varepsilon}}(y), x-y \right\rangle + L_{\varepsilon}(y) \|x-y\|^2.
\end{align*}
Dividing the last inequality by $2$ leads to
\begin{equation}
	\widetilde{f_{\varepsilon}}(x) \leq \widetilde{f_{\varepsilon}}(y) + \left\langle \nabla \widetilde{f_{\varepsilon}}(y), x-y \right\rangle + \frac{L_{\varepsilon}(y)}{2} \|x-y\|^2. \label{StateEq68}
\end{equation}
It is clear that the optimal point of $\widetilde{f_{\varepsilon}}$ is $z$ since $\nabla \widetilde{f_{\varepsilon}}(z) = 0$, therefore using (\ref{StateEq68}) with \\$x = y - \left( 1/L_{\varepsilon}(y) \right) \nabla \widetilde{f_{\varepsilon}}(y)$ yields
\begin{align*}
	\widetilde{f_{\varepsilon}}(z) &\leq \widetilde{f_{\varepsilon}}\left( y - \frac{1}{L_{\varepsilon}(y)} \nabla \widetilde{f_{\varepsilon}}(y) \right) \leq \widetilde{f_{\varepsilon}}(y) + \left\langle \nabla \widetilde{f_{\varepsilon}}(y), - \frac{1}{L_{\varepsilon}(y)} \nabla \widetilde{f_{\varepsilon}}(y) \right\rangle + \frac{L_{\varepsilon}(y)}{2} \left\lVert \frac{1}{L_{\varepsilon}(y)} \nabla \widetilde{f_{\varepsilon}}(y) \right\rVert ^2 \\
	&= \widetilde{f_{\varepsilon}}(y) - \frac{1}{2 L_{\varepsilon}(y)} \left\lVert \nabla \widetilde{f_{\varepsilon}}(y) \right\rVert ^2.
\end{align*}
Thus, using the definition of $\widetilde{f_{\varepsilon}}$ and the fact that $\nabla \widetilde{f_{\varepsilon}}(y) = \nabla f_{\varepsilon}(y) - \nabla f_{\varepsilon}(z)$, yields that
\begin{equation*}
	f_{\varepsilon}(z) \leq f_{\varepsilon}(y) + \left\langle \nabla f_{\varepsilon}(z), z - y \right\rangle - \frac{1}{2 L_{\varepsilon}(y)} \| \nabla f_{\varepsilon}(y) - \nabla f_{\varepsilon}(z) \|^2 .
\end{equation*}
Now, following the same arguments we can show that
\begin{equation*}
	f_{\varepsilon}(y) \leq f_{\varepsilon}(z) + \left\langle \nabla f_{\varepsilon}(y), y - z \right\rangle - \frac{1}{2 L_{\varepsilon}(z)} \| \nabla f_{\varepsilon}(z) - \nabla f_{\varepsilon}(y) \|^2 .
\end{equation*}
Combining the last two inequalities yields that
\begin{equation*}
	\left( \frac{1}{2 L_{\varepsilon}(z)} + \frac{1}{2 L_{\varepsilon}(y)} \right) \| \nabla f_{\varepsilon}(y) - \nabla f_{\varepsilon}(z) \|^2 \leq \left\langle \nabla f_{\varepsilon}(z) - \nabla f_{\varepsilon}(y), z - y \right\rangle ,
\end{equation*}
that is, 
\begin{equation*}
	\| \nabla f_{\varepsilon}(y) - \nabla f_{\varepsilon}(z) \| \leq \frac{2L_{\varepsilon}(z)L_{\varepsilon}(y)}{L_{\varepsilon}(z) + L_{\varepsilon}(y)} \|z - y\| ,
\end{equation*}
for all $z,y \in \mathbb{R}^n$. This proves the desired result.
\end{proof}

%The following lemma will be used to prove the sufficient decrease property of the KPALM algorithm.
%
%\begin{lemma} \label{State_SD_lemma}
%The following statement holds true.
%\begin{equation*}
%	f_{\varepsilon}(T_{\varepsilon}(y)) \leq f_{\varepsilon}(y) + \left\langle \nabla f_{\varepsilon}(y), T_{\varepsilon}(y)-y \right\rangle + \frac{L_{\varepsilon}(y)}{2} \|T_{\varepsilon}(y)-y\|^2,\quad \forall \: y \in \mathbb{R}^n .
%\end{equation*}	
%\end{lemma} 
%
%\begin{proof}
%From \Cref{State_h_prop}(\ref{State_h_prop4}) we have
%\begin{equation*}
%	h_{\varepsilon}(x,y) = h_{\varepsilon}(y,y) + \left\langle \nabla_x h_{\varepsilon}(y,y) , x-y \right\rangle + L_{\varepsilon}(y) \|x-y\|^2 .
%\end{equation*}
%\Cref{State_h_prop}(\ref{State_h_prop4}) states that $h_{\varepsilon}(y,y)=f_{\varepsilon}(y)$ and since $\nabla_{x} h_{\varepsilon}(y,y) = 2\nabla f_{\varepsilon}(y)$ we obtain
%\begin{equation*}
%	h_{\varepsilon}(x,y) = f_{\varepsilon}(y) + 2\left\langle \nabla f_{\varepsilon}(y), x-y \right\rangle + L_{\varepsilon}(y)\|x-y\|^2.
%\end{equation*}
%Substituting $x=T_{\varepsilon}(y)$ in the last identity yields
%\begin{equation*}
%	h_{\varepsilon}(T_{\varepsilon}(y),y) = f_{\varepsilon}(y) + 2\left\langle \nabla f_{\varepsilon}(y), T_{\varepsilon}(y)-y \right\rangle + L_{\varepsilon}(y) \|T_{\varepsilon}(y)  -y\|^2 .
%\end{equation*}
%Using \Cref{State_h_prop}(\ref{State_h_prop2}) leads to
%\begin{equation*}
%	2f_{\varepsilon} - f_{\varepsilon} \leq f_{\varepsilon}(y) + 2\left\langle \nabla f_{\varepsilon}(y), T_{\varepsilon}(y)-y \right\rangle + L_{\varepsilon}(y) \|T_{\varepsilon}(y)  -y\|^2 .
%\end{equation*}
%Rearranging the last inequality yields the desired result.
%\end{proof}

Now we get back to $\varepsilon$-KPALM algorithm and prove few technical results about the involved functions which are based on the auxiliary results obtained above.

\begin{proposition}[Bounds for $L^l_{\varepsilon}$] \label{State_L_bounds}
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by $\varepsilon$-KPALM. Then, the following two statements hold true.
\begin{enumerate}[(i)]
	\item For all $t \in \mathbb{N}$ and $l=1,2, \ldots, k$ we have
	\begin{equation*}
		L^l_{\varepsilon}(w(t+1),x(t)) \geq \frac{\underline{\beta}}{\left( {d_{\mathcal{A}}}^2 + {\varepsilon}^2 \right)^{1/2}} ,
	\end{equation*}
	where $d_{\mathcal{A}}$ is the diameter of $Conv(\mathcal{A})$ and $\underline{\beta}$ is given in (\ref{State_beta}). \label{State_L_bounds1}
	\item For all $t \in \mathbb{N}$ and $l=1,2, \ldots, k$ we have
	\begin{equation*}
		L^l_{\varepsilon}(w(t+1),x(t)) \leq \frac{m}{\varepsilon} .
	\end{equation*} \label{State_L_bounds2}
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[(i)]
	\item From \Cref{StateMainAssum}(\ref{StateMainAssum2}) and the fact that $x^l(t) \in Conv(\mathcal{A})$ for all $1 \leq l \leq k$, it follows that
	\begin{equation*}
		L^l_{\varepsilon}(w(t+1),x(t)) = \sum\limits_{i=1}^{m} \frac{w^i_l(t+1)}{\left( \|x^l(t) - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} \geq \frac{\sum_{i=1}^{m}w^i_l(t+1)}{\left( {d_{\mathcal{A}}}^2 + {\varepsilon}^2 \right)^{1/2}} \geq \frac{\underline{\beta}}{\left( {d_{\mathcal{A}}}^2 + {\varepsilon}^2 \right)^{1/2}} ,
	\end{equation*}
	as asserted. 
	\item Since $w(t+1) \in {\Delta}^m$ we have
	\begin{equation*}
		L^l_{\varepsilon}(w(t+1),x(t)) = \sum\limits_{i=1}^{m} \frac{w^i_l(t+1)}{\left( \|x^l(t) - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} \leq \sum\limits_{i=1}^{m} \frac{1}{\varepsilon} = \frac{m}{\varepsilon} ,
	\end{equation*}
	as asserted.
\end{enumerate}
\end{proof}

Now we prove the following result.

\begin{proposition} \label{State_H_eps_prop}
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by $\varepsilon$-KPALM. Then, for all $t \in \mathbb{N}$ we have
\begin{align*}
	H_{\varepsilon}(w(t+1),x(t+1)) 
	&\leq H_{\varepsilon}(w(t+1),x(t)) + \left\langle \nabla_x H_{\varepsilon}(w(t+1),x(t)), x(t+1)-x(t) \right\rangle \\
	&+ \sum\limits_{l=1}^{k} \frac{L^l_{\varepsilon}(w(t+1),x(t))}{2} \|x^l(t+1)-x^l(t)\|^2 .
\end{align*}
\end{proposition}

\begin{proof}
%We introduce the following function $H^l_{\varepsilon}: \mathbb{R}^{km} \times \mathbb{R}^{nk} \rightarrow \mathbb{R}$ defined by
%\begin{equation*}
%	H^l_{\varepsilon}(w,x) = \sum\limits_{i=1}^{m} w^i_l \left( \|x^l - a^i\|^2 + {\varepsilon}^2 \right)^{1/2} .
%\end{equation*}
%Applying \Cref{State_SD_lemma} yields
%Applying \Cref{State_h_prop}(\ref{State_h_prop4}) over $x=x^l(t+1)$ and $y=x^l(t)$, with $(w_1,w_2, \ldots, w_m) = \\(w^1_l(t+1),w^2_l(t+1), \ldots, w^m_l(t+1))$ and $(b^1,b^2, \ldots, b^m)=(a^1,a^2, \ldots, a^m)$ yields
%\begin{equation}
%\begin{aligned}
%	h_{\varepsilon}(x^l(t+1),x^l(t)) &= 
%	h_{\varepsilon}(x^l(t),x^l(t)) + \left\langle \nabla_x h_{\varepsilon}(x^l(t),x^l(t)), x^l(t+1)-x^l(t) \right\rangle \\
%	&+ L^l_{\varepsilon}(x^l(t))\|x^l(t+1)-x^l(t)\|^2 . \label{StateEq29}
%\end{aligned}
%\end{equation}
%From \Cref{State_h_prop}(\ref{State_h_prop1}) we have
%\begin{equation*}
%	h_{\varepsilon}(x^l(t),x^l(t)) = f_{\varepsilon}(x^l(t)) = \sum\limits_{i=1}^{m} w^l_l(t+1)\left( \|x^l(t)-a^i\|^2 + {\varepsilon}^2\right)^{1/2} .
%\end{equation*}
%Using the fact that $\nabla_x h_{\varepsilon}(y,y)=2f_{\varepsilon}(y)$, for all $y \in \mathbb{R}^n$, yields
%\begin{equation*}
%	\nabla_x h_{\varepsilon}(x^l(t),x^l(t)) = 2\nabla f_{\varepsilon}(x^l(t))= 2\sum\limits_{i=1}^{m}  \frac{w^i_l(t+1)(x^l(t) - a^i)}{\left( \|x^l(t)-a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} = 2\nabla_{x^l} H_{\varepsilon}(w(t+1),x(t)) .
%\end{equation*}
%Plugging last two identities into (\ref{StateEq29}) yields
%\begin{equation}
%\begin{aligned} 
%	h_{\varepsilon}(x^l(t+1),x^l(t)) &= 
%	\sum\limits_{i=1}^{m} w^l_l(t+1)\left( \|x^l(t)-a^i\|^2 + {\varepsilon}^2\right)^{1/2} \\
%	&+ 2\left\langle \nabla_{x^l} H_{\varepsilon}(w(t+1),x(t)), x^l(t+1)-x^l(t) \right\rangle \\
%	&+ L^l_{\varepsilon}(x^l(t))\|x^l(t+1)-x^l(t)\|^2 . \label{StateEq28}
%\end{aligned}
%\end{equation}
%From \Cref{State_h_prop}(\ref{State_h_prop2}) we obtain
%\begin{equation}
%	h_{\varepsilon}(x^l(t+1),x^l(t)) \geq 2f_{\varepsilon}(x^l(t+1)) - f_{\varepsilon}(x^l(t)). \label{StateEq27}
%\end{equation}
%Combining (\ref{StateEq28}) with (\ref{StateEq27}), dividing by $2$ and rearranging the terms yields
%\begin{align*}
%	\sum\limits_{i=1}^{m} w^i_l(t+1) \left( \|x^l(t+1) - a^i\|^2 + {\varepsilon}^2 \right)^{1/2} 
%	&\leq \sum\limits_{i=1}^{m} w^i_l(t+1) \left( \|x^l(t) - a^i\|^2 + {\varepsilon}^2 \right)^{1/2} \\
%	&+ \left\langle \nabla_{x^l} H_{\varepsilon}(w(t+1),x(t)), x^l(t+1)-x^l(t) \right\rangle \\
%	&+ \frac{L^l_{\varepsilon}(x^l(t))}{2} \|x^l(t+1)-x^l(t)\|^2 .
%\end{align*}
%\begin{equation*}
%	H^l_{\varepsilon}(w(t+1),x(t+1)) \leq H^l_{\varepsilon}(x(t)) + \left\langle \nabla_{x^l} H^l_{\varepsilon}(x(t)) , x^l(t+1)-x^l(t) \right\rangle + \frac{L^l_{\varepsilon}(x(t))}{2} \|x^l(t+1)-x^l(t)\|^2 .
%\end{equation*}
By definition (see (\ref{StateEq34})) we have, for $i=1,2, \ldots, m$, that
\begin{equation*}
	H^l_{\varepsilon}(w(t+1),x(t)) = f_{\varepsilon}(x^l) ,
\end{equation*}
where $v_i=w^i_l(t+1)$, $i=1,2, \ldots, m$. Therefore, by applying \Cref{State_h_prop}(\ref{State_h_prop4}) with $x=x^l(t+1)$ and $y=x^l(t)$, we get
\begin{align*}
	H^l_{\varepsilon}(w(t+1),x(t+1)) 
	&\leq H^l_{\varepsilon}(w(t+1),x(t)) + \left\langle \nabla_{x^l} H^l_{\varepsilon}(w(t+1),x(t)), x(t+1)-x(t) \right\rangle \\
	&+ \frac{L^l_{\varepsilon}(w(t+1),x(t))}{2} \|x^l(t+1)-x^l(t)\|^2 .  
\end{align*}
Summing the last inequality over $l=1,2, \ldots, k$, yields
\begin{align*}
	H_{\varepsilon}(w(t+1),x(t+1)) 
	&\leq H_{\varepsilon}(x(t)) + \sum\limits_{l=1}^{k} \frac{L^l_{\varepsilon}(w(t+1),x(t))}{2} \|x^l(t+1)-x^l(t)\|^2 \\
	&+ \sum\limits_{l=1}^{k} \left\langle \nabla_{x^l} H_{\varepsilon}(w(t+1),x(t)), x^l(t+1)-x^l(t) \right\rangle   .
\end{align*}
Replacing the last term with the following compact form
\begin{equation*}
	\sum\limits_{l=1}^{k} \left\langle \nabla_{x^l} H_{\varepsilon}(w(t+1),x(t)), x^l(t+1)-x^l(t) \right\rangle  = \left\langle \nabla_x H_{\varepsilon}(w(t+1),x(t)), x(t+1)-x(t) \right\rangle ,
\end{equation*}
and the result follows.
\end{proof}

Now we are finally ready to prove the two properties that needed for guaranteeing that the sequence that is generated by $\varepsilon$-KPALM converges to critical point of $\Psi_{\varepsilon}$.

\begin{proposition}[Sufficient decrease property]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by $\varepsilon$-KPALM. Then, there exists $\rho_1 > 0$ such that 
\begin{equation*}
	\rho_1 \|z(t+1) - z(t)\|^2 \leq \Psi_{\varepsilon}(z(t)) - \Psi_{\varepsilon}(z(t+1)) \quad \forall \: t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
As we already mentioned, the steps with respect to $w$ of KPALM and $\varepsilon$-KPALM  are similar and therefore following the same arguments given at the beginning of the proof of \Cref{State_KPALM_SDP} we have that
\begin{equation}
	\frac{\underline{\alpha}}{2} \|w(t+1) - w(t)\|^2 \leq H_{\varepsilon}(w(t),x(t)) - H_{\varepsilon}(w(t+1),x(t)) , \\ \label{StateEq37}
\end{equation}
where $\underline{\alpha} = \min\limits_{1 \leq i \leq m} \alpha_i $.
Applying \Cref{State_H_eps_prop} we get for all $t \in \mathbb{N}$ that
\begin{align}
	H_{\varepsilon}(w(t+1),x(t)) - H_{\varepsilon}(w(t+1),x(t+1)) 
	&\geq \sum\limits_{l=1}^{k} \frac{L^l_{\varepsilon}(w(t+1),x(t))}{2} \|x^l(t+1)-x^l(t)\|^2 \\
	&\geq \frac{\underline{\beta}}{\left( {d_{\mathcal{A}}}^2 + {\varepsilon}^2 \right)^{1/2}} \sum\limits _{l=1}^{k} \|x^l(t+1)-x^l(t)\|^2 \\
	&\geq \frac{\underline{\beta}}{\left( {d_{\mathcal{A}}}^2 + {\varepsilon}^2 \right)^{1/2}} \|x(t+1)-x(t)\|^2 , \label{StateEq39}
\end{align}
where the second inequality follows from \Cref{State_L_bounds}(\ref{State_L_bounds1}).
%Applying \Cref{StateEq64} with respect to $H_{\varepsilon}^{w_l(t+1)}(\cdot)$ yields
%\begin{equation*}
%	H_{\varepsilon}^{w_l(t+1)}(x^l(t+1)) - H_{\varepsilon}^{w_l(t+1)}(x^l) 
%	\leq \frac{L_{\varepsilon}^{w_l(t+1)}(x^l(t))}{2} \left( \|x^l(t) - x^l\|^2 - \|x^l(t+1) - x^l\|^2 \right), \quad \forall \: x^l \in \mathbb{R}^n,
%\end{equation*}
%for all $l=1,2, \ldots, k$.
%Setting $x^l = x^l(t)$ and rearranging yields
%\begin{equation}
%	\frac{L_{\varepsilon}^{w_l(t+1)}(x^l(t))}{2} \|x^l(t+1) - x^l(t)\|^2 \leq H_{\varepsilon}^{w_l(t+1)}(x^l(t)) - H_{\varepsilon}^{w_l(t+1)}(x^l(t+1)), \quad \forall \: 1 \leq l \leq k . \label{StateEq38}
%\end{equation}
%
%Denote ${\underline{L}(t)}= \min\limits_{1 \leq l \leq k} \left\lbrace L_{\varepsilon}^{w_l(t+1)}(x^l(t)) \right\rbrace$. Summing (\ref{StateEq38}) over $l = 1,2, \ldots ,k$ leads to
%\begin{equation}
%	\begin{split}
%	\frac{\underline{L}(t)}{2} \|x(t+1) - x(t)\|^2 &= 
%	\frac{\underline{L}(t)}{2} \sum\limits_{l=1}^{k} \|x^l(t+1) - x^l(t)\|^2 \\
%	& \leq \sum\limits_{l=1}^{k} \frac{L_{\varepsilon}^{w_l(t+1)}(x^l(t))}{2} \|x^l(t+1) - x^l(t)\|^2 \\
%	& \leq \sum\limits_{l=1}^{k} \left( H_{\varepsilon}^{w_l(t+1)}(x^l(t)) - H_{\varepsilon}^{w_l(t+1)}(x^l(t+1)) \right) \\
%	& = H_{\varepsilon}(w(t+1),x(t)) - H_{\varepsilon}(w(t+1),x(t+1)) .
%	\end{split} \label{StateEq39}
%\end{equation}
%
%Set $\rho_1 = \frac{1}{2} \min_{t \in \mathbb{N}} \left\lbrace \underline{\alpha}(t), \underline{L}(t) \right\rbrace$, and note that since $x^l(t) \in Conv(\mathcal{A})$ for all $1 \leq l \leq k$, then
%\begin{equation*}
%	L_{\varepsilon}^{w_l(t+1)}(x^l(t)) = \sum\limits_{i=1}^m \frac{w^i_l(t+1)}{\left( \|x^l(t) - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} \geq \frac{\sum\limits_{i=1}^m w^i_l(t+1)}{\left( {d_{\mathcal{A}}}^2 + {\varepsilon}^2 \right)^{1/2}} ,
%\end{equation*}
%where $d_{\mathcal{A}} = diam(Conv(\mathcal{A}))$, hence together with  \Cref{StateMainAssum} assures that $\rho_1 > 0$. 
Set $\rho_1 = \frac{1}{2} \min \left\lbrace \underline{\alpha}, \underline{\beta}/ \left( d_{\mathcal{A}}^2 + {\varepsilon}^2 \right)^{1/2}  \right\rbrace$. Summing (\ref{StateEq37}) and (\ref{StateEq39}) yields
\begin{align*}
	\rho_1 \|z(t+1) &- z(t)\|^2 
	 = \rho_1 \left( \|w(t+1) - w(t)\|^2 + \|x(t+1) - x(t)\|^2  \right) \leq \\
	&\leq \left[ H_{\varepsilon}(w(t),x(t)) - H_{\varepsilon}(w(t+1),x(t)) \right] + \left[ H_{\varepsilon}(w(t+1),x(t)) - H_{\varepsilon}(w(t+1),x(t+1)) \right] \\
	&= H_{\varepsilon}(z(t)) - H_{\varepsilon}(z(t+1)) \\
	&= \Psi_{\varepsilon}(z(t)) - \Psi_{\varepsilon}(z(t+1)),
\end{align*}
where the last equality follows from the fact that $G(w(t)) = 0$, for all $t \in \mathtt{N}$. This proves the desired result.
\end{proof}

The next lemma will be useful in proving the subgradient lower bounds for the iterates gap property of the sequence generated by $\varepsilon$-KPALM.

\begin{lemma} \label{StateEq40}
For any $x,y \in \mathbb{R}^{nk}$ such that $x^l,y^l \in Conv(\mathcal{A})$ for all $1 \leq l \leq k$ the following inequality holds 
\begin{equation*}
	\|d_{\varepsilon}^i(x) - d_{\varepsilon}^i(y)\| \leq \frac{ d_{\mathcal{A}}}{\varepsilon}\|x-y\|, \quad \forall \: i=1, 2, \ldots ,m ,
\end{equation*}
with $d_{\mathcal{A}} = diam(Conv(\mathcal{A}))$.
\end{lemma}

\begin{proof}
Define $\psi(t)=\sqrt{t + {\varepsilon}^2}$, for $t \geq 0$. Using the Lagrange mean value theorem over $a > b \geq 0$ yields
\begin{equation*}
	\frac{\psi(a) - \psi(b)}{a - b} = \psi'(c) = \frac{1}{2\sqrt{c + {\varepsilon}^2}} \leq \frac{1}{2\varepsilon},
\end{equation*}
where $c \in (b,a)$.
Therefore, for all $i=1,2, \ldots, m$ and $l=1,2, \ldots, k$ we have
\begin{align*}
	\abs{\left( \|x^l-a^i\|^2  + {\varepsilon}^2 \right)^{1/2} - \left( \|y^l-a^i\|^2 + {\varepsilon}^2 \right)^{1/2} } 
	&\leq \frac{1}{2\varepsilon} \abs{ \|x^l-a^i\|^2 + {\varepsilon}^2 - \left( \|y^l-a^i\|^2 + {\varepsilon}^2 \right) } \\
	&= \frac{1}{2\varepsilon} \abs{\|x^l-a^i\|^2 - \|y^l-a^i\|^2} \\
	&= \frac{1}{2\varepsilon} \abs{\|x^l-a^i\| + \|y^l-a^i\|} \cdot \abs{\|x^l-a^i\| - \|y^l-a^i\|} \\
	&\leq \frac{1}{\varepsilon} \: d_{\mathcal{A}}\|x^l-y^l\| .
\end{align*}
Hence,
\begin{align*}
	\|d_{\varepsilon}^i(x) - d_{\varepsilon}^i(y)\| 
	&= \left[ \sum_{l=1}^{k} \abs{\left( \|x-a^i\|^2  + {\varepsilon}^2 \right)^{1/2} - \left( \|y-a^i\|^2 + {\varepsilon}^2 \right)^{1/2} }^2 \right]^\frac{1}{2} \\
	&\leq \left[ \sum_{l=1}^{k} \left( \frac{1}{\varepsilon} \: d_{\mathcal{A}}\|x^l-y^l\| \right)^2 \right]^\frac{1}{2} \\
	&= \frac{ d_{\mathcal{A}}}{\varepsilon}\|x-y\| ,
\end{align*}
as asserted.
\end{proof}

%\begin{lemma}[Upper bound of the sequence $\left\lbrace \overline{L}(x(t)) \right\rbrace_{t \in \mathbb{N}}$] \label{StateEq41}
%Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}} = \left\lbrace (w(t) , x(t)) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by $\varepsilon$-KPALM, then for any $t \in \mathbb{N}$ we have
%\begin{equation*}
%	\overline{L}(x(t)) = \max\limits_{1 \leq l \leq k} \left\lbrace L_{\varepsilon}^{w_l(t+1)}(x^l(t)) + \frac{2 L_{\varepsilon}^{w_l(t+1)}(x^l(t)) L_{\varepsilon}^{w_l(t+1)}(x^l(t+1))}{L_{\varepsilon}^{w_l(t+1)}(x^l(t)) + L_{\varepsilon}^{w_l(t+1)}(x^l(t+1))} \right\rbrace	\leq \frac{2m}{\varepsilon} .
%\end{equation*}
%\end{lemma}
%
%\begin{proof}
%For any $w_l \in \left[ 0,1 \right]^m$ and $x^l \in \mathbb{R}^n$ we have
%\begin{equation*}
%	L_{\varepsilon}^{w_l}(x^l) = \sum\limits_{i=1}^m \frac{w^i_l}{\left( \|x^l - a^i\|^2 + {\varepsilon}^2 \right)^{1/2}} \leq \sum\limits_{i=1}^m \frac{1}{\varepsilon} = \frac{m}{\varepsilon}.
%\end{equation*}
%Therefore,
%\begin{equation*}
%	\overline{L}(x(t)) = \max\limits_{1 \leq l \leq k} \left\lbrace L_{\varepsilon}^{w_l(t+1)}(x^l(t)) + \frac{2}{\frac{1}{L_{\varepsilon}^{w_l(t+1)}(x^l(t))} + \frac{1}{L_{\varepsilon}^{w_l(t+1)}(x^l(t+1))}} \right\rbrace \leq \frac{m}{\varepsilon} + \frac{2}{\frac{2\varepsilon}{m}} = \frac{2m}{\varepsilon} ,
%\end{equation*}
%this proves the desired result.
%\end{proof}

\begin{proposition}[Subgradient lower bound for the iterates gap]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by $\varepsilon$-KPALM. Then, there exists $\rho_2 > 0$ and $\gamma(t+1) \in \partial \Psi_{\varepsilon}(z(t+1))$ such that 
\begin{equation*}
	\| \gamma(t+1)\| \leq \rho_2 \|z(t+1) - z(t)\|, \quad \forall \: t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
Repeating the steps of the proof in the case of KPALM yields that 
\begin{equation}
	\gamma(t+1) := \left( \left( d_{\varepsilon}^i(x(t+1)) + u^i(t+1) \right)_{i=1, \ldots ,m}, \nabla_x H_{\varepsilon}(w(t+1),x(t+1)) \right) \in \partial \Psi_{\varepsilon}(z(t+1)) , \label{StateEq45}
\end{equation}
where for all $1 \leq i \leq m, \: u^i(t+1) \in \partial \delta_{\Delta}(w^i(t+1))$ such that
\begin{equation}
	d_{\varepsilon}^i(x(t)) + \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) + u^i(t+1) = \mathbf{0} . \label{StateEq46}
\end{equation}
Plugging (\ref{StateEq46}) into (\ref{StateEq45}), and taking norm yields
\begin{align*}
	\| \gamma(t+1) \|
	&\leq \sum\limits_{i=1}^{m} \| d_{\varepsilon}^i(x(t+1)) - d_{\varepsilon}^i(x(t)) - \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) \| \\
	&+ \| \nabla_x H_{\varepsilon}(w(t+1),x(t+1)) \| \\
	&\leq \sum\limits_{i=1}^{m} \| d_{\varepsilon}^i(x(t+1)) - d_{\varepsilon}^i(x(t)) \| + \sum\limits_{i=1}^{m} \alpha_i(t) \| w^i(t+1) - w^i(t) \| \\
	&+ \| \nabla_x H_{\varepsilon}(w(t+1),x(t+1)) \| \\
	&\leq \frac{\sqrt{m} d_{\mathcal{A}}}{\varepsilon} \|x(t+1) - x(t)\| + \sqrt{m} \overline{\alpha} \|w(t+1) - w(t)\| + \| \nabla_x H_{\varepsilon}(w(t+1),x(t+1)) \|,
\end{align*}
where the last inequality follows from \Cref{StateEq40} and the fact that $\overline{\alpha} = \max\limits_{1 \leq i \leq m} \overline{\alpha_i}$. \\ 
Next we bound $\| \nabla_x H_{\varepsilon}(w(t+1),x(t+1)) \| \leq c\|x(t+1) - x(t)\|$, for some constant $c>0$. Indeed, for all $l=1,2, \ldots, k$, we have
\begin{align}
	\nabla_{x^l}H_{\varepsilon}(w(t+1),x(t+1)) 
	&= \nabla_{x^l}H_{\varepsilon}(w(t+1),x(t+1)) - \nabla_{x^l}H_{\varepsilon}(w(t+1),x(t)) \\
	&+ \nabla_{x^l}H_{\varepsilon}(w(t+1),x(t))\\
	&= \nabla_{x^l}H_{\varepsilon}(w(t+1),x(t+1)) - \nabla_{x^l}H_{\varepsilon}(w(t+1),x(t)) \\
	&+ L^l_{\varepsilon}(w(t+1),x(t)) \left( x^l(t) - x^l(t+1) \right) ,
\end{align}
where the last equality follows from (\ref{StateEq33}). Therefore,
\begin{align}
	\| \nabla_x H_{\varepsilon}(w(t+1),x(t+1)) \| 
	&\leq \sum\limits_{l=1}^{k} \| \nabla_{x^l} H_{\varepsilon}(w(t+1),x(t+1)) \| \\
	&\leq \sum\limits_{l=1}^{k} L^l_{\varepsilon}(w(t+1),x(t)) \|x^l(t+1) - x^l(t)\| \\
	&+ \sum\limits_{l=1}^{k} \| \nabla_{x^l} H_{\varepsilon}(w(t+1),x(t+1)) - \nabla_{x^l} H_{\varepsilon}(w(t+1),x(t))\| \\
	&\leq \frac{m}{\varepsilon} \sum\limits_{l=1}^{k} \|x^l(t+1)-x^l(t)\| + \sum\limits_{l=1}^{k} \gamma^l(t) \|x^l(t+1)-x^l(t)\| , \label{StateEq47}
\end{align}
where the last inequality follows from \Cref{State_L_bounds}(\ref{State_L_bounds2}) and \Cref{StateEq65} where
\begin{equation*}
	\gamma^l(t) = \frac{2 L^l_{\varepsilon}(w(t+1),x(t)) L^l_{\varepsilon}(w(t+1),x(t+1))}{L^l_{\varepsilon}(w(t+1),x(t)) + L^l_{\varepsilon}(w(t+1),x(t+1))} , \quad l=1,2, \ldots, k.
\end{equation*}
From \Cref{State_L_bounds}(\ref{State_L_bounds2}) we obtain that
\begin{equation*}
	\gamma^l(t) = \frac{2}{\frac{1}{L^l_{\varepsilon}(w(t+1),x(t))} + \frac{1}{L^l_{\varepsilon}(w(t+1),x(t+1))}} \leq \frac{2}{\frac{\varepsilon}{m} + \frac{\varepsilon}{m}} = \frac{m}{\varepsilon} .
\end{equation*}
Hence, from (\ref{StateEq47}), we have 
\begin{equation*}
	\| \nabla_x H_{\varepsilon}(w(t+1),x(t+1))\| \leq \frac{2m}{\varepsilon} \sum\limits_{l=1}^{k} \|x^l(t+1)-x^l(t)\| \leq \frac{2m\sqrt{k}}{\varepsilon} \|x(t+1) - x(t)\|.
\end{equation*}
%From (\ref{StateEq33}) we have
%\begin{equation*}
%\nabla H_{\varepsilon}^{w_l(t+1)}(x^l(t)) = L_{\varepsilon}^{w_l(t+1)}(x^l(t)) \left( x^l(t+1) - x^l(t) \right) , \quad \forall \: 1 \leq l \leq k,
%\end{equation*}
%applying \Cref{StateEq65} with respect to $H_{\varepsilon}^{w_l(t+1)}(\cdot)$ and plugging into (\ref{StateEq47}) yields
%\begin{equation*}
%\begin{aligned}
%	&\| \nabla_x H(w(t+1),x(t+1)) \| \leq \\
%	&\leq \sum\limits_{l=1}^{k} \left( L_{\varepsilon}^{w_l(t+1)}(x^l(t)) + \frac{2 L_{\varepsilon}^{w_l(t+1)}(x^l(t)) L_{\varepsilon}^{w_l(t+1)}(x^l(t+1))}{L_{\varepsilon}^{w_l(t+1)}(x^l(t)) + L_{\varepsilon}^{w_l(t+1)}(x^l(t+1))} \right) \|x^l(t+1) - x^l(t)\| .
%\end{aligned}
%\end{equation*}
Therefore, setting $\rho_2 = \sqrt{m} \left( \frac{d_{\mathcal{A}}}{\varepsilon} + \overline{\alpha} \right) + \frac{2m\sqrt{k}}{\varepsilon}$, yields and the result.
\end{proof}

The following lemma shows that the smoothed function $H_{\varepsilon}(w,x)$ indeed approximates $H(w,x)$.

\begin{lemma}[Closeness of smooth]
For any $(w,x) \in {\Delta}^m \times \mathbb{R}^{nk}$ and $\varepsilon > 0$ the following inequalities hold true
\begin{equation*}
	H(w,x) \leq H_{\varepsilon}(w,x) \leq H(w,x) + m\varepsilon .
\end{equation*}
\end{lemma}

\begin{proof}
Applying the inequality
\begin{equation*}
	\left( a+b \right)^{\lambda} \leq a^{\lambda} + b^{\lambda}, \quad \forall \: a,b \geq 0, \: \lambda \in \left( 0,1 \right] ,
\end{equation*}
with $a = \|x^l - a^i \|^2$ , $b = {\varepsilon}^2$ and $\lambda = \frac{1}{2}$, yields
\begin{equation*}
	\left(\|x^l - a^i \|^2 + {\varepsilon}^2 \right)^{1/2} \leq \|x^l - a^i \| + \varepsilon , \quad \forall \: 1 \leq l \leq k, \: 1 \leq i \leq m .
\end{equation*}
Together with the fact that
\begin{equation*}
	\|x^l - a^i \| \leq \left(\|x^l - a^i \|^2 + {\varepsilon}^2 \right)^{1/2},
\end{equation*}
yields the following inequality
\begin{equation*}
	\|x^l - a^i \| \leq \left(\|x^l - a^i \|^2 + {\varepsilon}^2 \right)^{1/2} \leq \|x^l - a^i \| + \varepsilon ,
\end{equation*}
for all $l=1,2, \ldots, k$ and $i=1,2, \ldots, m$.
Multiplying each inequality by $w^i_l$ and summing over $l=1,2, \ldots, k$ and $i=1,2, \ldots, m$ we obtain
\begin{equation*}
	H(w,x) \leq H_{\varepsilon}(w,x) \leq H(w,x) + \sum\limits_{i=1}^m \sum\limits_{l=1}^k w^i_l \varepsilon .
\end{equation*}
Since for all $i=1,2, \dots, m, \: w^i \in \Delta$, the result follows.
\end{proof}

\newpage


\section{Returning to KMEANS}
\subsection{Similarity to KMEANS}
The famous KMEANS algorithm has close relation to KPALM algorithm. KMEANS alternates between cluster assignment and centers update steps as well. In detail, we can write its steps in the following manner

\begin{framed}
\noindent \textbf{KMEANS}
\begin{enumerate}[(1)]
	\item Initialization: $x(0) \in \mathbb{R}^{nk}$.
	\item General step $\left( t=0,1, \ldots \right)$:
	\begin{enumerate}[(2.1)]
		\item Cluster assignment: for $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			w^i(t+1) = \arg\!\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(x(t)) \rangle\right\rbrace . \label{StateEq12}
		\end{equation}
		\item Centers update: for $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			x^l(t+1) = \frac{\sum_{i=1}^{m} w^i_l(t+1) a^i}{\sum_{i=1}^{m} w^i_l(t+1)} . \label{StateEq13}
		\end{equation}
%		\item Stopping criteria: halt if 
%		\begin{equation}
%			\forall 1 \leq l \leq k \quad C^l(t+1)=C^l(t) \label{StateEq15}
%		\end{equation}
	\end{enumerate}
\end{enumerate}
\end{framed}

It is easy to see that if we take $\alpha_i(t) = 0$ for all $1 \leq i \leq m$ and $t \in \mathbb{N}$, then KPALM becomes KMEANS. We aim to employ PALM theory once more and show that the sequence generated by KMEANS converges to a critical point of $\Psi(z)$, as defined is (\ref{StateEq4}). The sufficient decrease proof of \Cref{State_Clustering_SqNorm} breaks down in this case, since it is based on \Cref{StateMainAssum}(\ref{StateMainAssum1}), that is, $\alpha_i(t) > \underline{\alpha_i} > 0$, for all $t \in \mathbb{N}$ and $i=1,2, \ldots, m$. However, the proof of the subgradient lower bound for iterates gap property follows through as is. It the following discussion we present the means to treat the case that $\alpha_i(t) = 0$, and prove the sufficient decrease property.

\begin{lemma} \label{StateLemma_x_bounds_w}
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KMEANS. Then, there exists $c > 0$ such that
\begin{equation*}
	\|w^i(t+1)-w^i(t)\| \leq c \|x(t+1)-x(t)\| , \quad \forall \: i=1,2, \ldots\ m, \: t \in \mathbb{N} .
\end{equation*}
\end{lemma}

\begin{proof}
At each iteration KMEANS partitions the set $\mathcal{A}$ into $k$ clusters, and the center of each cluster is its mean. Since the number of these partitions if finite, then there exists a finite set $\mathcal{C} = \left\lbrace x^1,x^2, \ldots, x^N \right\rbrace \subset \mathbb{R}^{nk}$ such that for all $t \in \mathbb{N}$, $x(t) \in \mathcal{C}$. We denote
\begin{equation*}
	r = \min\limits_{1 \leq j < l \leq N} \|x^j-x^l\|,
\end{equation*}
and set $c = \sqrt{2}/r$.
At each iteration, the point $a^i$ can move from one cluster to another, hence
\begin{equation*}
	\|w^i(t+1)-w^i(t)\| \leq \sqrt{2} .
\end{equation*}
Therefore, combining these arguments yields
\begin{equation*}
	\frac{\|w^i(t+1-w^i(t)\|}{\|x(t+1)-x(t)\|} \leq \frac{\sqrt{2}}{r} .
\end{equation*}
In case that $x(t+1)=x(t)$, this implies that none of the clusters has changed, hence we proved the statement in both cases.
\end{proof}

Equipped with the last lemma we briefly prove the sufficient decrease property of KMEANS.

\begin{proposition}[Sufficient decrease property for KMEANS sequence]
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KMEANS. Then, there exists $\rho_1 > 0$ such that 
\begin{equation*}
	\rho_1 \|z(t+1) - z(t)\|^2 \leq \Psi_{\varepsilon}(z(t)) - \Psi_{\varepsilon}(z(t+1)) \quad \forall \: t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
The function $x \mapsto H(w(t),x)$ remains strongly convex with parameter $\beta(w(t))$ (see (\ref{StateEq17})), hence we have sufficient decrease in $x$ variable, namely,
\begin{equation}
	\frac{\underline{\beta}}{2} \|x(t+1)-x(t)\|^2 \leq H(w(t),x(t)) - H(w(t+1),x(t+1)) . \label{StateEq70}
\end{equation}
Setting $\rho_1 = \underline{\beta} /2(1 + mc^2)$, we can write
\begin{align*}
	\rho_1 \|z(t+1)-z(t)\|^2 &= \rho_1 \sum\limits_{i=1}^{m} \|w^i(t+1)-w^i(t)\|^2 + \rho_1 \|x(t+1)-x(t)\|^2 \\
	&\leq \rho_1 (1 + mc^2) \|x(t+1) - x(t)\|^2 \\
	&\leq H(w(t),x(t)) - H(w(t+1),x(t+1)) \\
	&= \Psi(z(t)) - \Psi(z(t+1))
\end{align*}
where the first inequality follows from \Cref{StateLemma_x_bounds_w}, the second follows from (\ref{StateEq70}), and the last equality follows from the fact that $G(w(t))=0$, for all $t \in \mathbb{N}$.
\end{proof}

\newpage

\subsection{KMEANS Local Minima Convergence Proof}

In this section we present a simple and direct proof that KMEANS converges to local minima. We start with rewriting the KMEANS algorithm, in its most familiar form
\begin{framed}
\noindent \textbf{KMEANS}
\begin{enumerate}[(1)]
	\item Initialization: $x(0) \in \mathbb{R}^{nk}$.
	\item General step $\left( t=0,1, \ldots \right)$:
	\begin{enumerate}[(2.1)]
		\item Cluster assignment: for $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			C^l(t+1) = \left\lbrace a \in \mathcal{A} \mid \| a - x^l(t) \| \leq \|a - x^j(t) \|, \quad \forall 1 \leq l \leq k \right\rbrace . \label{StateEq20}
		\end{equation}
		\item Centers update: for $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			x^l(t+1) = mean(C^l(t+1)) := \frac{1}{\left| C^l(t+1) \right|} \sum\limits_{a \in C^l(t+1)} a . \label{StateEq21}
		\end{equation}
		\item Stopping criteria: halt if 
		\begin{equation}
			\forall 1 \leq l \leq k \quad C^l(t+1)=C^l(t) \label{StateEq22}
		\end{equation}
	\end{enumerate}
\end{enumerate}
\end{framed}

As in KPALM, KMEANS needs \Cref{StateMainAssum}(\ref{StateMainAssum2}) for step (\ref{StateEq21}) to be well defined. In order to prove the convergence of KMEANS to local minimum, we will need to following assumption.

\begin{assumption} \label{StateEq23}
Let $t \in \mathbb{N}$ be the final iteration of KMEANS run, then we assume that each $a \in \mathcal{A}$ belongs exclusively to single cluster $C^l(t)$.
\end{assumption}

For any $x \in \mathbb{R}^{nk}$ we denote the super-partition of $\mathcal{A}$ with respect to $x$ by $\overline{C^l}(x) = \left\lbrace a \in \mathcal{A} \mid \right. \\ \left. \|a - x^l\| \leq \|a - x^j\| , \quad \forall j \neq l \right\rbrace$, for all $1 \leq l \leq k$, and the sub-partition of $\mathcal{A}$ by $\underline{C^l}(x) = \left\lbrace a \in \mathcal{A} \mid \right. \\ \left. \|a - x^l\| < \|a - x^j\|, \quad \forall j \neq l \right\rbrace$.
Moreover, denote $R_{lj}(t) = \min\limits_{a \in C^l(t)} \left\lbrace \|a - x^j(t)\| - \|a - x^l(t)\| \right\rbrace$ for all $1 \leq l,j \leq k$, and $r(t) = \min\limits_{l \neq j} R_{lj}$. \\
Due to \Cref{StateEq23} we have that $\overline{C^l}(x(t)) = \underline{C^l}(x(t)) = C^l(t+1)$, for all $1 \leq l \leq k, \: t \in \mathbb{N}$, we also have that $r(t) > 0$ for all $t \in \mathbb{N}$.

\begin{proposition} \label{StateEq24}
Let $(C(t), x(t))$ be the clusters and centers KMEANS returns. Denote by $U = B\left( x^1(t),\frac{r(t)}{2}\right) \times  B\left( x^2(t),\frac{r(t)}{2}\right) \times \cdots \times B\left( x^l(t),\frac{r(t)}{2} \right)$ an open neighbourhood of $x(t)$, then for any $x \in U$ we have $C^l(t) = \underline{C^l}(x)$ for all $1 \leq l \leq k$.
\end{proposition}

\begin{proof}
Pick some $a \in C^l(t)$, then $x^l(t-1)$ is the closest center among the centers of $x(t-1)$. Since KMEANS halts at step $t$, then from (\ref{StateEq22}) we have $x(t)=x(t-1)$, thus $x^l(t)$ is the closest center to $a$ among the centers of $x(t)$. Further we have
\begin{equation}
	r(t) \leq \|x^j(t) - a\| - \|x^l(t) -a\| \quad \forall j \neq l . \label{StateEq25}
\end{equation}
Next, we show that $a \in \underline{C^l}(x)$, indeed
\begin{align*}
	\|a - x^l\| -  \|a - x^j\| &\leq \|a - x^l(t)\| + \|x^l(t) - x^l\| - \left( \|a - x^j(t)\| - \|x^j(t) - x^j\| \right) \\
	& = \|a - x^l\| - \|a - x^j(t)\| + \|x^l(t) - x^l\| + \|x^j(t) - x^j\| \\
	& < \|a - x^l\| - \|a - x^j(t)\| + r(t) \\
	& \leq -r(t) + r(t) = 0 ,
\end{align*}
where the second inequality holds since $x^l \in B\left( x^l(t), \frac{r(t)}{2} \right)$ and $x^j \in B\left( x^j(t), \frac{r(t)}{2} \right)$, and the third inequality follows from (\ref{StateEq25}), and we get that $C^l(t) \subseteq \underline{C^l}(x)$. 
By definition of $\underline{C^l}(x)$ we have that for any $l \neq j, \: \underline{C^l}(x) \cap \underline{C^j}(x)=\emptyset$, and for all $1 \leq l \leq k, \: \underline{C^l}(x) \subseteq \mathcal{A}$. Now, since $C(t)$ is a partition of $\mathcal{A}$, then $C^l(t) = \underline{C^l}(x)$ for all $1 \leq l \leq k$.
\end{proof}

\begin{proposition}[KMEANS converges to local minimum]
Let $(C(t), x(t))$ be the clusters and centers KMEANS returns, then $x(t)$ is local minimum of $F$ in $U = B\left( x^1(t),\frac{r(t)}{2}\right) \times  B\left( x^2(t),\frac{r(t)}{2}\right) \\ \times \cdots \times B\left( x^l(t),\frac{r(t)}{2} \right) \subset \mathbb{R}^{nk}$.
\end{proposition}

\begin{proof}
The minimum of $F$ in $U$ is
\begin{equation*}
\min\limits_{x \in U} F(x) = \min\limits_{x \in U} \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(x)} \|a - x^l \|^2 = \min\limits_{x \in U} \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(t)} \|a - x^l \|^2 ,
\end{equation*}
where the last equality follows from \Cref{StateEq24}.\\
The function $x \mapsto \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(t)} \|a - x^l \|^2$ is strictly convex, separable in $x^l$ for all $1 \leq l \leq k$, and reaches its minimum at $\frac{1}{\left| C^l(t) \right|} \sum\limits_{a \in C^l(t)} a = mean(C^l(t)) = x^l(t),$ and the result follows.
\end{proof}

\newpage

\section{Numeric Results}
In this section we show the numeric results and compare the algorithms presented at this paper with other algorithms that are commonly used to address the clustering problem.

\subsection{Iris Dataset}
We use the famous Iris dataset to test the performance of the KPALM algorithm. It is important to note that choosing the parameter $\alpha$ is left to the user, and as presented below, has a significant effect on the convergence rate and the goodness of clustering, namely the value of $\Psi$ function over the generated series. All plots in this section are made by averaging over $100$ trials, each trial with random starting point.

\begin{figure}[h]
    \centering
    \includegraphics{dynamic_alpha_kpalm}
    \caption{Comparison of $\Psi$ values for different alpha parameters within KPALM.}
    \label{fig:dynamic_alpha_psi_comp}
\end{figure} 

\Cref{fig:dynamic_alpha_psi_comp} shows that dynamic $\alpha$ parameter with fast decrease in its value, such as $\alpha_i(t)=\frac{diam(\mathcal{A})}{2^{t-1}}$, may achieve better clustering function values (that is lower), and increase the rate of conversion.

In \Cref{fig:algs_psi_comp} we made a comparison between KPALM with dynamic $\alpha$ parameter, $\alpha_i(t)=\frac{diam(\mathcal{A})}{2^{t-1}}$, with KMEANS amd KMEANS++. It demonstrates that KPALM can reach lower $\Psi$ values then KMEANS, and these are similar to the values achieved with KMEAN++.

\begin{figure}[t]
    \centering
    \includegraphics{psi_algs_comparison}
    \caption{Comparison of $\Psi$ values for KMEANS, KMEANS++ and KPALM.}
     \label{fig:algs_psi_comp}
\end{figure}

%\begin{figure}[t]
%    \centering
%    \scalebox{.5}{\includegraphics{relative_delta_algs_comparison}}
%    \caption{Comparison of $\frac{\|x(t+1)-x(t)\|}{\|x(t)\|}$ for KMEANS, KMEANS++ and KPALM.}
%    \label{fig:dynamic_alpha_relative_delta_comp}
%\end{figure}

\Cref{fig:iters_comp} shows the number of iteration needed to reach precision 1e-3 between consecutive $\Psi$ values.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=\textwidth]{iterations_algs_comparison}
        \caption{Number of iterations of KMEANS, KMEANS++ and KPALM with $\alpha(t)=diam(\mathcal{A})/2^{t-1}$.}
        \label{fig:iters_algs_comp}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=\textwidth]{iterations_dynamic_alpha_kpalm_comparison}
        \caption{Number of iterations of KPALM with different updates of $\alpha(t)$ parameter.}
        \label{fig:iters_dyn_alpha_kpalm_comp}
    \end{subfigure}
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=\textwidth]{iterations_dynamic_alpha_eps_kpalm_comparison}
        \caption{Number of iterations of $\varepsilon$-KPALM with different updates of $\alpha(t)$ parameter.}
        \label{fig:iters_dyn_alpha_eps_kpalm_comp}
    \end{subfigure}
    \caption{Comparison of number of iterations needed to reach 1e-3 precision of $\Psi$.}\label{fig:iters_comp}
\end{figure}

%\section{Clustering via ADMM Approach}
%
%Introducing some new variable into the problem leads to the following clustering problem notation
%\begin{equation*}
%\begin{split}
%	&\min\limits_{x \in \mathbb{R}^{nk}} \min\limits_{w \in \mathbb{R}^{km}} \left\lbrace \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l d(x^l,a^i) \mid w^i \in \Delta, i=1,2, \ldots, m \right\rbrace \\
%	&= \min\limits_{x \in \mathbb{R}^{nk}, w \in \mathbb{R}^{km}, z \in \mathbb{R}^{km}} \left\lbrace \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l z^i_l \: \bigg| 
%\begin{array}{c c c}
% w^i \in \Delta, & i=1,2, \ldots, m, & \\
% z^i_l = d(x^l,a^i), & i=1,2, \ldots, m, & l=1,2, \ldots, k
%\end{array}
%\right\rbrace .
%\end{split}
%\end{equation*}
%
%The augmented Lagrangian that is associated with this problem is 
%\begin{equation}
%	L_{\rho}(w,x,z,y) = \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l z^i_l + \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} y^i_l (z^i_l - d(x^l,a^i)) + \frac{\rho}{2} \sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} \left(z^i_l - d(x^l,a^i)\right)^2 . \label{StateEq59}
%\end{equation}
%Thus the ADMM formulas for (\ref{StateEq60}) are as follows
%\begin{equation*}
%\begin{split}
%	w(t+1) &= \arg\!\min\limits_{w \in \Delta^m} L_{\rho}(w,x(t),z(t),y(t)), \\
%	\Rightarrow \quad w^i(t+1) &= \arg\!\min\limits_{w^i \in \Delta} \sum\limits_{l=1}^{k} w^i_l z^i_l(t) = \arg\!\min\limits_{w^i \in \Delta} \left\langle w^i, z^i(t) \right\rangle, \quad 1 \leq i \leq m , \\
%	x(t+1) &= \arg\!\min\limits_{x \in \mathbb{R}^{nk}} L_{\rho}(w(t+1),x,z(t),y(t)), \\
%	\Rightarrow \quad x^l(t+1) &= \arg\!\min\limits_{x^l \in \mathbb{R}^{nk}} -\sum\limits_{i=1}^{m} y^i_l(t) d(x^l, a^i) + \frac{\rho}{2} \sum\limits_{i=1}^{m} \left(z^i_l - d(x^l,a^i)\right)^2, \quad 1 \leq l \leq k , \\
%	z(t+1) &= \arg\!\min\limits_{z \in \mathbb{R}^{km}} L_{\rho}(w(t+1),x(t+1),z,y(t)), \\
%	\Rightarrow \quad z^i(t+1) &= \arg\!\min\limits_{z^i \in \mathbb{R}^{km}} \left\langle w^i(t+1), z^i \right\rangle +  \left\langle y^i(t), z^i \right\rangle + \frac{\rho}{2} \left\lVert z^i - \left( d(x^l(t+1), a^i \right)_{l=1, \ldots , k} \right\lVert^2 \\
%	&= \left(d(x^l(t+1), a^i \right)_{l=1, \ldots , k} - \frac{1}{\rho}\left( w^i(t+1) + y^i(t) \right), \quad 1 \leq i \leq m, \\
%	y^i_l(t+1) &= y^i_l(t) + \rho (z^i_l(t+1) - d(x^l(t+1), a^i), \quad 1 \leq i \leq m, \: 1 \leq l \leq k.
%\end{split}
%\end{equation*}

%\begin{thebibliography}{99}

%\end{thebibliography}

\end{document}
