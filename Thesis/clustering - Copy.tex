\documentclass[11pt]{article} 
\usepackage[american]{babel}
\usepackage{makeidx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}
%\usepackage{MnSymbol}

\oddsidemargin= -12pt \evensidemargin= -12pt
\topmargin=-45pt
\binoppenalty=10000
\relpenalty=10000
\textwidth=6.5in\textheight=9in \baselineskip=18pt
\parskip=6pt plus 1pt
\numberwithin{equation}{section}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[proposition]
\newtheorem{remark}{Remark}

\begin{document}

%\begin{center} 
%\Large {Tel-Aviv University}
%
%\Large {The Raymond and Beverly Sackler Faculty of} \\
%\Large {Exact Sciences} \\
%\Large {The Department of Statistics and Operations} \\
%\Large {Research}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {\bf Draft }
%
%\bigskip
%\bigskip
%\bigskip
%\bigskip
%
%\Large{Thesis Submitted Towards the Degree of Master} \\
%\Large {of Science in Operations Research}
%
%\bigskip
%
%\Large {Sergey Voldman}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {Supervisors:} \\
%\Large {Prof. Marc Teboulle} \\
%\Large {Prof. Shoham Sabach}
%
%\bigskip
%\bigskip
%
%\Large{2015}
%
%\end{center}
%
%\newpage
%
%\begin{center} \Large {\bf Draft}
%\end{center}
%
%\begin{abstract} 
%
%To-do...
%
%\newpage
%
%\end{abstract}
%
%\section{Introduction}
%
%To-do...
%

\newpage

\section{The Clustering Problem}

Let $\mathcal{A}= \left\lbrace a^1, \dots ,a^m \right\rbrace$ be a given set of points in $\mathbb{R}^n$, and let $1 < k < m$ be a fixed given number of clusters. The clustering problem consists of partitioning the data $\mathcal{A}$ into $k$ subsets $\left\lbrace A^1, \dots ,A^k \right\rbrace$, called clusters. For each $l=1, \cdots ,k$, the cluster $A_l$ is represented by its center $x^l$, and we want to determine $k$ cluster centers $\left\lbrace x_1, \cdots ,x_k \right\rbrace$ such that the sum of proximity measures from each point $a^i$ to a nearest cluster center $x^l$ is minimized.

The clustering problem formulation is given by

\begin{equation}
	\min\limits_{x^1, \dots ,x^k \in \mathbb{R}^n} \sum\limits_{i=1}^{m} \min\limits_{1 \le l \le k} d(x^l,a^i) , \label{StateEq1}
\end{equation}

\noindent with $\textit{d}(\cdot ,\cdot)$ being a distance-like function.

\section{Problem Reformulation and Notations}

We introduce some notations that will be used throughout this document.

\noindent $A = (a^1, \cdots , a^m) \in \left(\mathbb{R}^n\right)^m$, where $a^i \in \mathbb{R}^n,\smallskip i=1, \cdots , m$

\noindent $W = (w^1, \cdots , w^m) \in \left(\mathbb{R}^k\right)^m$, where $w^i \in \mathbb{R}^k,\smallskip i=1, \cdots , m$

\noindent $X = (x^1, \cdots , x^k) \in \left(\mathbb{R}^n\right)^k$, where $x^l \in \mathbb{R}^n,\smallskip l=1, \cdots , k$

\noindent $d^{i}(X) = (d(x^1,a^i), \cdots , d(x^k,a^i)) \in \mathbb{R}^k,\smallskip i=1, \cdots , m$

\noindent $\Delta = \left\lbrace u \in \mathbb{R}^k \mid \sum\limits_{l=1}^{k} u_l = 1, u_l \geq 0 , l=1, \dots ,k \right\rbrace$

\noindent For some $S \subseteq \mathbb{R}^n$, $\delta_S(p) = \begin{cases} 0 &\mbox{if } p \in S \\ 
\infty &\mbox{if } p \not\in S \end{cases}$

\noindent $\langle u,v \rangle = \sum\limits_{l=1}^{k} u_l \cdot v_l$, for $u,v \in \mathbb{R}^k$

Using the functional optimization representation of minimum of $k$ values, i.e. $\min\limits_{1 \leq l \leq k} u_l = \newline \min \left\lbrace \langle u,v \rangle \mid v \in \Delta \right\rbrace$, and applying it over $\left(\ref{StateEq1}\right)$, gives a smooth reformulation of the clustering problem

\begin{equation}
	\min\limits_{X \in \left(\mathbb{R}^n\right)^k} \sum\limits_{i=1}^{m} \min\limits_{w^i \in \Delta} \langle w^i , d^i(X) \rangle \label{StateEq2}
\end{equation}

Further replacing the constrain over $w^i$ with $\delta_{\Delta}(\cdot)$ function results in a equivalent formulation

\begin{equation}
	\min\limits_{X \in \left(\mathbb{R}^n\right)^k , W \in \left(\mathbb{R}^k\right)^m} \left\lbrace \sum\limits_{i=1}^{m} \langle w^i , d^i(X) \rangle + \delta_{\Delta}(w^i) \right\rbrace \label{StateEq3}
\end{equation}

Finally, introducing several more useful definitions, for each $i=1, \cdots , m$

\begin{center}
$H_i(W,X) = \langle w^i , d^i(X) \rangle$
\\ \smallskip
$G(w^i) = \delta_{\Delta}(w^i)$
\\ \smallskip
$H(W,X) = \sum\limits_{i=1}^{m} H_i(W,X)$
\\ \smallskip
 $G(W) = \sum\limits_{i=1}^{m} G(w^i)$
\\
\end{center}

Replacing the terms in (\ref{StateEq2}) with the functions above gives a compact form that is equivalent to the original clustering problem

\begin{equation}
	\min \left\lbrace \Psi(Z) := H(W,X) + G(W) \mid Z := (W,X) \in (\mathbb{R}^k)^m \times (\mathbb{R}^n)^k \right\rbrace \label{StateEq4}
\end{equation}


\section{Clustering via PALM Approach}

\subsection{Introduction to PALM Theory}

Presentation of PALM's requirements and of the algorithm steps  $\cdots$

\subsection{Clustering with PALM for $d(u,v) = \|u-v\|^2$}

In this section we tackle the clustering problem with distance-like function $d(u,v) = \|u-v\|^2$. Using the discussion about PALM, we will construct a semi-PALM algorithm.
Since the clustering problem has a specific structure, we are ought to exploit it in the following manner.
First we notice that the map 
$W \mapsto H(W,X)=\sum\limits_{i=1}^{m} \langle w^i , d^i(X) \rangle$ is linear in $W$, so there is no need to linearize it. In addition, the map 
$X \mapsto H(W,X) = \sum\limits_{i=1}^{m} \langle w^i , d^i(X) \rangle = 
\sum\limits_{i=1}^{m} \sum\limits_{l=1}^{k} w^i_l \|x^l - a^i\|^2 =
\sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is convex in $X$, hence we can drop the proximal term in PALM algorithm.

Now we propose the semi-PALM algorithm for clustering.
\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random vectors $(W(0),X(0)) \in \Delta^m \times (\mathbb{R}^n)^k$

	\item For each $t=0,1, \cdots$ generate a sequence $\left\lbrace(W(t),X(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: Take $\nu \in \left(0,1\right]$, compute $\beta(t) = \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t)\right \rbrace$, set
		$\alpha(t)=\nu \beta(t) $ and for $i=1, \cdots ,m$ compute
		\begin{equation}
			w^i(t+1) = \arg\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(X(t)) \rangle + \frac{\alpha(t)}{2} \|w^i - w^i(t)\|^2 \right\rbrace \label{StateEq5}
		\end{equation}
		
		\item Centers Update: For $l=1, \cdots ,k$ compute $x^l \in \mathbb{R}^n$ via
		\begin{equation}
			X(t+1) = \arg\min \left\lbrace H(W(t+1), X) \mid X \in (\mathbb{R}^n)^k \right\rbrace \label{StateEq6}
		\end{equation}
	\end{enumerate}
\end{enumerate}

\newpage

At each step $t$, the PALM-Clustering algorithm alternates between cluster assignment and centers update. The explicit formulas for step $t$ are given below

\begin{equation}
w^i(t+1) = \Pi_{\Delta} \left(w^i(t) - \frac{d^i(X(t))}{\alpha(t)}\right) \quad i=1, \cdots ,m \label{StateEq7}
\end{equation}

\begin{equation}
x^l(t+1) = \frac{\sum_{i=1}^{m} w^i_l(t+1) a^i}{\sum_{i=1}^{m} w^i_l(t+1)} \quad l=1, \cdots ,k \label{StateEq8}
\end{equation}

\begin{remark} 
	\begin{enumerate}[(i)]
		\item $\alpha(t)$ is the step-size, and it must be positive. If for some step $t \in \mathbb{N}$ $\alpha(t)=0$ then $\exists l' \in \left[ 1,k \right]$ such that $\beta(t) = \sum\limits_{i=1}^{m} w^i_{l'} = 0$, since $\forall l \in \left[ 1,k \right] ,\forall i \in \left[ 1,m \right] : w^i_l \geq 0$ then $\forall i \in \left[ 1,m \right] w^i_{l'}=0$. Thus, none of the points in $\mathcal{A}$ belong to cluster $l'$, in that case the algorithm can halt. Hence from now on we assume that $\forall t \in \mathbb{N}$, $\beta(t) = \min\limits_{1 \leq l \leq k} \left\lbrace \sum\limits_{i=1}^{m} w^i_l(t)\right \rbrace > 0$, and it follows that $\alpha(t) > 0 $.
		\item $\forall t \in \mathbb{N} \quad W(t) \in \Delta^m \Rightarrow \Psi(Z(t)) = H(W(t),X(t)) + G(W(t)) = H(W(t),X(t))$.
	\end{enumerate}
\end{remark}

\begin{lemma}[Strong convexity of $H(W,X)$ in $X$] 
At step $t$, the mapping $X \mapsto H(W(t),X)$ is strongly convex $\Leftrightarrow \beta(t) > 0$.
\end{lemma}

\begin{proof}
Since the mapping $X \mapsto H(W(t),X) = 
\sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is $C^2$, it is strongly convex $\Leftrightarrow$  its Hessian matrix smallest eigenvalue is positive.

\begin{center}
$\nabla_{x^j} \nabla_{x^l} H(W(t),X) = 
\begin{cases} 0 &\mbox{if } j \neq l, \quad j,l \in \left[ 1,k \right] 
\\ 2\sum\limits_{i=1}^{m} w^i_l(t) &\mbox{if } j = l, \quad j,l \in \left[ 1,k \right] \end{cases}$
\end{center}

Since the Hessian is diagonal, the smallest eigenvalue is $\min\limits_{1 \leq l \leq k} 2\sum\limits_{i=1}^{m} w^i_l(t) = 
\min\limits_{1 \leq l \leq k} 2\beta(t)$, and the result follows.
\end{proof}

Now we are ready to prove the decrease property of the PALM-Clustering algorithm.

\begin{proposition}[Sufficient decrease property]\ \\
$\exists \rho_1 > 0$ such that $\rho_1 \|Z(t+1) - Z(t)\|^2 \leq \Psi(Z(t)) - \Psi(Z(t+1)), \quad \forall t \in \mathbb{N} $.
\end{proposition}

\begin{proof}
From (\ref{StateEq5}) we derive the following inequality
\begin{equation*}
	\begin{split}
	H_i(W(t+1),X(t)) + \frac{\alpha(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	= \langle w^i(t+1) , d^i(X(t)) \rangle + \frac{\alpha(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	\leq \langle w^i(t) , d^i(X(t)) \rangle + \frac{\alpha(t)}{2} \|w^i(t) - w^i(t)\|^2 \\
	= \langle w^i(t) , d^i(X(t)) \rangle \\
	= H_i(W(t),X(t))
	\end{split}
\end{equation*}

Hence, we obtain
\begin{equation*}
	\frac{\alpha(t)}{2} \|w^i(t+1) - w^i(t)\|^2 
	\leq H_i(W(t),X(t)) - H_i(W(t+1),X(t))
\end{equation*}
Summing this inequality over $i=1, \cdots ,m$ gives
\begin{equation*}
	\begin{split}
	\frac{\alpha(t)}{2} \|W(t+1) - W(t)\|^2 
	= \frac{\alpha(t)}{2} \sum\limits_{i=1}^{m} \|w^i(t+1) - w^i(t)\|^2 \\
	\leq \sum\limits_{i=1}^{m} H_i(W(t),X(t)) - \sum\limits_{i=1}^{m} H_i(W(t+1),X(t)) \\
	= H(W(t),X(t)) - H(W(t+1),X(t)) \\
	\end{split}
\end{equation*}

Recall that the mapping $X \mapsto H(W(t),X)$ is strongly convex with parameter $2 \beta(t) > 0$, hence we have

\begin{equation*}
	\begin{split}
	H(W(t+1),X(t)) - H(W(t+1),X(t+1)) \\
	\geq \nabla_X H(W(t+1),X(t+1))^{T}(X(t)-X(t+1)) + \frac{2\beta(t)}{2} \|X(t) - X(t+1)\|^2 \\
	= \beta(t) \|X(t) - X(t+1)\|^2 \\
	\end{split}
\end{equation*}
where the last equality follows from (\ref{StateEq6}). \\
Set $\rho_1 = \min\left\lbrace \alpha(t) , \beta(t) \right\rbrace = \alpha(t)$, combined with the previous inequalities, we have

\begin{equation*}
	\begin{split}
	\rho_1 \|Z(t+1) - Z(t)\|^2 
	= \rho_1 \left( \|W(t+1) - W(t)\|^2 + \|X(t+1) - X(t)\|^2 \right) \\
	\leq \left[ H(W(t),X(t)) - H(W(t+1),X(t)) \right] + \left[ H(W(t+1),X(t)) - H(W(t+1),X(t+1)) \right] \\
	= H(Z(t)) - H(Z(t+1)) = \Psi(Z(t)) - \Psi(Z(t+1)).
	\end{split}
\end{equation*}
\end{proof}

Next, we aim to prove the subgradient lower bound for iterates property. We start with few preliminary results.

\begin{proposition}[Sufficient Lipschitz continuity condition]
Let $f:\mathbb{R}^n \to \mathbb{R}$ be $C^1$ function, if $\|\nabla f(z)\|$ is bounded on $z \in \Omega$, then $\exists M > 0$ such that $\forall x,y \in \Omega$ $\|f(x) - f(y)\| \leq M \|x - y\|$, where $M = \sup\limits_{z \in \Omega} \| \nabla f(z)\|$.
\end{proposition}

\begin{lemma}
$\left\lbrace \| \nabla_X d^i(X(t+1))\| \right\rbrace_{t \in \mathbb{N}}$ is bounded set.
\end{lemma}

\begin{proof}
\begin{equation*}
	\begin{split}
	\| \nabla_X d^i(X(t+1))\| = 2\| \left( x^1(t+1)-a^i, \cdots , x^k(t+1)-a^i \right)\| \\
	\leq 2\sum\limits_{l=1}^{k} \|x^l(t+1)-a^i\|
	\leq 2\sum\limits_{l=1}^{k} \| \frac{\sum\limits_{j=1}^{m} w^j_l(t+1) a^j}{\sum\limits_{j=1}^{m} w^j_l(t+1)} - a^i \| \\
	\leq 2\left(\left( \sum\limits_{l=1}^{k} \sum\limits_{j=1}^{m} \frac{w^j_l(t+1)}{\sum\limits_{j=1}^{m} w^j_l(t+1)} \|a^j\| \right) + k\|a^i\| \right) 
	\leq 2k \left(\|a^i\| + \sum\limits_{j=1}^{m} \|a^j\| \right)
	\end{split}
\end{equation*}
\end{proof}

From the last two results it follows that
\begin{equation}
\exists M > 0 \quad \| d^i(X(t+1) - d^i(X(t))\| \leq M \| X(t+1) - X(t) \| , \quad \forall t \in \mathbb{N}. \label{StateEq11}
\end{equation}

\begin{proposition}[Subgradient lower bound for iterates property]\ \\
$\exists \rho_2 > 0$ and $\gamma(t+1) \in \partial \Psi(Z(t+1))$ such that $\| \gamma(t+1)\| \leq \rho_2 \|Z(t+1) - Z(t)\|^2 , \quad \forall t \in \mathbb{N} $.
\end{proposition}

\begin{proof}
$\Psi = H + G$, then
\begin{equation*}
	\begin{split}
	\partial \Psi = \nabla H + \partial G  
	= \left( \nabla_{W}H, \nabla_{X}H \right) + \left( \left( \partial_{w^i}\delta_{\Delta} \right)_{i=1, \cdots ,m}, \left( \vec{0} \right)_{l=1, \cdots ,k} \right) \\
	= \left( \left( \nabla_{w^i} H_i + \partial_{w^i} \delta_{\Delta} \right)_{i=1, \cdots ,m} , \nabla_X H \right)
	\end{split}
\end{equation*}
Evaluating the last relation at $Z(t+1)$ yields
\begin{equation*}
	\begin{split}
	\partial \Psi(Z(t+1)) 
	= \left( \left( \nabla_{w^i} H_i(W(t+1),X(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \cdots ,m} , \nabla_X H(W(t+1),X(t+1)) \right) \\
	= \left( \left( d^i(X(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \cdots ,m} , \nabla_X H(W(t+1),X(t+1)) \right) \\
	= \left( \left( d^i(X(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1, \cdots ,m} , \vec{0} \right) \\
	\end{split}
\end{equation*}
where the last equality follows from (\ref{StateEq6}), that is the optimality condition of $X(t+1)$. \\
Taking the norm of the last equation yields
\begin{equation}
	\| \partial \Psi(Z(t+1))\| 
	\leq \sum\limits_{i=1}^{m} \| d^i(X(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \|. \label{StateEq9}
\end{equation}
The optimality condition of $w^i(t+1)$ that is derived from (\ref{StateEq5}), yields $\forall i=1, \cdots ,m$ that there $\exists u^i(t+1) \in \partial \delta_{\Delta}(w^i(t+1))$ such that
\begin{equation}
	d^i(X(t)) + \alpha(t) \left( w^i(t+1) - w^i(t) \right) + u^i(t+1) = 0 \label{StateEq10}
\end{equation}
Setting $\gamma(t+1) = \left( \left( d^i(X(t+1)) + u^i(t+1) \right)_{i=1, \cdots ,m}, \vec{0} \right) \in \partial \Psi(Z(t+1))$, and plugging (\ref{StateEq10}) into (\ref{StateEq9}) we have
\begin{equation*}
	\begin{split}
	\| \gamma(t+1) \|
	\leq \sum\limits_{i=1}^{m} \| d^i(X(t+1)) - d^i(X(t)) - \alpha(t) \left( w^i(t+1) - w^i(t) \right) \| \\
	\leq \sum\limits_{i=1}^{m} \| d^i(X(t+1)) - d^i(X(t)) \| + m \alpha(t) \|Z(t+1) - Z(t)\| \\
	\leq \sum\limits_{i=1}^{m} M \| X(t+1)) - d^i(X(t)) \| + m \alpha(t) \|Z(t+1) - Z(t)\| \\
	\leq m \left( M + \alpha(t) \right) \|Z(t+1) - Z(t)\| \\
	\end{split}
\end{equation*}
where the third inequality follows from (\ref{StateEq11}). \\
Define $\rho_2 = m \left( M + \alpha(t) \right)$ and the result follows.
\end{proof}

\subsection{Similarity to KMEANS}
The famous KMEANS algorithm has close proximity to PALM-clustering algorithm. KMEANS alternates between cluster assignments and center updates as well. In detail, we can write its steps in the following manner

\begin{enumerate}[(1)]
	\item Initialization: Set $t=0$, and pick random centers $Y(0) \in (\mathbb{R}^n)^k$

	\item For each $t=0,1, \cdots$ generate a sequence $\left\lbrace(V(t),Y(t))\right\rbrace_{t \in \mathbb{N}}$ as follows:
	\begin{enumerate}[(2.1)]
		\item Cluster Assignment: For $i=1, \cdots ,m$ compute
		\begin{equation}
			v^i(t+1) = \arg\min\limits_{v^i \in \Delta} \left\lbrace \langle v^i , d^i(Y(t)) \rangle\right\rbrace \label{StateEq12}
		\end{equation}
		
		\item Centers Update: For $l=1, \cdots ,k$ compute
		\begin{equation}
			y^l(t+1) = \frac{\sum_{i=1}^{m} v^i_l(t+1) a^i}{\sum_{i=1}^{m} v^i_l(t+1)} \label{StateEq13}
		\end{equation}
	\end{enumerate}
\end{enumerate}
The KMEANS algorithm obviously resemble PALM-clustering algorithm. Assuming same starting point $X(0) = Y(0)$ and by taking $\nu \to 0$, we have
\begin{equation*}
	\begin{split}
	V(t) = \lim_{\nu \to 0} W(t) \\
	Y(t) = \lim_{\nu \to 0} X(t)
	\end{split}
\end{equation*}

\newpage

%\section{Clustering via ADMM Approach }
%
%First we add new variables $z^l, l=1, \cdots ,k$, and formulate an equivalent problem to the clustering problem (see (1.2)):
%
%\begin{equation}
%	\min\limits_{x^1, \dots ,x^k \in \mathbb{R}^n} \min\limits_{w^1, \dots ,w^m \in \mathbb{R}^k} \min\limits_{z^1, \dots ,z^k \in S} \left\lbrace \sum\limits_{i=1}^{m} v_i \sum\limits_{l=1}^{k} w^i_l d(x^l , a^i) \mid w^i \in \Delta^i , i=1, \dots ,m , x^l = z^l, l=1, \dots ,k \right\rbrace
%\end{equation}
%
%We present the augmented Lagrangian associated with the clustering problem
%
%\begin{equation}
%	L_\rho({\bf x},{\bf z},{\bf y},{\bf w}) = H({\bf x},{\bf w}) + \sum\limits_{l=1}^k (y^l)^T (x^l -z^l) + \frac{\rho}{2} \sum\limits_{l=1}^k \| x^l -z^l \|^2
%\end{equation}
%
%\noindent ADMM update:
%\begin{center}
%	$x^{l,t+1} := \frac{\rho z^{l,t} - y^{l,t} + 2\sum\limits_{i=1}^m {v_i w^{i,t}_l a^i} }{\rho + 2\sum\limits_{i=1}^m v_i w^{i,t}_l}$ \\ \bigskip
%	$z^{l,t+1} := \Pi_S(x^{l,t+1} + \frac{y^{l,t}}{\rho} )$ \\ \bigskip
%	$y^{l,t+1} := y^{l.t} + \rho (x^{l,t+1} - z^{l,t+1})$ \\ \bigskip
%	$w^{i,t+1} \in \left\lbrace w \in \mathbb{R}^k \mid w \in \Delta^i,\text{ such that if }l \not\in Nearest({\bf x}^{t+1} ,a^i) \text{ then } w^i_l=0 \right\rbrace $
%\smallskip
%	$\text{where } Nearest({\bf x} ,a^i) := \left\lbrace 1 \leq l \leq k \mid \|x^l -a^i\|= \min\limits_{1 \leq j \leq k} \|x^j - a^i\| \right\rbrace$
%\end{center}
%
%\newpage


%\begin{thebibliography}{99}

%\end{thebibliography}



\end{document}
