\documentclass[11pt]{article} 
\usepackage[american]{babel}
\usepackage{makeidx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumerate}

\oddsidemargin= -12pt \evensidemargin= -12pt
\topmargin=-45pt
\binoppenalty=10000
\relpenalty=10000
\textwidth=6.5in\textheight=9in \baselineskip=18pt
\parskip=6pt plus 1pt
\numberwithin{equation}{section}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[proposition]


\begin{document}

%\begin{center} 
%\Large {Tel-Aviv University}
%
%\Large {The Raymond and Beverly Sackler Faculty of} \\
%\Large {Exact Sciences} \\
%\Large {The Department of Statistics and Operations} \\
%\Large {Research}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {\bf Draft }
%
%\bigskip
%\bigskip
%\bigskip
%\bigskip
%
%\Large{Thesis Submitted Towards the Degree of Master} \\
%\Large {of Science in Operations Research}
%
%\bigskip
%
%\Large {Sergey Voldman}
%
%\bigskip
%\bigskip
%\bigskip
%
%\Large {Supervisors:} \\
%\Large {Prof. Marc Teboulle} \\
%\Large {Prof. Shoham Sabach}
%
%\bigskip
%\bigskip
%
%\Large{2015}
%
%\end{center}
%
%\newpage
%
%\begin{center} \Large {\bf Draft}
%\end{center}
%
%\begin{abstract} 
%
%To-do...
%
%\newpage
%
%\end{abstract}
%
%\section{Introduction}
%
%To-do...
%
%\newpage
%
%
%Hi Shoham, \\
%
%I Added the PALM steps for the clustering problem, and also reformulated parts of document according to the notes you wrote me. \\
%
%Regarding note (4), it implicitly suggests that $\sum\limits_{i=1}^m w^i_l = 1$, how come? {\bf You are right. I modified item (4).}\\
%
%I started formulating Lagrangians as part of ADMM algorithm. If I omit some of the constrains, and don't take them into the Lagrangian, then when I compute for instance $x^{k+1} = argmin_{x} L_{\rho}(x,z^k,y^k)$, should it be solved subjected to the omitted constrain? {\bf If you write  constraint in the Lagrangian with corresponding multiplier then, of course, you can omit it from the constraint set.} \\
%
%
%\newpage
%
%{\bf Hi Sergei, \\
%
%First of all, the file you sent me looks good and mostly correct. Few remarks: \\
%
%(1) For simplicity, I will introduce some notations like ${\bf x} = (x^1, x^{2} , \ldots ,x^{m})$ and similarly ${\bf w} = (w^1, w^{2} , \ldots ,w^{m})$. \\
%
%(2) The first observation that you had to do here is that you $H$ is a quadratic function with respect to ${\bf x}$ while it is linear function with respect to ${\bf w}$. \\
%
%(3) In your item (ii) you say that the Lipschitz moduli is $2$. This is not correct. Using the terms that we have used in the PALM paper, the Lipschitz moduli of $\nabla_{x^l}H$ should be written as a function of all the other involved variables, i.e., in this case (of the clustering) of the variables $x^{n}$ for $n \neq l$ and ${\bf w}$. But as can be seen from what you showed, in this case, there is no dependence on $x^{n}$ for $n \neq l$ but only ${\bf w}$. This is important fact that should be noted. \\
%
%(4) In addition to the previous comment. You may get a better upper bound for the Lipschitz constant, for example, if we define $w_{l}^{max} = \max_{1 \leq i \leq m} w_{l}^{i}$ then we get that $2\sum\limits_{i=1}^{m} v_i w_l^i \leq 2w_{l}^{max}\sum\limits_{i=1}^{m} v_i = 2w_{l}^{max}$. It is clear that $w_{l}^{max} \geq 1$. So we might get a better constant which could be important in the implementation of the algorithm.\\
%
%(5) It is also important to note that $H$ is strongly convex function with respect to each variable $x^{l}$ with strong convexity parameter $\rho_{l} = 2\sum\limits_{i=1}^{m} v_i w_l^i$ (and also strongly convex with respect to whole ${\bf x}$).
%
%(6) Since with respect to ${\bf w}$ the function $H$ is linear it is obvious that the Lipschitz constant of the gradient is zero. This is not really a problem and we can deal with it quite easily. You can find in the PALM paper a discussion about it!
%
%(7) Please proceed and formulate the PALM algorithm in this case. Verify the steps of the algorithm!}

\newpage

\section{The Clustering Problem}

Let $\mathcal{A}= \left\lbrace a^1, \dots ,a^m \right\rbrace$ be a given set of point in the subset $\textit{S} \subset \mathbb{R}^n$, and let $1 < k < m$ be a fixed given number of clusters. The clustering problem consists of partitioning the data $\mathcal{A}$ into $k$ subsets $\left\lbrace A^1, \dots ,A^k \right\rbrace$, called clusters. For each $l=1, \cdots ,k$, the cluster $A_l$ is represented by its center $x^l$, and we want to determine $k$ cluster centers $\left\lbrace x_1, \cdots ,x_k \right\rbrace$ such that the sum of proximity measures from each point $a^i$ to a nearest cluster center $x^l$ is minimized. In hard clustering problem, we demand the clusters are mutually exclusive, and in soft clustering case, non-empty intersection of clusters is allowed.

\section{Clustering via PALM approach }

The clustering problem formulation is given by

\begin{equation}
	\min\limits_{x^1, \dots ,x^k \in S} F(x^1, \dots ,x^k) := \sum\limits_{i=1}^{m} v_i \min\limits_{1 \le l \le k} d(x^l,a^i), \label{StateEq1}
\end{equation}

\noindent with $\textit{d}(\cdot ,\cdot)$ being a distance-like function, and $v_i$ are positive weights such that $\sum\limits_{i=1}^{m} v_i = 1$. 

An equivalent smooth formulation to the clustering problem

\begin{equation}
	\min\limits_{x^1, \dots ,x^k \in S} \min\limits_{w^1, \dots ,w^m \in \mathbb{R}^k} \left\lbrace \sum\limits_{i=1}^{m} v_i \sum\limits_{l=1}^{k} w^i_l d(x^l , a^i) \mid w^i \in \Delta^i , i=1, \dots ,m \right\rbrace , \label{StateEq2}
\end{equation}

\noindent where $\Delta^i$ is the unit simplex in $\mathbb{R}^k$ given by

\begin{equation}
	\Delta^i = \left\lbrace w^i \in \mathbb{R}^k \mid \sum\limits_{l=1}^{k} w^i_l =1, w^i_l \geq 0 , l=1, \dots ,k \right\rbrace .
\end{equation}

PALM algorithms addresses nonconvex-nonsmooth problems of the form

\begin{equation}
	minimize_{x,y} \Psi(x,y):=f(x)+g(y)+H(x,y) , \label{StateEq3}
\end{equation}

\noindent and in the extended form for \textit{p} blocks

\begin{equation}
	minimize\left\lbrace \Psi(x_1, \dots ,x_p):= \sum\limits_{i=1}^{p}f_i(x_i)+H(x_1, \dots ,x_p) : x_i \in \mathbb{R}^{n_i} \right\rbrace , \label{StateEq4}
\end{equation}

\noindent where $H : \mathbb{R}^N \rightarrow \mathbb{R}$ with $N= \sum^{p}_{i=1} n_i$ is assumed to be $C^1$ and each $f_i , i= 1, \dots ,p$, is proper and lower-semicontinuous function.

Applying the PALM notations to the clustering problem formulation (1.2), with distance-like function $d(u,v)= \| u-v \|^2$, setting $f_l(x^l)= \delta_S(x^l)$, $l=1, \dots ,k $, $g_i(w^i)= \delta_{\Delta^i}(w^i)$, $i=1, \dots ,m$ and $H(x^1, \dots ,x^k,w^1, \dots ,w^m)= \sum\limits_{i=1}^{m} v_i \sum\limits_{l=1}^{k} w^i_l d(x^l , a^i)$.

Next, we confirm all requirements of $f_l$, $g_i$ and $H$ as listed in Assumptions 1 and 2 at (reference to PALM article). For simplicity, we introduce some notations ${\bf x} = (x^1, x^{2} , \ldots ,x^{k})$ and similarly ${\bf w} = (w^1, w^{2} , \ldots ,w^{m})$. Also ${\bf x}^{-l} = (x^1, \ldots ,x^{l-1}, x^{l+1}, \ldots ,x^{k})$ and similarly \\ ${\bf w}^{-i} = (w^1, \ldots ,w^{i-1}, w^{i+1}, \ldots ,w^{m})$.
\begin{enumerate}[(i)]
  \item Since $f_l,g_i,H \geq 0$ they all are proper. $g_i$ and $H$ are lower semicontinuous since $\Delta_i$ is closed and $H$ in $C^2$. As for lower semicontinuity of $f_l$ it requires $S$ to be closed.
  \item The partial gradient $\nabla_{x^l}H(\textbf{x}, \textbf{w})$ is globally Lipschitz with moduli $L_{x^l}({\bf x}^{-l}, {\bf w})=2\sum\limits_{i=1}^{m} v_i w_l^i \leq \\ \leq 2w_{l}^{max}\sum\limits_{i=1}^{m} v_i = 2w_{l}^{max}$, for $l=1, \dots ,k$, where $w^{max}_l:=\max\limits_{i=1, \cdots ,m} w^i_l$.
  \item $H$ is linear with respect to ${\bf w}$ thus $\nabla_{x^l}H(\textbf{x}, \textbf{w})$ is globally Lipschitz with moduli $L_{w^i}({\bf x}, {\bf w}^{-i})=0$, for $i=1, \dots ,m$. For PALM's proximal steps remain always well-defined, we set \\ $L_{w^i}({\bf x}, {\bf w}^{-i})= \mu_i > 0$, for $i=1, \dots ,m$ (see Remark 3 (iii)). Similarly, in case $L_{x^l}({\bf x}^{-l}, {\bf w})$ is too close to $0$, we set $L_{x^l}({\bf x}^{-l}, {\bf w}) = \nu_l >0$, for $l=1, \cdots ,k$.
%  \item $\| \nabla_{x^l}H(x^1, \dots ,x^{l-1},s,x^{l+1}, \dots ,x^k, \textbf{w}) - \nabla_{x^l}H(x^1, \dots ,x^{l-1},t,x^{l+1}, \dots ,x^k, \textbf{w}) \| = \\ \| \sum\limits_{i=1}^{m} v_i w_l^i \cdot 2(s-a^i) - \sum\limits_{i=1}^{m} v_i w_l^i \cdot 2(s-a^i) \| = 2\sum\limits_{i=1}^{m} v_i w_l^i \| s-t \| \leq 2 \| s-t \|$, thus the partial gradient $\nabla_{x^l}H$ is globally Lipschitz with moduli $L_{x^l}(\cdot)=2$, for $l=1, \dots ,k$.\\
%  Also, $\| \nabla_{w^i}H(\textbf{x},w^1, \dots ,w^{i-1},s,w^{l+1}, \dots ,w^m) - \nabla_{w^i}H(\textbf{x},w^1, \dots ,w^{i-1},t,w^{l+1}, \dots ,w^m) \| = \\ \| (v_i \|x^1 - a^i \|, \dots ,v_i\|x^k - a^i\|)^T - (v_i \|x^1 - a^i \|, \dots ,v_i\|x^k - a^i\|)^T \| = 0$, thus the partial gradient $\nabla_{w^i}H$ is globally Lipschitz with moduli $L_{w^i}(\cdot)=0$, for $i=1, \dots ,m$.
%  \item $\inf \left\lbrace L_{x^l}(\cdot) \right\rbrace = \inf \left\lbrace L_{w^i}(\cdot) \right\rbrace =0 \ngtr 0$ (this is a problem) 
%  and $\sup \left\lbrace L_{x^l}(\cdot) \right\rbrace = \sup \left\lbrace L_{w^i}(\cdot) \right\rbrace =2.$
  \item $\inf \left\lbrace L_{w^i}({\bf x}, {\bf w}^{-i}) \right\rbrace = \sup \left\lbrace L_{w^i}({\bf x}, {\bf w}^{-i}) \right\rbrace = \mu_i, i=1, \cdots ,m$ \\ and $\sup \left\lbrace L_{x^l}({\bf x}^{-l}, {\bf w}) \right\rbrace \leq 2w^{max}_l$, $\inf \left\lbrace L_{x^l}({\bf x}^{-l}, {\bf w}) \right\rbrace\geq \nu_l, l=1, \cdots ,k$.
  \item $\nabla H$ is Lipschitz continuous on bounded subset, since $H$ in $C^2$ (see Remark 3 (iv)).
  \item PALM requires $\Psi$ to be KL function. $H$ is real polynomial function, thus satisfies the KL property. $\Delta_i$ is semi-algebraic set, and we require $S$ to be semi-algebraic set.
\end{enumerate}

Next, we formulate PALM's steps for the clustering problem, and explicitly compute the proximal formulas.

{\bf PALM-Clustering}
\begin{enumerate}[(1)]
  \item Initialization: Select random vectors $x^{l,0} \in S, l=1, \cdots ,k$ and $w^{i,0} \in \Delta^i, i=1 \cdots ,m$.
  \item For each $t=0,1, \cdots$ generate a sequence $\left\lbrace (x^{1,t}, \cdots , x^{k,t}, w^{1,t}, \cdots ,w^{m,t}) \right\rbrace_{t \in \mathbb{N}}$ as follows:
  \begin{enumerate}[(2.1)]
  	\item For each $l=1, \cdots ,k $ compute:
  	\begin{enumerate}[(2.1.1)]
  		\item Take $\gamma_l>1$, set $c^t_l = \gamma_l L_{x^l}(x^{1,t+1}, \cdots, x^{l-1,t+1}, x^{l+1,t}, \cdots ,x^{k,t}, w^{1,t}, \cdots ,w^{m,t})$ and compute
  		\begin{center}
  			$x^{l,t+1} \in prox^{f_l}_{c^t_l}(x^{l,t} - \frac{1}{c^t_l}\nabla_{x^l}H(x^{1,t+1}, \cdots, x^{l-1,t+1}, x^{l,t}, x^{l+1,t}, \cdots ,x^{k,t}, w^{1,t}, \cdots ,w^{m,t}))$ \\ $= \Pi_S \left( x^{l,t} - \frac{\sum\limits_{i=1}^m v_iw^{i,t}_l 2(x^{l,t}-a^i)}{\gamma_l \max \left\lbrace \nu_l, 2\sum\limits_{i=1}^m v_iw^{i,t}_l \right\rbrace}\right) = \Pi_S \left( x^{l,t}\left(1- \frac{\sum\limits_{i=1}^m v_iw^{i,t}_l}{\gamma_l \max \left\lbrace \frac{\nu_l}{2}, \sum\limits_{i=1}^m v_iw^{i,t}_l \right\rbrace}\right) + \frac{\sum\limits_{i=1}^m v_iw^{i,t}_l a^i}{\gamma_l \max \left\lbrace \frac{\nu_l}{2}, \sum\limits_{i=1}^m v_iw^{i,t}_l \right\rbrace} \right)$
  		\end{center}
  	\end{enumerate}
  \end{enumerate}
  \begin{enumerate}[(2.2)]
  	\item For each $i=1, \cdots ,m $ compute:
  	\begin{enumerate}[(2.2.1)]
  		\item Take $\beta_i>1$, set $d^t_i = \beta_i L_{w^i}(x^{1,t+1}, \cdots ,x^{k,t+1}, w^{1,t+1}, \cdots ,w^{i-1,t+1}, w^{i+1,t}, \cdots ,w^{m,t})$ and compute
  		\begin{center}
  			$w^{i,t+1} \in prox^{g_i}_{d^t_i}(w^{i,t} - \frac{1}{d^t_i}\nabla_{w^i}H(x^{1,t+1}, \cdots ,x^{k,t+1}, w^{1,t+1}, \cdots ,w^{i-1,t+1}, w^{i,t}, w^{i+1,t}, \cdots ,w^{m,t})$ \\ $= \Pi_{\Delta^i}(w^{i,t} - \frac{v_i}{\beta_i \mu_i} (w^{i,t}_1 \| x^{1,t+1} - a^i \|^2, \cdots , w^{i,t}_k \| x^{k,t+1} - a^i \|^2 )^T)$ \\ $ = \Pi_{\Delta^i}((w^{i,t}_l (1 - \frac{v_i \| x^{l,t+1} - a^i \|^2}{\beta_i \mu_i} ))_{1 \leq l \leq k})$
  		\end{center}
  	\end{enumerate}
  \end{enumerate}
\end{enumerate}

\newpage

\section{Clustering via ADMM approach }

First we add new variables $z^l, l=1, \cdots ,k$, and formulate an equivalent problem to the clustering problem (see (1.2)):

\begin{equation}
	\min\limits_{x^1, \dots ,x^k \in \mathbb{R}^n} \min\limits_{w^1, \dots ,w^m \in \mathbb{R}^k} \min\limits_{z^1, \dots ,z^k \in S} \left\lbrace \sum\limits_{i=1}^{m} v_i \sum\limits_{l=1}^{k} w^i_l d(x^l , a^i) \mid w^i \in \Delta^i , i=1, \dots ,m , x^l = z^l, l=1, \dots ,k \right\rbrace
\end{equation}

We present the augmented Lagrangian associated with the clustering problem

\begin{equation}
	L_\rho({\bf x},{\bf z},{\bf y},{\bf w}) = H({\bf x},{\bf w}) + \sum\limits_{l=1}^k (y^l)^T (x^l -z^l) + \frac{\rho}{2} \sum\limits_{l=1}^k \| x^l -z^l \|^2
\end{equation}

\noindent ADMM update:
\begin{center}
	$x^{l,t+1} := \frac{\rho z^{l,t} - y^{l,t} + 2\sum\limits_{i=1}^m {v_i w^{i,t}_l a^i} }{\rho + 2\sum\limits_{i=1}^m v_i w^{i,t}_l}$ \\ \bigskip
	$z^{l,t+1} := \Pi_S(x^{l,t+1} + \frac{y^{l,t}}{\rho} )$ \\ \bigskip
	$y^{l,t+1} := y^{l.t} + \rho (x^{l,t+1} - z^{l,t+1})$ \\ \bigskip
	$w^{i,t+1} \in \left\lbrace w \in \mathbb{R}^k \mid w \in \Delta^i,\text{ such that if }l \not\in Nearest({\bf x}^{t+1} ,a^i) \text{ then } w^i_l=0 \right\rbrace $
\smallskip
	$\text{where } Nearest({\bf x} ,a^i) := \left\lbrace 1 \leq l \leq k \mid \|x^l -a^i\|= \min\limits_{1 \leq j \leq k} \|x^j - a^i\| \right\rbrace$
\end{center}

\newpage

\noindent
$\|\gamma(t+1)\| \leq \rho_2\|z(t+1)-z(t)\| \\$
$\nabla H$

%\begin{thebibliography}{99}

%\end{thebibliography}



\end{document}
