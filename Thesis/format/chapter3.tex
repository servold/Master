 \chapter{Clustering: The Squared Euclidean Norm Case} \label{Chptr3}

\noindent \noindent \hrulefill

We develop an algorithm, called KPALM, for the clustering problem given in (\ref{StateEq4}) with squared Euclidean distance-like function. We show that the acquired model is a sum of smooth and nonsmooth function, therefore it motivates us to apply the PALM idea and the convergence methodology described in \Cref{State_PALM_Theory}. We prove that the generated sequence converges to a critical point of the objective function $\sigma$ of the clustering problem (see (\ref{StateEq4})). We show that the k-means algorithm is a special case of KPALM algorithm and can be recovered from the developed framework, however, the convergence analysis can not be used for the k-means algorithm. Therefore, we prove the convergence of k-means to critical point, separately by using again the general methodology of \Cref{State_PALM_Theory}. Assuming the uniqueness of labeling in the output of the k-means algorithm we improve the convergence result to local minimum.

\noindent \noindent \hrulefill

\section{Clustering with PALM}\label{State_Clustering_SqNorm}

In this section we tackle the clustering problem, given in (\ref{StateEq4}), for which the proximity function $d(\cdot,\cdot)$ is taken to be the classical distance function defined by $d(u,v) = \|u-v\|^2$. We devise a PALM-like algorithm, based on the discussion in the previous section.
Since the clustering problem has specific structure, we are ought to exploit it in the following manner.
\begin{enumerate}[(1)]
	\item The function $w \mapsto H(w,x)$, for fixed $x$, is linear and therefore there is no need to linearize it as suggested in the framework which was discussed in \Cref{State_PALM_Theory}.
	\item The function $x \mapsto H(w,x)$, for fixed $w$, is quadratic and convex. Hence, there is no need to add a proximal term as suggested in the framework of PALM.
\end{enumerate}

As in the PALM algorithm, our algorithm is based on the old approach of Alternating Minimization, with the following adaptations which are motivated by the observations mentioned above. More precisely, with respect to $w$ we suggest to regularize the first subproblem with proximal term as follows
\begin{equation}
	w^i(t+1) = \arg\!\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i - w^i(t)\|^2 \right\rbrace, \quad i=1,2, \ldots, m , \label{State_w_update}
\end{equation}
where $\alpha_i(t) > 0$ for all $i=1,2, \ldots, m$. On the other hand, with respect to $x$ we perform exact minimization, that is,
\begin{equation}
	x(t+1) = \arg\!\min \left\lbrace H(w(t+1), x) \mid x \in \mathbb{R}^{nk} \right\rbrace . \label{State_x_update}
\end{equation}
It is easy to check that all subproblems, with respect to $w^i$, $i=1,2, \ldots, m$, and $x$, can be rewritten explicitly (where we use $P_{\Delta}$ for the orthogonal projection onto the set $\Delta$). Thus, we can present now the KPALM algorithm.
\begin{framed}
\noindent \textbf{KPALM}
\begin{enumerate}[(1)]
	\item Initialization: $(w(0),x(0)) \in \Delta^m \times \mathbb{R}^{nk} .$
	\item General step $\left( t=0,1, \ldots \right)$:
	\begin{enumerate}[(2.1)]
		\item Cluster assignment: choose certain $\alpha_i(t) > 0$, $i=1,2, \ldots, m$, and compute
		\begin{equation}
			w^i(t+1) = P_{\Delta} \left(w^i(t) - \frac{d^i(x(t))}{\alpha_i(t)}\right) . \label{StateEq5}
		\end{equation}
		\item Center update: for each $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			x^l(t+1) = \frac{\sum_{i=1}^{m} w^i_l(t+1) a^i}{\sum_{i=1}^{m} w^i_l(t+1)} . \label{StateEq6}
		\end{equation}
	\end{enumerate}
\end{enumerate}
\end{framed}

We begin our analysis of the KPALM algorithm with the following boundedness property of the generated sequence. For simplicity, from now on, we denote $z(t):=\left( w(t),x(t) \right)$, $t \in \mathbb{N}$.
\begin{proposition}[Boundedness of KPALM sequence] \label{boundedness_prop}
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM. Then, the following statements hold true.
\begin{enumerate}[(i)]
	\item For all $l=1, 2, \ldots ,k$, the sequence $\left\lbrace x^l(t) \right\rbrace_{t \in \mathbb{N}}$ is contained in $Conv(\mathcal{A})$, the convex hull of $\mathcal{A}$, and therefore bounded by $M = \max\limits_{1 \leq i \leq m} \| a^i \|$. \label{boundedness_prop_1}
	\item The sequence $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded in $\mathbb{R}^{km} \times \mathbb{R}^{nk}$. \label{boundedness_prop_2}
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}[(i)]
	\item  Let $1\leq l \leq k$. We set $\lambda_i = w^i_l(t)/\sum\limits_{j=1}^{m} w^j_l(t), i=1, 2, \ldots ,m$, then $\lambda_i \geq 0$ and $\sum\limits_{i=1}^{m} \lambda_i = 1$. From (\ref{StateEq6}) we have
	\begin{equation}
		x^l(t) = \frac{\sum_{i=1}^{m} w^i_l(t) a^i}{\sum_{i=1}^{m} w^i_l(t)} 
		= \sum_{i=1}^{m} \left( \frac{ w^i_l(t)}{\sum_{j=1}^{m} w^j_l(t)} \right) a^i 
		= \sum\limits_{i=1}^{m} \lambda_i a^i \in Conv(\mathcal{A}), \label{StateBound}
	\end{equation}
	which proves that $x^l(t)$ is in the convex hull of $\mathcal{A}$, for all $l = 1, 2, \ldots, k$ and $t \in \mathbb{N}$. Taking the norm of $x^l(t)$ and using (\ref{StateBound}) yields that
	\begin{equation*}
		\| x^l(t) \| = \left\lVert \sum_{i=1}^{m} \lambda_i a^i \right\lVert
		\leq \sum_{i=1}^{m} \lambda_i \| a^i \|
		\leq \sum_{i=1}^{m} \lambda_i \max\limits_{1 \leq i \leq m} \| a^i \| = M .
	\end{equation*}
	\item The sequence $\left\lbrace w(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded, since $w^i(t) \in \Delta$ for all $i=1, 2, \ldots ,m$ and $t \in \mathbb{N}$. Combined with the previous item, the result follows.
\end{enumerate} 
\end{proof}

The following assumption will be crucial for the coming analysis.
\begin{assumption}\label{StateMainAssum}
\begin{enumerate}[(i)] 
	\item For all $1 \leq i \leq m$, the chosen sequence of parameters $\left\lbrace \alpha_i(t) \right\rbrace_{t \in \mathbb{N}}$ is bounded, that is, there exist $\underline{\alpha_i} > 0$ and $\overline{\alpha_i} < \infty$ such that
		\begin{equation}
			\underline{\alpha_i} \leq \alpha_i(t) \leq \overline{\alpha_i}, \quad \forall \: t \in \mathbb{N}.
		\end{equation}		 \label{StateMainAssum1}
	\item For all $t \in \mathbb{N}$ there exists $\underline{\beta} > 0$ such that
		\begin{equation}
			2 \min\limits_{1 \leq l \leq k} \sum\limits_{i=1}^{m} w^i_l(t) := \beta(w(t)) \geq \underline{\beta}. \label{State_beta}
		\end{equation}		 \label{StateMainAssum2}
\end{enumerate}
\end{assumption}
It should be noted that Assumption \ref{StateMainAssum}(\ref{StateMainAssum1}) is very mild since the parameters $\alpha_i(t)$, for $1 \leq i \leq m$ and $t \in \mathbb{N}$, can be chosen arbitrarily by the user and therefore it can be controlled such that the boundedness property holds true. Assumption \ref{StateMainAssum}(\ref{StateMainAssum2}) is essential since if it is not true then $w^i_l(t)=0$ for all $1 \leq i \leq m$, which means that the center $x^l$ does not play any role in the solution process which is, of course, meaningless situation.

\begin{lemma}[Strong convexity of $H(w,x)$ in $x$] \label{StateEq14}
The function $x \mapsto H(w,x)$ is strongly convex with parameter $\beta(w)$ which defined in (\ref{State_beta}), whenever $\beta(w) > 0$.
\end{lemma}

\begin{proof}
Since the function $x \mapsto H(w,x) = 
\sum\limits_{l=1}^{k} \sum\limits_{i=1}^{m} w^i_l \|x^l - a^i\|^2$ is $C^2$, it is strongly convex if and only if the smallest eigenvalue of the corresponding Hessian matrix is positive. Indeed, the Hessian is given by

\begin{center}
$\nabla_{x^j} \nabla_{x^l} H(w,x) = 
\begin{cases} 0 &\mbox{if } j \neq l, \quad 1 \leq j,l \leq k ,
\\ 2\sum\limits_{i=1}^{m} w^i_l &\mbox{if } j = l, \quad 1 \leq j,l \leq k. \end{cases} $
\end{center}

Since the Hessian is a diagonal matrix, the smallest eigenvalue is $\beta(w) = \\ 2\min\limits_{1 \leq l \leq k} \sum\limits_{i=1}^{m} w^i_l$, and the result follows.
\end{proof}

Now we are ready to prove global convergence of the sequence $\left\lbrace z(t)\right\rbrace_{t \in \nn}$ generated by KPALM to a critical point of $\sigma$ given in (\ref{StateEq4}). We will follow here the general procedure which was discussed is \Cref{State_PALM_Theory}. Therefore we need to prove that $\left\lbrace z(t)\right\rbrace_{t \in \nn}$ is a gradient-like descent sequence (see \Cref{gradient_like_seq_def}), that is, that conditions (C1) and (C2) hold. We begin by proving condition (C1).

\begin{proposition}[Sufficient decrease property] \label{State_KPALM_SDP}
Suppose that Assumption \ref{StateMainAssum} holds true and let $\left\lbrace z(t) \right\rbrace_{t \in \nn}$ be the sequence generated by KPALM. Then there exists $\rho_1 > 0$ such that 
\begin{equation*}
	\rho_1 \|z(t+1) - z(t)\|^2 \leq \sigma(z(t)) - \sigma(z(t+1)), \quad \forall \: t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
From step (\ref{StateEq5}), see also (\ref{State_w_update}), we derive, for each $i=1,2, \ldots, m$, the following inequality
\begin{align*}
	H^i(w(t+1),x(t)) &+ \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 = \\ 
	&= \langle w^i(t+1) , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \\
	& \leq \langle w^i(t) , d^i(x(t)) \rangle + \frac{\alpha_i(t)}{2} \|w^i(t) - w^i(t)\|^2 \\
	& = \langle w^i(t) , d^i(x(t)) \rangle \\
	& = H^i(w(t),x(t)) .
\end{align*}
Hence, we obtain for all $t \in \nn$, that
\begin{equation}
	\frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 
	\leq H^i(w(t),x(t)) - H^i(w(t+1),x(t)) . \label{StateEq18}
\end{equation}
Denote $\underline{\alpha} = \min\limits_{1 \leq i \leq m} \underline{\alpha_i}$. Summing inequality (\ref{StateEq18}) over $i=1, 2, \ldots ,m$ yields
\begin{align}
	\frac{\underline{\alpha}}{2} \|w(t+1) - w(t)\|^2 
	& = \frac{\underline{\alpha}}{2} \sum\limits_{i=1}^{m} \|w^i(t+1) - w^i(t)\|^2 \nonumber \\
	& \leq \sum\limits_{i=1}^{m} \frac{\alpha_i(t)}{2} \|w^i(t+1) - w^i(t)\|^2 \nonumber \\
	& \leq \sum\limits_{i=1}^{m} \left[ H^i(w(t),x(t)) - H^i(w(t+1),x(t)) \right] \nonumber \\
	& = H(w(t),x(t)) - H(w(t+1),x(t)) ,  \label{StateEq16}
\end{align}
where the first inequality follows from Assumption \ref{StateMainAssum}(\ref{StateMainAssum1}) and the definition of $\underline{\alpha}$.

From Assumption \ref{StateMainAssum}(\ref{StateMainAssum2}) we have that $\beta(w(t)) \geq \underline{\beta}$, for all $t \in \nn$, and from \Cref{StateEq14} it follows that the function $x \mapsto H(w(t),x)$ is strongly convex with parameter $\beta(w(t))$. H using the strong convexity yields that
\begin{align}
	H(w(t+1),x(t))  - H(w(t+1),x(t+1)) &\geq \left\langle \nabla_x H(w(t+1),x(t+1)) , x(t)-x(t+1) \right\rangle \nonumber \\
	&\quad\quad\quad + \frac{\beta(w(t))}{2} \|x(t) - x(t+1)\|^2 \nonumber \\
	& = \frac{\beta(w(t))}{2} \|x(t+1) - x(t)\|^2  \nonumber \\
	& \geq \frac{\underline{\beta}}{2} \|x(t+1) - x(t)\|^2 , \label{StateEq17}
\end{align}
where the equality follows from (\ref{State_x_update}), since $\nabla_{x} H(w(t+1), x(t+1)) = 0$.
Set $\rho_1 = \frac{1}{2}\min\left\lbrace \underline{\alpha} , \underline{\beta} \right\rbrace$, by combining (\ref{StateEq16}) and (\ref{StateEq17}), we get
\begin{align*}
	\rho_1 \|z(t+1)- z(t)\|^2 
	&= \rho_1 \left( \|w(t+1) - w(t)\|^2 + \|x(t+1) - x(t)\|^2  \right) \\
	&\leq \left[ H(w(t),x(t)) - H(w(t+1),x(t)) \right] \\
	&\quad\quad\quad + \left[ H(w(t+1),x(t)) - H(w(t+1),x(t+1)) \right] \\
	&= H(z(t)) - H(z(t+1)) \\
	&= \sigma(z(t)) - \sigma(z(t+1)),
\end{align*}
where the last equality follows from the fact that $G(w(t)) = 0$, since $w(t) \in \Delta^m$ for all $t \in \mathbb{N}$, and therefore $H(z(t))=\sigma(z(t))$, $t \in \mathbb{N}$. This proves the desired result.
\end{proof}

Now, we are focusing on proving condition (C2) and we will need the following technical result.

\begin{lemma} \label{StateEq11}
The function $d^i$ defined in (\ref{d^i_def}) is Lipschitz continuous in $S$ with constant $4M$, that is,
\begin{equation*}
	\norm{d^i(x) - d^i(y)} \leq 4M \norm{x - y}, \quad \forall \: x,y \in S,
\end{equation*}
where $M = \max\limits_{1 \leq i \leq m} \norm{a^i}$ and  $S=\left\lbrace x \in \rr^k : x^l \in Conv(\mathcal{A}), \; l=1,2,\ldots,k \right\rbrace$.
\end{lemma}

\begin{proof}
Fix any $x,y \in S$, since $d(u,v) = \| u-v \|^2$, we get that
\begin{align*} 
	\norm{d^i(x)  - d^i(y)} 
	&= \left[ \sum\limits_{l=1}^{k} \abs{ \norm{x^l - a^i}^2 - \norm{ y^l -a^i}^2 }^2 \right]^{\frac{1}{2}} \\
	&= \left[ \sum\limits_{l=1}^{k} \left\lvert \norm{x^l}^2 - 2\left\langle x^l,a^i \right\rangle + \norm{a^i}^2 - \norm{y^l}^2 + 2\left\langle y^l,a^i \right\rangle - \norm{a^i}^2 \right\rvert ^2 \right]^{\frac{1}{2}} \\ 
	&\leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \norm{x^l}^2 - \norm{y^l}^2 } + \abs{ 2\left\langle y^l - x^l , a^i \right\rangle } \right)^2 \right]^{\frac{1}{2}} \\ 
	&\leq \left[ \sum\limits_{l=1}^{k} \left( \abs{ \norm{x^l} - \norm{y^l} } \cdot \abs{ \norm{x^l} + \norm{y^l} } + 2 \norm{y^l - x^l} \cdot \norm{a^i} \right)^2 \right]^{\frac{1}{2}} \\
	&\leq \left[ \sum\limits_{l=1}^{k} \left( \norm{x^l - y^l} \cdot 2M + 2 \norm{x^l - y^l}\cdot M \right)^2 \right]^{\frac{1}{2}} \\
	&= \left[ \sum\limits_{l=1}^{k} (4M)^2 \norm{x^l - y^l}^2 \right]^{\frac{1}{2}} \\
	&= 4M \norm{x - y} ,
\end{align*}
where the last inequality follows from the fact that $x^l,y^l \in Conv(\mathcal{A})$ and hence $\norm{x^l},\norm{y^l} \leq M$ for all $l=1,2,\ldots,k$. This proves the desired result.
\end{proof}

Now, using this result we can show that $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ satisfies condition (C2). It is important to note that in this case the boundedness of the generated sequence is obtained (see \Cref{boundedness_prop}(\ref{boundedness_prop_2})) and not need to be assumed.

\begin{proposition}[Subgradient lower bound for the iterates gap] \label{State_KPALM_SGP}
Suppose that Assumption \ref{StateMainAssum} holds true and let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by KPALM.
For each $t \in \nn$ define 
\begin{equation*}
	\gamma(t) := \left( \left( d^i(x(t)) - d^i(x(t-1)) - \alpha_i(t-1)(w^i(t) - w^i(t-1)) \right)_{i=1,2, \ldots, m}, \mathbf{0} \right).
\end{equation*}
Then $\gamma(t) \in \partial \sigma(z(t))$ and there exists $\rho_2 > 0$ such that 
\begin{equation*}
	\| \gamma(t+1)\| \leq \rho_2 \|z(t+1) - z(t)\|, \quad \forall \: t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
By the definition of $\sigma$ (see (\ref{StateEq4})) we get
\begin{equation*}
	\partial \sigma = \nabla H + \partial G  
= \left( \left( \nabla_{w^i} H^i + \partial_{w^i} \delta_{\Delta} \right)_{i=1,2, \ldots ,m} , \nabla_x H \right) .
\end{equation*}
Evaluating the last relation at $z(t+1)$ yields
\begin{align}
	\partial \sigma(z(t + 1)) &= \left( \left( \nabla_{w^i} H^i(w(t+1),x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1,2, \ldots ,m} , \right. \nonumber \\
	&\quad\quad\quad \left. \nabla_x H(w(t+1),x(t+1)) \right) \nonumber \\
	& = \left( \left( d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1,2, \ldots ,m} , \right. \nonumber \\
	&\quad\quad\quad \left. \nabla_x H(w(t+1),x(t+1)) \right) \nonumber \\
	& = \left( \left( d^i(x(t+1)) + \partial_{w^i} \delta_{\Delta}(w^i(t+1)) \right)_{i=1,2, \ldots ,m} , \mathbf{0} \right) , \label{StateEq9}
\end{align}
where the last equality follows from (\ref{State_x_update}), that is, the optimality condition of $x(t+1)$.

The optimality condition of $w^i(t+1)$ which derived from (\ref{State_w_update}), yields that for all $i=1, 2, \ldots ,m$ there exists $u^i(t+1) \in \partial \delta_{\Delta}(w^i(t+1))$ such that
\begin{equation}
	d^i(x(t)) + \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) + u^i(t+1) = \mathbf{0} . \label{StateEq10}
\end{equation}
Setting $\gamma(t+1) := \left( \left( d^i(x(t+1)) + u^i(t+1) \right)_{i=1,2, \ldots ,m}, \mathbf{0} \right)$, it follows from (\ref{StateEq9})  that $\gamma(t+1) \in \partial \sigma(z(t+1))$. Using (\ref{StateEq10}) we obtain
\begin{equation*}
	\gamma(t+1) = \left( \left( d^i(x(t+1)) - d^i(x(t)) - \alpha_i(t)(w^i(t+1) - w^i(t)) \right)_{i=1,2, \ldots, m}, \mathbf{0} \right).
\end{equation*}
Hence, by defining $\overline{\alpha} = \max\limits_{1 \leq i \leq m} \overline{\alpha_i}$, we obtain
\begin{align*}
	\| \gamma(t+1) \|
	& \leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) - \alpha_i(t) \left( w^i(t+1) - w^i(t) \right) \| \\
	& \leq \sum\limits_{i=1}^{m} \| d^i(x(t+1)) - d^i(x(t)) \| + \sum\limits_{i=1}^{m} \alpha_i(t) \| w^i(t+1) - w^i(t) \| \\
	& \leq \sum\limits_{i=1}^{m} 4M \| x(t+1) - x(t) \| +  \overline{\alpha} \sqrt{m}\|w(t+1) - w(t)\| \\
	& \leq \left( 4Mm + \overline{\alpha}\sqrt{m} \right) \|z(t+1) - z(t)\| , 
\end{align*}
where the third inequality follows from \Cref{StateEq11} and the fact that $x^l(t) \in Conv(\mathcal{A})$ for all $l=1,2,\ldots,k$ and $t \in \nn$ (see \Cref{boundedness_prop}). Define $\rho_2 = 4Mm + \overline{\alpha}\sqrt{m}$, and the result follows.
\end{proof}

We record our main convergence result of the KPALM algorithm in the following theorem.

\begin{theorem} \label{KPALM_conv_thrm}
Suppose that Assumption \ref{StateMainAssum} holds true and let $\left\lbrace z(t) \right\rbrace_{t \in \nn}$ be the sequence generated by KPALM. Then, the sequence $\left\lbrace z(t) \right\rbrace_{t \in \nn}$ converges to a critical point of $\sigma$.
\end{theorem}

\begin{proof}
Due to \Cref{State_KPALM_SDP,State_KPALM_SGP} it follows that the sequence $\left\lbrace z(t) \right\rbrace_{t \in \nn}$ is a gradient-like descent sequence. The function $\sigma$ is clearly a semi-algebraic function, therefore in light of \Cref{SDP_SGP_conv_thrm} it is sufficient to prove that $\omega(z(0)) \subset crit(\sigma)$. 
Indeed, let $z^*=\left( w^*,x^* \right) \in \omega(z(0))$ be a limit point of  $\left\lbrace z(t) \right\rbrace_{t \in \nn} = \left\lbrace \left( w(t),x(t) \right) \right\rbrace_{t \in \nn}$ (such a point exists since the sequence is bounded) and let $\left\lbrace \left( w(t_k),x(t_k) \right) \right\rbrace_{t \in \nn}$ be a subsequence such that $\left( w(t_k),x(t_k) \right) \rightarrow \left( w^*,x^* \right)$ as $k \rightarrow \infty$. Since $w(t_k) \in \Delta^m$ for all $k \in \nn$ and due to the closedness of $\Delta^m$ it follows that $w^* \in \Delta^m$, hence $G(w^*)=0$. Therefore, combined with the continuity of $H$ we obtain
\begin{align*}
\lim\limits_{k \rightarrow \infty} \sigma\left( w(t_k),x(t_k) \right) &= \lim\limits_{k \rightarrow \infty} \left\lbrace H\left(w(t_k),x(t_k)\right) + G\left(w(t_k)\right) \right\rbrace = \lim\limits_{k \rightarrow \infty} H\left(w(t_k),x(t_k)\right) \\
&= H\left(w^*,x^* \right) = H\left(w^*,x^* \right) + G(w^*) = \sigma\left(w^*,x^*\right)
\end{align*}
Combining $\gamma(t) \in \partial \sigma(t)$ (see \Cref{State_KPALM_SGP}) with the gradient-like descent property of the sequence $\left\lbrace z(t) \right\rbrace_{t \in \nn}$ implies that $\gamma(t) \rightarrow \mathbf{0}$ as $t \rightarrow \infty$.
Indeed, from \Cref{State_KPALM_SDP} we have
\begin{equation*}
	\rho_1 \norm{z(t+1)-z(t)}^2 \leq \sigma\left(z(t)\right) - \sigma\left(z(t+1)\right).
\end{equation*}
Hence the sequence $\left\lbrace \sigma(z(t))\right\rbrace_{t \in \nn}$ is decreasing and since the sequence is also bounded it converges to some real number $\underline{\sigma}$. Therefore, we deduce that $\norm{z(t+1)-z(t)} \rightarrow 0$ as $t \rightarrow \infty$. Together with \Cref{State_KPALM_SGP} we obtain that $\norm{\gamma(t)} \rightarrow 0$ as $t \rightarrow \infty$.
The closedness property of $\partial\sigma$ implies thus that $\mathbf{0} \in \partial \sigma \left(w^*,x^*\right)$. This proves that $\left(w^*,x^*\right)$ is a critical point of $\sigma$.
\end{proof}

\section{Convergence Analysis of k-means}

The well-known k-means algorithm has close relation to KPALM algorithm, and similarly k-means alternates between cluster assignment and centers update steps as well. In detail, we can write its steps in the following manner.

\begin{framed}
\noindent \textbf{k-means}
\begin{enumerate}[(1)]
	\item Initialization: $x(0) \in \mathbb{R}^{nk}$.
	\item General step $\left( t=0,1, \ldots \right)$:
	\begin{enumerate}[(2.1)]
		\item Cluster assignment: for $i=1, 2, \ldots ,m$ compute
		\begin{equation}
			w^i(t+1) = \arg\!\min\limits_{w^i \in \Delta} \left\lbrace \langle w^i , d^i(x(t)) \rangle\right\rbrace . \label{StateEq12}
		\end{equation}
		\item Center update: for $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			x^l(t+1) = \frac{\sum_{i=1}^{m} w^i_l(t+1) a^i}{\sum_{i=1}^{m} w^i_l(t+1)} . \label{StateEq13}
		\end{equation}
%		\item Stopping criteria: halt if 
%		\begin{equation}
%			\forall 1 \leq l \leq k \quad C^l(t+1)=C^l(t) \label{StateEq15}
%		\end{equation}
	\end{enumerate}
\end{enumerate}
\end{framed}

It is easy to see that if we take in (\ref{State_w_update}), $\alpha_i(t) = 0$ for all $1 \leq i \leq m$ and $t \in \mathbb{N}$, then KPALM becomes k-means. In this case, it is clear that Assumption \ref{StateMainAssum}(\ref{StateMainAssum1}) does not hold and therefore we can not use the convergence analysis presented before for the KPALM algorithm. We aim to use the theory described in \Cref{State_PALM_Theory} once again and show that the sequence generated by k-means converges to a critical point of $\sigma$, as defined is (\ref{StateEq4}). The proof of sufficient decrease property (see \Cref{State_KPALM_SDP}) collapses in this case, since it is based on Assumption \ref{StateMainAssum}(\ref{StateMainAssum1}), that is, $\alpha_i(t) > \underline{\alpha_i} > 0$, for all $t \in \mathbb{N}$ and $i=1,2, \ldots, m$. However, the proof of the subgradient lower bound for the iterates gap property (see \Cref{State_KPALM_SGP}) follows through as is. Now we will prove a sufficient decrease property for k-means. To this end we will first need the following result.

\begin{lemma} \label{StateLemma_x_bounds_w}
Let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by k-means. Then, there exists $c > 0$ such that
\begin{equation*}
	\|w^i(t+1)-w^i(t)\| \leq c \|x(t+1)-x(t)\| , \quad \forall \: i=1,2, \ldots\ m, \: t \in \mathbb{N} .
\end{equation*}
\end{lemma}

\begin{proof}
If $x(t+1)=x(t)$ for some $t \in \nn$ then the result is true since $w(t+1)=w(t)$. Indeed, since we are dealing with hard clustering it means that each data point $a^i$, $i=1,2,\ldots,m$, is assigned to a single cluster and therefore if the centers didn't change then it implies that the clusters didn't change. Assume that $x(t+1) \neq x(t)$ at each $t \in \nn$. At each iteration (with respect to $w$), k-means partitions the set $\mathcal{A}$ into $k$ clusters, and the center of each cluster is its mean. Since the number of the obtained partitions if finite, there exists a finite set of possible centers for all clusters. Denote by $\mathcal{C} =\left\lbrace c^1,c^2,\ldots,c^N \right\rbrace \subset \rr^{nk}$, the set of all different possible centers. From the center update rule (see step (\ref{StateEq13})) it is clear that $x(t)\in \mathcal{C}$ for all $t\in \nn$. In addition, since $\mathcal{C}$ contains $N$ different points we can define
\begin{equation*}
	r:=\min\limits_{1 \leq i <j \leq N} \norm{c^i - c^j},
\end{equation*}
and obviously $r \neq 0$. By the definition of step (\ref{StateEq12}) we have that $w^i(t)\in\Delta$ for all $i=1,2,\ldots,m$ and $t \in \nn$. Thus, it is clear that
\begin{equation*}
	\|w^i(t+1)-w^i(t)\| \leq \sqrt{2}.
\end{equation*}
Since $x(t) \in \mathcal{C}$ for all $t \in \nn$ it follows that $\norm{x(t+1)-x(t)} \geq r$ and therefore
\begin{equation*}
	\|w^i(t+1)-w^i(t)\| \leq \frac{\sqrt{2}}{r}\norm{x(t+1)-x(t)},
\end{equation*}
and the result follows.
\end{proof}

Equipped with the last lemma we can prove the sufficient decrease property of k-means.

\begin{proposition}[Sufficient decrease property for k-means sequence] \label{k_means_SDP}
Suppose that Assumption \ref{StateMainAssum}(\ref{StateMainAssum2}) holds true and let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by k-means. Then, there exists $\rho_1 > 0$ such that 
\begin{equation*}
	\rho_1 \|z(t+1) - z(t)\|^2 \leq \sigma(z(t)) - \sigma(z(t+1)), \quad \forall \: t \in \mathbb{N} .
\end{equation*}
\end{proposition}

\begin{proof}
The function $x \mapsto H(w(t),x)$ remains strongly convex with parameter $\beta(w(t))$ (see (\ref{StateEq17})), hence, as obtained in  \Cref{State_KPALM_SDP}, we have a sufficient decrease in the $x$ variable, namely,
\begin{equation}
	\frac{\underline{\beta}}{2} \|x(t+1)-x(t)\|^2 \leq H(w(t),x(t)) - H(w(t+1),x(t+1)) . \label{StateEq70}
\end{equation}
Setting $\rho_1 = \underline{\beta} \left(m/r^2 + 0.5\right)$, we can write
\begin{align*}
	\rho_1 \|z(t+1)-z(t)\|^2 &= \rho_1 \sum\limits_{i=1}^{m} \|w^i(t+1)-w^i(t)\|^2 + \rho_1 \|x(t+1)-x(t)\|^2 \\
	&\leq \rho_1 \left(1 + m\frac{2}{r^2}\right) \|x(t+1) - x(t)\|^2 \\
	&\leq H(w(t),x(t)) - H(w(t+1),x(t+1)) \\
	&= \sigma(z(t)) - \sigma(z(t+1))
\end{align*}
where the first inequality follows from \Cref{StateLemma_x_bounds_w}, the second follows from (\ref{StateEq70}), and the last equality follows from the fact that $G(w(t))=0$, for all $t \in \mathbb{N}$.
\end{proof}

In light of the preceding discussion we deduce the following convergence result.

\begin{theorem}[k-means converges to critical point]
Suppose that Assumption \ref{StateMainAssum}(\ref{StateMainAssum2}) holds true and let $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ be the sequence generated by k-means. Then, the sequence $\left\lbrace z(t) \right\rbrace_{t \in \mathbb{N}}$ converges to a critical point of $\sigma$.
\end{theorem}

\begin{proof}
Due to \Cref{k_means_SDP,State_KPALM_SGP} it follows that the sequence $\left\lbrace z(t) \right\rbrace_{t \in \nn}$ is a gradient-like descent sequence. The function $\sigma$ is clearly a semi-algebraic function, and the proof that $\omega(z(0)) \subset crit(\sigma)$ is analogous to the one given in \Cref{KPALM_conv_thrm}, hence \Cref{SDP_SGP_conv_thrm} implies the desired result.
\end{proof}

\section{Convergence to Local Minima of k-means}

In this section we present a simple and direct proof that k-means converges to local minima. We start with rewriting the k-means algorithm, in its most familiar form. It should be noted that the algorithm stops whenever all clusters do not change, i.e., $C^l(t+1)=C^l(t)$ for some $t \in \nn$ and all $1 \leq l \leq k$.
\clearpage
\begin{framed}
\noindent \textbf{k-means}
\begin{enumerate}[(1)]
	\item Initialization: $x(0) \in \mathbb{R}^{nk}$.
	\item General step $\left( t=0,1, \ldots \right)$:
	\begin{enumerate}[(2.1)]
		\item Cluster assignment: for $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			C^l(t+1) = \left\lbrace a \in \mathcal{A} : \| a - x^l(t) \| \leq \|a - x^j(t) \|, \; \forall \; 1 \leq j \leq k \right\rbrace. \label{StateEq20}
		\end{equation}
		\item Center update: for $l=1, 2, \ldots ,k$ compute
		\begin{equation}
			x^l(t+1) = \frac{1}{\left| C^l(t+1) \right|} \sum\limits_{a \in C^l(t+1)} a . \label{StateEq21}
		\end{equation}
%		\item Stopping criteria: halt if 
%		\begin{equation}
%			\forall 1 \leq l \leq k \quad C^l(t+1)=C^l(t) \label{StateEq22}
%		\end{equation}
	\end{enumerate}
\end{enumerate}
\end{framed}

As in KPALM, k-means needs Assumption \ref{StateMainAssum}(\ref{StateMainAssum2}) for step (\ref{StateEq21}) to be well defined. In order to prove the convergence of k-means to local minimum, we will need the additional assumption.

\begin{assumption} \label{StateEq23}
Suppose that k-means stops after $\widetilde{t} \in \nn$ iterations, then each $a \in \mathcal{A}$ belongs exclusively to single cluster $C^l(\widetilde{t})$.
\end{assumption}

For any $x \in \mathbb{R}^{nk}$ we denote the super-partition of $\mathcal{A}$ with respect to $x$ by $\overline{C^l}(x) = \left\lbrace a \in \mathcal{A} \mid \right. \left. \|a - x^l\| \leq \|a - x^j\| , \; \forall \; j \neq l \right\rbrace$, for all $1 \leq l \leq k$, and the sub-partition of $\mathcal{A}$ by $\underline{C^l}(x) = \left\lbrace a \in \mathcal{A} \mid \right. \left. \|a - x^l\| < \|a - x^j\|, \; \forall \; j \neq l \right\rbrace$.
Moreover, denote $R_{lj}(t) = \min\limits_{a \in C^l(t)} \left\lbrace \|a - x^j(t)\| - \|a - x^l(t)\| \right\rbrace$ for all $1 \leq l,j \leq k$, and $r(t) = \min\limits_{l \neq j} R_{lj}(t)$. \\
Due to Assumption \ref{StateEq23} we have that $\overline{C^l}(x(\widetilde{t})) = \underline{C^l}(x(\widetilde{t})) = C^l(\widetilde{t}+1)$, for all $1 \leq l \leq k$, we also have that $r(\widetilde{t}) > 0$.

\begin{proposition} \label{StateEq24}
Let $(C(\widetilde{t}), x(\widetilde{t}))$ be the clusters and centers k-means returns. Denote by $U = B\left( x^1(\widetilde{t}),\frac{r(\widetilde{t})}{2}\right) \times  B\left( x^2(\widetilde{t}),\frac{r(\widetilde{t})}{2}\right) \times \cdots \times B\left( x^l(\widetilde{t}),\frac{r(\widetilde{t})}{2} \right)$ an open neighbourhood of $x(\widetilde{t})$, then for any $x \in U$ we have $C^l(\widetilde{t}) = \underline{C^l}(x)$ for all $1 \leq l \leq k$.
\end{proposition}

\begin{proof}
Pick some $a \in C^l(\widetilde{t})$, then $x^l(\widetilde{t}-1)$ is the closest center among the centers of $x(\widetilde{t}-1)$. Since k-means stops after $\widetilde{t}$ iterations it is clear that $x(\widetilde{t})=x(\widetilde{t}-1)$, thus $x^l(\widetilde{t})$ is the closest center to $a$ among the centers of $x(\widetilde{t})$. Further we have
\begin{equation}
	r(\widetilde{t}) \leq \|x^j(\widetilde{t}) - a\| - \|x^l(\widetilde{t}) -a\|, \quad \forall \; j \neq l . \label{StateEq25}
\end{equation}
Next, we show that $a \in \underline{C^l}(x)$, indeed
\begin{align*}
	\|a - x^l\| -  \|a - x^j\| &\leq \|a - x^l(\widetilde{t})\| + \|x^l(\widetilde{t}) - x^l\| - \left( \|a - x^j(\widetilde{t})\| - \|x^j(\widetilde{t}) - x^j\| \right) \\
	& = \|a - x^l\| - \|a - x^j(\widetilde{t})\| + \|x^l(\widetilde{t}) - x^l\| + \|x^j(\widetilde{t}) - x^j\| \\
	& < \|a - x^l\| - \|a - x^j(\widetilde{t})\| + r(\widetilde{t}) \\
	& \leq -r(\widetilde{t}) + r(\widetilde{t}) = 0 ,
\end{align*}
where the second inequality holds since $x^l \in B\left( x^l(\widetilde{t}), \frac{r(\widetilde{t})}{2} \right)$ and $x^j \in B\left( x^j(\widetilde{t}), \frac{r(\widetilde{t})}{2} \right)$, and the third inequality follows from (\ref{StateEq25}). This proves that $a \in C^l(x)$ and therefore $C^l(\widetilde{t}) \subseteq \underline{C^l}(x)$ for all $l=1,2,\ldots,k$. 
By the definition of $\underline{C^l}(x)$ we have for any $l \neq j$ that $\underline{C^l}(x) \cap \underline{C^j}(x)=\emptyset$. Now, since $C(\widetilde{t})$ is a partition of $\mathcal{A}$, and $\underline{C^l}(x) \subset \mathcal{A}$ for all $1 \leq l \leq k$ it must hold that $C^l(\widetilde{t}) = \underline{C^l}(x)$ for all $1 \leq l \leq k$.
\end{proof}

\begin{proposition}[k-means converges to local minimum]
Let $\left(C(\widetilde{t}), x(\widetilde{t})\right)$ be the clusters and centers k-means returns, then $x(\widetilde{t})$ is local minimum of $F$ (see (\ref{clustering_objective})) on $U = B\left( x^1(\widetilde{t}),\frac{r(\widetilde{t})}{2}\right) \times  B\left( x^2(\widetilde{t}),\frac{r(\widetilde{t})}{2}\right) \times \cdots \times B\left( x^l(\widetilde{t}),\frac{r(\widetilde{t})}{2} \right) \subset \mathbb{R}^{nk}$.
\end{proposition}

\begin{proof}
The minimum of $F$ in $U$ is
\begin{equation*}
\min\limits_{x \in U} F(x) = \min\limits_{x \in U} \sum\limits_{l=1}^{k} \sum\limits_{a \in \underline{C^l}(x)} \|a - x^l \|^2 = \min\limits_{x \in U} \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(\widetilde{t})} \|a - x^l \|^2 ,
\end{equation*}
where the last equality follows from \Cref{StateEq24}. \\
The function $x \mapsto \sum\limits_{l=1}^{k} \sum\limits_{a \in C^l(\widetilde{t})} \|a - x^l \|^2$ is strictly convex, separable in $x^l$ for all $1 \leq l \leq k$, and reaches its minimum at $\frac{1}{\left| C^l(\widetilde{t}) \right|} \sum\limits_{a \in C^l(\widetilde{t})} a = x^l(\widetilde{t}),$ and the result follows.
\end{proof}
