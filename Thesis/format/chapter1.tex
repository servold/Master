 \chapter{Introduction} \label{intro}

\noindent \noindent \hrulefill

This chapter presents the importance of the clustering problem and describes the usefulness of clustering in many applications. Moreover, we describe the existing types and approaches of clustering, and review the literature on the most popular clustering center-based algorithms. We discuss the contributions of this thesis, mainly in developing new algorithms and the use of a novel methodology to prove their convergence results.

\noindent \noindent \hrulefill

\section{Background and Motivation}

The clustering problem is a task of grouping objects which are similar. It consists of partitioning a dataset into subsets, called clusters, such that the data points in each cluster are similar with respect to a specific criteria. \medskip

The clustering problem is a fundamental problem in the machine learning field, and it arises in a wide scope of applications, such as data mining, pattern recognition, information retrieval and many others. For example, in image segmentation, one is interested in partitioning the pixels of an image into objects, where each pixel can be described via its location in the image and its color given in RGB format. Another example is learning the probability density of some data, where the data is assumed to be drawn from a mixtures of distributions. Each partition of the data is represented by a unimodal probability density model, and a summation of all the cluster models gives a multimodal density for the entire dataset. Vector quantization is yet another example, where large sets of points are represented by their centroid point. This approach can be used for data compression, data correction and pattern recognition.\medskip

There are several categories of clustering methods, each has a direct impact on the final clustering structure.
\begin{enumerate}[(i)]
	\item Hierarchical versus partitioning clustering. In partitioning clustering the dataset is divided into clusters, whereas in hierarchical clustering each cluster may have sub-clusters, thus forming a tree which leaves are single points of the dataset.
	\item Hard versus soft and fuzzy clustering. In hard clustering each data point is assigned to single cluster, versus a soft clustering where each point may be assigned to more than one cluster, hence clusters may overlap. In fuzzy clustering for each point there is a distribution that describes the probability of a point to be part of a certain cluster.
	\item Complete versus partial clustering. In complete clustering all points in the dataset are assigned to clusters, whereas in partial clustering some points may be intentionally skipped and are not being assigned to a cluster.
\end{enumerate}

Finding the optimal partition of a fixed number of clusters for some given dataset is known to be an NP-hard problem, and hence cannot be solved efficiently. Most algorithms seek to minimize certain objective function, and usually achieve local rather than global minimum solution. In this work we focus on partitioning clustering, where the number of clusters in known in advance. Most partitioning clustering methods iteratively update the cluster centers, and hence they are often referred as center-based clustering methods. \medskip

We introduce few notations for the upcoming discussion. Let $\mathcal{A}= \left\lbrace a^1, a^2, \ldots ,a^m \right\rbrace$ be a given set of points in $\mathbb{R}^n$, and let $1 < k < m$ be a fixed given number of clusters. The clustering problem consists of partitioning the dataset $\mathcal{A}$ into $k$ subsets $\left\lbrace C^1, C^2, \ldots ,C^k \right\rbrace$, called clusters. For each $l=1, 2, \ldots ,k$, the cluster $C^l$ is represented by its center $x^l \in \mathbb{R}^n$. We describe several well-known center-based clustering algorithms.
\begin{enumerate}[(i)]
	\item The k-means algorithm. This algorithm is probably the most famous within the clustering scope, and dates back to MacQueen from 1967 (see \cite{M1967}). The k-means algorithm partitions the data into $k$ sets. The solution is then a set of $k$ centers, each of which is located at the centroid of the data for which it is the closest center. The k-means algorithm performs hard clustering, and each point is labeled according to its closest center. This algorithm can be discribed as an optimization algorithm (see precise details below) which minimizes the following objective function
	\begin{equation*}
		f_{KM}(x) = \sum\limits_{i=1}^{m}\min\limits_{1 \le l \le k} \norm{a^i-x^l}^2.
	\end{equation*}
	The simplicity of the algorithm both in the theoretical and implementation aspects made it very popular.
	\item The fuzzy k-means (FKM) algorithm. The FKM algorithm is a soft clustering method. For each data point the result of the FKM algorithm is a distribution of membership over the clusters. The objective function that the FKM algorithm minimizes is
	\begin{equation*}
		f_{FKM}(x) = \sum\limits_{i=1}^{m}\sum\limits_{l=1}^{k}(w^i_l)^\beta \norm{a^i-x^l}^2.
	\end{equation*}
	The parameter $w^i_l$ denotes the probability that data point $a^i$ is assigned to cluster $x^l$, thus it is under the constraints $\sum_{l=1}^{k} w^i_l = 1$ for all $1 \leq i \leq m$ and $w^i_l \geq 0$. The parameter $\beta > 1$ governs the "fuzzy partition". Setting $\beta = 1$ results in the standard k-means algorithm (see \Cref{State_Clustering_Reformulation} for more details).
	\item The Expectation-Maximization (EM) algorithm. The EM algorithm is used extensively in statistical estimation problems for learning mixtures of distributions. It is a soft clustering algorithm. The objective function that EM maximizes is 
	\begin{equation*}
		f_{EM} = \sum\limits_{i=1}^{m} \log \left( \sum\limits_{l=1}^{k} p\left(a^i|x^l\right) p\left(x^l\right) \right),
	\end{equation*}
	where  $p\left(a^i|x^l\right)$ is the probability of $a^i$ given that it is generated by the Gaussian distribution with center $x^l$ and $p\left(x^l\right)$ is the prior probability of center $x^l$.
\end{enumerate}
An interesting paper of Teboulle \cite{T2007} shows that these center-based clustering algorithms can be recovered from a certain proposed continuous optimization framework which will be used in this work (see more details below). The smoothing methodologies for the clustering problem are based on nonlinear means and on approximation of appropriate asymptotic functions. \medskip

Most of the existing clustering methods are sensitive to the starting point, namely choosing different starting point result in significant differences in the final clustering. There are plethora of heuristic initialization methods. One such initialization method is to choose $k$ random data points as staring centers, assuming uniform distribution or some other prior distribution on the data. Another popular method is k-means++, where the first center is chosen at random from the dataset, and for each $2 \leq l \leq k$, the center $x^l$ is the furthest data point from the data points chosen so far. \medskip

We begin this work with a formulation of the clustering problem which consists of minimizing the sum of finite collection of min-functions. This is a nonsmooth and nonconvex optimization problem, in its most general case. The clustering problem is given by
\begin{equation}
	\min\limits_{x \in \mathbb{R}^{nk}} \left\lbrace F(x) := \sum\limits_{i=1}^{m} \min\limits_{1 \le l \le k} d(x^l,a^i) \right\rbrace , \label{StateEq1}
\end{equation}
where $x=\left( x^1,x^2, \ldots, x^k \right) \in \rr^{nk}$ with $\textit{d}(\cdot ,\cdot)$ being a distance-like function. \medskip

We focus on two cases of distance-like functions. The first is the squared Euclidean norm, which is the standard proximity measure used in the k-means algorithm. For this case, we derive an equivalent smooth optimization problem for the clustering problem presented in (\ref{StateEq1}) and prove convergence result for the suggested algorithm via the methodology which was recently developed in \cite{BST2014} and will be discussed in great details below. The second distance-like function that we study is the Euclidean norm. In this case we present an approximation model, in order to overcome the lack of smoothness in the problem. Then we propose an algorithm to solve the approximated model which combines ideas which were used in the squared Euclidean case with a classical smoothing ideas which was used in \cite{BS2015}. We present numeric experiments, that show the superiority of the Euclidean norm distance function for datasets in which the data points are spread relatively sparsely form their centers. \medskip

The lack of smoothness in this model can be overcome, yet the nonconvex nature of the clustering problem will accompany the discussions throughout this work. Significant amount of studies have been made on convex models, even though in many cases the original optimization problem is nonconvex. To overcome the lack of convexity, one of the common approaches is usually achieved by considering a convex relaxation of the original problem. In this thesis we take a different route and consider the problem in its original nonconvex form. Very recently this complicated route became more relevant and interesting thanks to few papers (see \cite{AB2009,ABS2013,BST2014} and the references therein) which pave the way for dealing with nonconvex problems using sophisticated mathematical tools as will be explained later in \Cref{State_PALM_Theory}.


\section{Outline and Contributions of The Thesis}

Our main objectives and contributions in this thesis are as follows.
\begin{itemize}
%  \item To reformulate the clustering problem starting from its usual discrete form into a continuous formulation, which enable to treat the problem with powerful continuous optimization tools.
	\item To develop algorithms that address the clustering problem for two different distance-like functions and present numerical tests which demonstrate the effectiveness of proposed algorithms.
	
	\item To demonstrate the usefulness of KL theory and the general methodology developed in \cite{BST2014} to tackle the clustering problem.
  
	\item To prove the convergence of k-means to a critical point.
\end{itemize}

We outline now the contents of this thesis.
\begin{itemize}
	\item In Chapter 2 we transform the initial formulation  of the clustering problem into a smooth one. In addition, we recall the KL theory and the general methodology, which was developed in \cite{BST2014}, that will be used in our analysis of the proposed algorithms.
	\item In Chapter 3 we tackle the clustering problem with squared Euclidean norm distance-like function, which is the most common distance used in many other clustering algorithms. The proposed clustering algorithm is based on alternating minimization, and it is similar to k-means algorithm. In this case the objective function is smooth and we can to apply the general methodology, and prove the convergence of the generated sequence to a critical point.
	\item In Chapter 4 we tackle the clustering problem with Euclidean norm distance-like function. Providing an approximation to the original objective function we overcome the lack of smoothness and then proceed with the general methodology and prove again the convergence of the generated sequence to a critical point of the approximating objective function.
	\item In Chepter 5 we show that the k-means algorithm can be recovered from the proposed model of the clustering problem presented in Chapter 2. In addition, we prove that k-means algorithm convergence to a critical point, and under additional assumption, we extend the convergence to a local minimum.
	\item In Chapter 6 we compare the performance of the proposed algorithms, and some existing center-based clustering algorithms, according to some common criteria.
\end{itemize}

\section{Notation and Terminology}

The following notations will be used throughout this thesis

\begin{table}[htbp]\caption{Table of Notations}
\begin{center}% used the environment to augment the vertical space
% between the caption and the table
\begin{tabular}{r p{13cm} }
\hline
$\mathcal{A}$ & dataset for clustering of size $m$\\
$k$  & the number of clusters\\
$x^l$ & center of cluster $l$, for each $l=1,2,\ldots,k$\\
$\left\langle\cdot,\cdot\right\rangle$ & the standard dot product in Euclidean space, that is $\left\langle u,v \right\rangle = \sum\limits_{i=1}^{d} u_l v_l$\\
$\norm{\cdot}$ & Euclidean norm $\norm{x}=\sqrt{\sum\limits_{l=1}^{d} {x_l}^2}$\\
$\Delta$  & the simplex i.e., $\Delta = \left\lbrace u \in \mathbb{R}^d : \sum\limits_{l=1}^{d} u_l = 1, \: u \geq 0 \right\rbrace$\\
$\delta_{S}(\cdot)$  & delta function of set $S \subset \rr^d$, which is defined to be $0$ in $S$ and $\infty$ otherwise\\
dom$\sigma$ & domain of function $\sigma$, which is all vectors $v$ such that $\sigma(v) < \infty$\\
$\partial\sigma$ & subdifferential of function $\sigma$ (see \Cref{subdiff_def})\\
crit$\sigma$ & set of all critical points of function $\sigma$, that is all vectors $v$ such that $0 \in \partial\sigma(v)$\\
dist$(u,S)$ & distance function, for any point $u \in \rr^d$ and set $S \in \rr^d$, dist$(u,S):=\inf\left\lbrace \norm{u-v} : v \in S \right\rbrace$\\
$H(w,x)$ & sum of distances of $x^l$ from each data point in $\mathcal{A}$, adjusted by the non-negative weights $w^i$, $H(w,x)=\sum\limits_{i=1}^m \sum\limits_{l=1}^k w^i_ld\left(x^l, a^i\right)$\\
$G(w)$ & sum of delta functions which constrain each $w^i$ to be in the simplex, that is $G(w)=\sum\limits_{i=1}^{m} \delta_{\Delta}(w^i)$\\
$\Psi$ & the objective function in the clustering problem, defined by $\Psi(w,x)=H(w,x)+G(w)$\\
\hline
\end{tabular}
\end{center}
\label{tab:TableOfNotations}
\end{table}

