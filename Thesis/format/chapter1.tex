 \chapter{Introduction} \label{intro}

\noindent \noindent \hrulefill

This chapter presents the importance of the clustering problem and describes the usefulness of clustering in many applications. Moreover, we describe existing types and approaches for clustering, and review the literature of the most popular clustering center-based algorithms. We outline the main contributions of the thesis which consists of: (i) the development of new algorithms, (ii) a novel methodology to prove global convergence of the proposed methods, (iii) numerical examples illustrating their performance in comparison with standard schemes.

\noindent \noindent \hrulefill

\section{Background and Motivation}

The clustering problem is a task of grouping objects which are similar. It consists of partitioning a dataset into subsets, called clusters, such that the data points in each cluster are similar with respect to a specific criteria. \medskip

The clustering problem is a fundamental problem in the machine learning field, and it arises in a wide scope of applications, such as data mining, pattern recognition, information retrieval and many others. For example, in image segmentation, one is interested in partitioning the pixels of an image into objects, where each pixel can be described via its location in the image and its color given in RGB format. Another example is learning the probability density of some data, where the data is assumed to be drawn from a mixtures of distributions. Each partition of the data is represented by a unimodal probability density model, and a summation of all the cluster models gives a multimodal density for the entire dataset. Vector quantization is yet another example, where large sets of points are represented by their centroid point. This approach can be used for data compression, data correction and pattern recognition.\medskip

There are several types of clustering approaches, each has a direct impact on the final clustering structure.
\begin{enumerate}[(i)]
	\item Hierarchical versus partitioning clustering. In partitioning clustering the dataset is divided into clusters, whereas in hierarchical clustering each cluster may have sub-clusters, thus forming a tree which leaves are single points of the dataset.
	\item Hard versus soft and fuzzy clustering. In hard clustering each data point is assigned to single cluster, versus a soft clustering where each point may be assigned to more than one cluster, hence clusters may overlap. In fuzzy clustering for each point there is a distribution that describes the probability of a point to be part of a certain cluster.
	\item Complete versus partial clustering. In complete clustering all points in the dataset are assigned to clusters, whereas in partial clustering some points may be intentionally skipped and are not being assigned to a cluster.
\end{enumerate}

Finding the optimal partition of a fixed number of clusters for some given dataset is known to be an NP-hard problem (see \cite{GJ1979}), and hence cannot be solved efficiently. Most algorithms seek to minimize certain objective function, and usually achieve local rather than global minimum solution. \medskip

In this work we focus on partitioning clustering, where the number of clusters in known in advance. Most partitioning clustering methods iteratively update the cluster centers, and hence they are often referred as center-based clustering methods. \medskip

We introduce few notations for the upcoming discussion. Let $\mathcal{A}= \left\lbrace a^1, a^2, \ldots ,a^m \right\rbrace$ be a given set of points in $\mathbb{R}^n$, and let $1 < k < m$ be a fixed given number of clusters. The clustering problem consists of partitioning the dataset $\mathcal{A}$ into $k$ subsets $\left\lbrace C^1, C^2, \ldots ,C^k \right\rbrace$, called clusters. For each $l=1, 2, \ldots ,k$, the cluster $C^l$ is represented by its center $x^l \in \mathbb{R}^n$. We describe few well-known center-based clustering algorithms.
\begin{enumerate}[(i)]
	\item The k-means algorithm. This algorithm is probably the most famous within the clustering scope, and dates back to Steinhaus (1956), MacQueen (1967) and Lloyd (1982) (see \cite{S1956,M1967,L1982}). The k-means algorithm partitions the data into $k$ sets. The solution is then a set of $k$ centers, each of which is located at the centroid of the data for which it is the closest center. The k-means algorithm performs hard clustering, and each point is labeled according to its closest center. This algorithm can be described as an optimization algorithm (see precise details below) which minimizes the following objective function
	\begin{equation*}
		f_{KM}(x^1,x^2,\ldots,x^k) = \sum\limits_{i=1}^{m}\min\limits_{1 \le l \le k} \norm{a^i-x^l}^2.
	\end{equation*}
	The simplicity of the algorithm both in the updating rules and the implementation aspects made it very popular. There has been enormous improved techniques designed targeting a variety of applications (see \cite{JMF1999} for a review).
	
	\item The fuzzy k-means (FKM) algorithm. The FKM algorithm is a soft clustering method. For each data point the result of the FKM algorithm is a distribution of membership over the clusters (see \cite{B1981} for the original paper by Bezdek on FKM). The objective function that the FKM algorithm minimizes is
	\begin{equation*}
		f_{FKM}(w^1,w^2,\ldots,w^m,x^1,x^2,\ldots,x^k) = \sum\limits_{i=1}^{m}\sum\limits_{l=1}^{k}(w^i_l)^\beta \norm{a^i-x^l}^2.
	\end{equation*}
	The variable $w^i_l$ denotes the probability that data point $a^i$ is assigned to cluster $x^l$, thus it is under the constraints $\sum_{l=1}^{k} w^i_l = 1$ for all $1 \leq i \leq m$ and $w^i_l \geq 0$. The parameter $\beta > 1$ governs the "fuzzy partition". Setting $\beta = 1$ results in the standard k-means algorithm (see \Cref{State_Clustering_Reformulation} for more details). The FKM algorithm is the Gauss-Seidel method applied to $f_{FKM}$, that is, keeping $\left(x^1,x^2,\ldots,x^k\right)$ fixed and minimizing with respect to $\left(w^1,w^2,\ldots,w^m\right)$, and then vice-versa (see \cite[p. 528]{DHS2001}).
	\item The Expectation-Maximization (EM) algorithm. The EM algorithm (Dempster et al. \cite{DLR1977}) is used extensively in statistical estimation problems for learning mixtures of distributions. It is a soft clustering algorithm. The objective function that EM maximizes is 
	\begin{equation*}
		f_{EM}(x^1,x^2,\ldots,x^k) = \sum\limits_{i=1}^{m} \log \left( \sum\limits_{l=1}^{k} p\left(a^i|x^l\right) p\left(x^l\right) \right),
	\end{equation*}
	where  $p\left(a^i|x^l\right)$ is the probability of $a^i$ given that it is generated by the Gaussian distribution with center $x^l$ and $p\left(x^l\right)$ is the prior probability of center $x^l$. The algorithm is guaranteed to converge to a local maximum of the likelihood function $f_{EM}$ (see \cite{W1983}).
\end{enumerate}
An interesting paper of Teboulle \cite{T2007} shows that these center-based clustering algorithms can be recovered from a certain proposed continuous optimization framework which will be used in this work too (see more details below). The unified model presented in \cite{T2007} provides an elegant and simple way to describe the models presented above and more. To tackle this model, Teboulle uses smoothing techniques which are based on approximation of an asymptotic convex nonlinear mean functions. The suggested center-based algorithm for soft clustering, the Smooth k-means (SKM) algorithm, generates a sequence whose each limit point is a stationary point of the approximated objective function.\medskip
%
%The k-means objective function, $f_{KM}$, can be extended to a more general form,
%	\begin{equation}
%		F(x)=\sum\limits_{i=1}^m v_i \min\limits_{1\leq l \leq k} d\left(x^l,a^i\right), \label{clustering_objective}
%	\end{equation}
%	where $d(\cdot,\cdot)$ is some distance-like function and $v_i$ positive weights which satisfy $\sum_{i=1}^m v_i=1$. The smoothing methods in \cite{T2007} are based on replacing the nonsmooth term $\min_{1\leq l \leq k} d\left(x^l,a^i\right) = -\sigma_{\Delta}(-d(x))$ with an approximation of an asymptotic convex nonlinear mean function defined by
%	\begin{equation*}
%		G^{\infty}_h(z) = \lim\limits_{s\rightarrow 0^+} sh^{-1} \left(\sum\limits_{l=1}^k \pi_l h\left(\frac{z_l}{s} \right) \right),
%	\end{equation*}
%	where $\pi \in \Delta_+:=int\left(\Delta\right)$ and $h \in \mathcal{H}$ which is the class of function defined by
%	\begin{equation*}
%		\mathcal{H} = \left\lbrace h \in C^3(int(\text{dom}h)) : h'>0, \; h''>0, \text{ and } t\mapsto -\frac{h'(t)}{h''(t)} \text{ is convex}\right\rbrace.
%	\end{equation*}
%	 Therefore, the approximating smooth objective function of the original nonsmooth clustering problem function, defined in (\ref{clustering_objective}), takes the following form
%	\begin{equation*}
%		F_s(x) = -s\sum\limits_{i=1}^m v_i h^{-1} \left(\sum\limits_{l=1}^k \pi_l h\left(\frac{-d\left(x^l,a^i\right)}{s} \right) \right),
%	\end{equation*}
%	where $s>0$ serves as a smoothing parameter. The suggested center-based algorithm for soft clustering, the Smooth k-means (SKM) algorithm, generates a sequence whose each limit point is a stationary point of $F_s$.\medskip

We begin this work with the formulation of the clustering problem, used in \cite{T2007}, which consists of minimizing the sum of finite collection of min-functions. This is a nonsmooth and nonconvex optimization problem, in its most general case. The clustering problem is given by
\begin{equation}
	\min\limits_{x \in \mathbb{R}^{nk}} \left\lbrace F(x) := \sum\limits_{i=1}^{m} \min\limits_{1 \le l \le k} d(x^l,a^i) \right\rbrace , \label{clustering_objective}
\end{equation}
where $x=\left( x^1,x^2, \ldots, x^k \right) \in \rr^{nk}$ with $\textit{d}(\cdot ,\cdot)$ being a distance-like function. \medskip

We focus on two cases of distance-like functions. The first is the squared Euclidean norm, which is the standard proximity measure used in the k-means algorithm. For this case, we use an equivalent smooth optimization problem for the clustering problem presented in (\ref{clustering_objective}) and suggest a simple and efficient first order method for tackling it. We prove convergence result for the suggested algorithm via the methodology which was recently developed in \cite{BST2014} and will be discussed in great details below. 
The second distance-like function that we study is the Euclidean norm. In this case we present an approximation model, in order to overcome the lack of smoothness in the problem, introduced by the norm. Then we propose an algorithm to solve the approximated model which combines ideas which were used in the squared Euclidean case with a classical smoothing idea which was used in \cite{BS2015}. We present numerical experiments, that show the superiority of the Euclidean norm distance function for datasets in which the data points are spread relatively sparsely form their centers. \medskip

The lack of smoothness in the general model  (\ref{clustering_objective}) can be overcome, yet the nonconvex nature of the clustering problem remains a major difficulty. Significant amount of studies have been made on convex models, even though in many cases the original optimization problem is nonconvex. To overcome the lack of convexity, one of the common approaches is usually achieved by considering a convex relaxation of the original problem. In this thesis we take a different route and consider the problem in its original nonconvex form. Very recently this complicated route became more relevant and interesting thanks to few papers (see \cite{AB2009,ABS2013,BST2014} and the references therein) which pave the way for dealing with nonconvex problems using sophisticated mathematical tools as will be detailed later in \Cref{State_PALM_Theory}.


\section{Outline and Contributions of The Thesis}

Our main objectives and contributions in this thesis are as follows.
\begin{itemize}
%  \item To reformulate the clustering problem starting from its usual discrete form into a continuous formulation, which enable to treat the problem with powerful continuous optimization tools.
	\item To develop algorithms that address the clustering problem for two different distance-like functions and present numerical tests which demonstrate the effectiveness of the proposed algorithms.
	
	\item To demonstrate the usefulness of the Kurdyka-{\L}ojasiewicz (KL) property and the general methodology developed in \cite{BST2014} to tackle the clustering problem. Specially, when the proximity function is taken to be the Euclidean norm the obtained optimization problem is completely nonsmooth (that is, all the involved functions are nonsmooth), thus in this thesis we first use this general methodology which combined with smoothing idea in the nonconvex setting.
  
	\item To prove the convergence of k-means to a critical point, and extend the result to a local minima, assuming the closest center to each data point is unique.
\end{itemize}

We outline now the contents of each chapter in this thesis.
\begin{itemize}
	\item In \Cref{Chptr2} we transform the initial discrete formulation  of the clustering problem (see (\ref{clustering_objective})) into a smooth model. In addition, we recall the KL theory and the general methodology, which was developed in \cite{BST2014}, that will be used in our analysis of the proposed algorithms.
	\item In \Cref{Chptr3} we tackle the clustering problem with the squared Euclidean norm distance-like function, which is the most common distance used in many other clustering algorithms. The proposed clustering algorithm (KPALM) is based on the alternating minimization method, and it is similar to the k-means algorithm. In this case the objective function is smooth and we can apply the general methodology, and prove convergence of the generated sequence to a critical point of the corresponding objective function. We show that the k-means algorithm can be recovered from the proposed KPALM algorithm by choosing specific parameters. In addition, we prove that k-means algorithm convergence to a critical point, and under additional assumption, we extend the convergence to a local minimum.
	\item In \Cref{Chptr4} we study the clustering problem with the Euclidean norm distance-like function and propose an iterative algorithm ($\varepsilon$-KPALM) to tackle a smoothed approximation model. We provide an approximation to the original objective function which overcome the lack of smoothness and then proceed with the general methodology and prove again convergence of the generated sequence to a critical point of the approximated smooth objective function.
	\item In \Cref{Chptr5} we illustrate the performance of the proposed algorithms, and compare them with some existing center-based clustering algorithms.
\end{itemize}

\clearpage
\section{Notation and Terminology}

The following notations will be used throughout this thesis

\begin{table}[htbp]\caption{Table of Notations}
\begin{center}% used the environment to augment the vertical space
% between the caption and the table
\begin{tabular}{r p{13cm} }
\hline
$\mathcal{A}$ & dataset for clustering of size $m$\\
$k$  & the number of clusters\\
$x^l$ & center of cluster $l$, for each $l=1,2,\ldots,k$; $x=\left( x^1,x^2,\cdots,x^k \right)$\\
$\left\langle\cdot,\cdot\right\rangle$ & the standard dot product in Euclidean space, that is $\left\langle u,v \right\rangle = \sum\limits_{i=1}^{d} u_l v_l$\\
$\norm{\cdot}$ & Euclidean norm $\norm{x}=\sqrt{\sum\limits_{l=1}^{d} {x_l}^2}$, for any $x \in \rr^d$\\
$\Delta$  & the simplex i.e., $\Delta = \left\lbrace u \in \mathbb{R}^d : \sum\limits_{l=1}^{d} u_l = 1, \: u \geq 0 \right\rbrace$\\
$\delta_{S}(\cdot)$  & indicator function of a set $S \subset \rr^d$, which is defined to be $0$ in $S$ and $\infty$ otherwise\\
dom$\sigma$ & effective domain of the function $\sigma$, i.e., dom$\sigma:=\left\lbrace v \in \rr^d : \sigma(v)<\infty \right\rbrace$\\
$\partial\sigma$ & subdifferential of the function $\sigma$ (see \Cref{subdiff_def})\\
crit$\sigma$ & set of all critical points of function $\sigma$, that is all vectors $v$ such that $0 \in \partial\sigma(v)$\\
dist$(u,S)$ & distance function, for any point $u \in \rr^d$ and set $S \in \rr^d$, dist$(u,S):=\inf\left\lbrace \norm{u-v} : v \in S \right\rbrace$\\
$H(w,x)$ & sum of distances of $x^l$ from each data point in $\mathcal{A}$, adjusted with the non-negative weights $w^i$, $H(w,x)=\sum\limits_{i=1}^m \sum\limits_{l=1}^k w^i_ld\left(x^l, a^i\right)$\\
$G(w)$ & sum of indicator functions which constraints each $w^i$ to be in the simplex, that is $G(w)=\sum\limits_{i=1}^{m} \delta_{\Delta}(w^i)$\\
$\sigma$ & the objective function in the clustering problem, defined by $\sigma(w,x)=H(w,x)+G(w)$\\
\hline
\end{tabular}
\end{center}
\label{tab:TableOfNotations}
\end{table}

